Initially, my machines were named after classical philosophers and mathematicians (starting with Euclid, after the computer in Pi, but then proceeding to Plato, Aristotle, and Archimedes). However, Plato & Aristotle died almost simultaneously, and I switched to a non-classical schema focusing on mathematicians with Descartes, Liebniz, and Hume. Currently, my network consists of: Vonneumann (the server), Erdos (my work machine), Hilbert (my smartphone — because Hilbert is associated with both infinite hotels and space-filling curves), and Agrippa (the chromecast). Retired names include Godel (the PDA), Cantor (a now-dead server), and Conway (a laptop, named after John Conway of Game of Life fame).
By Rococo Modem Basilisk on May 21, 2015.
Against certain naive varieties of transhumanist sentiment
A frequent idea that I run across when speaking to technophiles with transhumanist leanings these days is the superiority of tech over biology. People will say, “I’ll upload my brain and live forever”, or “I’ll replace my arm so that I can be stronger”, or “I’ll get a wetwire to the internet so I can read faster”. This isn’t a new idea; I said variations on the same thing when I was fifteen. But, it’s absolutely stupid.
We have never built a machine with a lifespan and resilience comparable to a human being. Machine failure rates fall along a bathtub curve, but while an expected human lifespan is betweenseventy and eighty years these days, it’s the rare freak of a machine that still functions after ten or twenty years — let along thirty, let alone continuously. Biological systems have insanely complex self-repair and self-maintenance systems, and the reason we live for eighty yearsis that our parts are continuously being maintained, rather than undergoing routine maintenance on a human scale of weekly-monthly-yearly. The very first programmable electromechanical computers were built in the 30s (depending on who you ask and how you define it, you can push it forward or back about ten years), meaning that a human being living an average lifetime that was born at the same moment as the very first programmable computer in the modern sense would be dying *right now*; drum and disk storage is twenty years younger(along with transistors, ram that’s not based on relays/mercury tubes/ CRTs, programming languages other than machine code, and interactive terminals), and the internet is about fifteen years younger than *that* (along with email, pipes, directories, and asymmetric key cryptography). Someone born at the moment the first packet was sent over the internet would be middle-aged. Nevertheless, all these systems have changed drastically many times over the course of their lifetime, in incompatible ways. All of the component parts have been replaced many times over. At various points in the past, all these systems have had *complete* failures (yes, including the internet). These systems are not at the point where they could be expected to safeguard the lifetime of a rat, let alone extend the lifetime of a human being.
Likewise, with prosthetic arms. Prosthetic arms are great — for people who are missing their arms. Cochlear implants aren’t competing with people’s real ears; they’re competing with being deaf. The prosthetic eyes that have finally gotten FDA approval have approximately the same resolution as a TRS-80 Model 100  — they aren’t competing with real eyes, but with total blindness.
Wetwires are in an even worse position. The current state of the art in brain implants can, with incredibly invasive and dangerous brain surgery, temporarily hook your brain up to 200 I/O lines, each of which neurons *might* decide to grow on. Wetwires are competing with reading: a four thousand year old technology that’s constantly being improved upon, that takes advantage of the human eye and optic nerve — a pipe so fat that the eye is considered to be a part of the brain, a pipe so fat that the eye does complex processing independently of the visual cortex and can independently trigger signals to the amygdala about emotionally striking scenes before the visual cortex can even receive the image data. Furthermore, reading is a technology that the government of every developed nation spends huge amounts of money on installing into its citizens! Wetwires can’t compete with that.
That said, this isn’t the end of transhumanism, or even of grinding. Implants aren’t going to go away. It’s just that we aren’t looking at them correctly.
Implants are a *long* way away from replacing the things that human beings already do well, like living and thinking and reading and moving. Generally speaking, to the extent that it’s trivial to do so, when there’s a problem with scale, we invent an external technology to handle it — when we need to turn bolts, we build wrenches that fit in our hands instead of hacking our arms off and replacing them with wrenches. If we depend upon implant tech (and other varieties of transhuman tech) to fund itself by being an improvement over what humans already are capable of doing, then the whole field will go bankrupt. But, there aretwo fields in which this kind of tech can excel. One is performing aworse job at than the human body at tasks that the human body already does — prosthetics for people with missing limbs, and replacement parts for people whose parts are faulty or missing. The other is allowing human beings to do things they’ve never done before — not by increasing scale, but by qualitative change.
The cochlear implant kind of sucks. When it’s installed, wires are stuck to the cochlea — a snail-shaped fluid-filled organ in the inner ear that does the heavy lifting in hearing (the rest of the ear is basically involved in amplification and protection). In normal hearing, vibrations from outside the ear are amplified by a set of bones that operate like a cam assembly, before pressing on a flexible membrane on the big end of the cochlea, and the movement of tiny hairs inside the cochlea produces the perception of sound, with the position of the hairs that are most stimulated determined by the frequency of the sound. In a cochlear implant, the wires cause the hairs to be stimulated directly, with galvanism, and so the number of wires installed corresponds to the resolution of sound available. We do not have the technology to produce CD-quality sound. We don’t even have the technology to produce speak-and-spell-quality sound. People with cochlear implants are stuck trying to decode speech based on fewer distinct frequencies than there are bars on a child’s xylophone. But the cochlear implant, as an accident of its nature, has one improvement over the ear — it has a built-in headphone jack. Cochlear implant-like technologies are far from being an improvement over ears, but when combined with throat mics or other mechanisms for producing the fundamental elements of speech from subvocalizations, they might be an improvement over the walkie-talkie. At the point at which this technology has enough demand to make people voluntarily submit to brain surgery, I expect that this is exactly how it will be used (and I expect the first market to be military or paramilitary — people who, on a life or death basis, need to communicate without using their hands and without being heard by other people nearby).
There’s another trend going on, as well. Just as desktops became laptops and laptops became smartphones, smartphones are on the cusp of becoming wearables, and wearables will become implants.
However, this change-over is very rarely quick, and even more rarely complete. Before desktops, we had minicomputers, and before minicomputers, mainframes; however, minicomputers are not quite gone (IBM still sells machines running z/ OS, although most of the market is dying), and desktops are hardly going anywhere.
Every few years, the entire tech industry pulls out twenty-year-old project from the MIT Media Lab or CMU or PARC or somewhere and collectively decides to shit its pants over it. Recently, we’ve been hitting a quadruple-whammy: wearable computers, the Internet of Things, 3d printing, and virtual reality.
The current wearable computer boom started with Google Glass taking pretty much equally from the work that Thad Starner and Steve Mann were doing in the early 90s; appropriately, Starner was brought onto the Glass project, while Mann was completely uncredited despite the fact that they took the name from him. And, despite the fact that Glass was a complete PR disaster, Google definitely decided what parts of Starner’s work to borrow with an eye toward PR — Starner’s most interesting idea, subliminal reminders, was omitted from Glass and the Glass UI standards and Glass UI frameworks were written in such away that subliminal reminders should be completely impossible. Now, in an almost hilariously ironic turn of events, Microsoft has taken essentially exactly the same technology, made it steroscopic, reframed it in terms of geolocation-centric AR (something Glass was never going to be capable of doing, by design), and turned it into a massive PR success.
In comparison, the current Internet of Things boom seems to be driven entirely by industry-wide amnesia. That’s not entirely unfair, since the industry has, until now, had a very hard time figuring out what to call it. The current term of art is the Internet of Things, but from around 1995 to around 2005, everybody was calling it Ubiquitous Computing. The IoT is hitting a lot of the same media roadblocks as VR did in the early90s, which makes me think that it’s probably around the same point in the hype cycle, although technologically, it’s definitely further along.
Ten years ago, when I was an unemployed teenager, I had two big projects that were lighting up my eyes. One of them was a wearable computer project. The other was a UbiComp project — what you’d now call the Internet of Things. At the time, the wearable computer project was by far less feasible; displays were expensive, cpus were expensive, making either run off a battery and getting the thing small enough and light enough to fit on your body meant lowering its capabilities to an extreme. I designed several prototype wearable computers around the AT90S8515 — an 8-bit microcontroller that cost$10 and had 127 bytes of ram — and various LED-based displays, but it was clear that unless I was willing to either buy thousand-dollar equipment or strap a laptop to my back and make due with The First Church of Space JesusThe First Church of Space Jesusaudio cues as an interface, wearable computers were really infeasible. (I ended up strapping a laptop to my back and using audio cues, in the end.) The UbiComp project, on the other hand, was completely within the realm of possibility — I had a working prototype for a system for communal cooperative use of a single computer, based on identifier tokens stored on a cheap wiimote knockoff that doubled as an input device; the cost of the system was the cost of a random desktop computer, a projector, and a $20 wiimote knockoff. If I had had steady disposable income, I could have formed a corporation and finished my prototype and become yet another failed IoT startup — the technology was there, solid, and absolutely trivial.
Today, IoT is even easier. My potentially-$300 [≈ cost of PS3 gaming system, 2011] computer could be replaced with a $20 raspberry pi. Wiimote knockoffs don’t even cost $20 anymore. The projector costs more than the rest of the system in total, and my homebrewed account-sharing system could be replaced with the kind of cloud-based thing that newbies whip up in minutes and brag about on hacker news. A couple years ago, I did a wearable computer, too — with about $350worth of parts (a raspberry pi, a twiddler, a $100 head mounted display, and a USB battery pack), I built something that, while not comparable in usability to a laptop, beat the pants off the absolute best I could do with that kind of money in 2005 — mostly because of economies of scale provided by the popularity of smartphones. PDAs manufactured in 2005 couldn’t really run 800x600 color VGA, or even 300x200 color VGA — too slow. (Maybe you could do it if you were especially clever. I wasn’t clever enough to make up for my lack of riches — wagering the cost of disassembling an expensive PDA on my ability to make it drive a display was too rich for my blood.) A single-board computer capable of running Linux in 2005 was a fucking high-end single-board computer. But, the iPhone came out — a single board computer running BSD shoved into a PDA — then the Android phones started appearing a couple years later — cheaper single board computers running Linux and Java shoved into PDAs. Now the chips that run Linux in smartphones are cheap enough that Texas Instruments will give away a handful of free samples to anybody with a university-affiliated email address, complete with specialized circuitry for fast video decoding. Single board computers running Linux can be sold for $20 and make enoughmoney to prop-up a non-profit organization. Meanwhile, some nerds figured out that a series of cheap wifi chips could be reflashed, andnow you can buy complete postage-stamp-sized wifi-enabled systems that can run Lua for $5.
So, we’re at the point now where you can stick the guts of a smartphone on the side of your head and have a head-mounted smartphone with a battery life of about two hours, or you can stick the guts of your smartphone on your wrist and have a smartphone with a battery life of about a day if you barely ever have the screen on. Or, you can stick the guts of a smartphone in your pocket and stick a screen on your head, and actually have a reasonable battery life with reasonable usage. We aren’t at the point where we can start making fully wearable never-take-em-off computers with reasonable battery life and reasonable capability, although I think that if we take a page out of the MIT Media Lab book and combine this with IoT, we might be able to make due with what we have for a little longer. This has problems — centralized IoT is the domain of natural monopolies, with most of them fated to go the way of AppleTalk (although centralized IoT is all the rage now, with every consortium of manufacturers competing to make their own incompatible standards on the off chance that theirs will be the one to take off); meanwhile, decentralized IoT is the stuff of IT nightmares, where failures in logistics and/or security can lead to a lightbulb DDoSing your house and/or the white house. My own design, which was based on a federated model with an open protocol and a market for competing vendors, has unfortunately been obviated by time — it was based on the assumption that the normal use would be an evolution of the cyber-cafe, and it probably would have worked in 2005, but no longer makes sense in the same universe as widespread smartphone ownership and devices like chromecast. Offloading computing from wearables onto IoT nodes will require an extreme of either security or naivete — and because security is complicated, I fully expect a future hellworld of incredibly insecure wearable/IoT mesh networking comparable to the amazing terror of running Windows 9x on the internet in the 90s. Welcome back to an era where anybody with a modicum of knowledge can remote control your computer and nobody can patch it for five years; except this time, the computer is strapped to your face.
This is a problem that *must* be solved before the wearables become implantables. Implants need to be smaller than wearables. Right now, the state of medical device security is pretty low — while medical device software, along with airplane control software and nuclear power plant software, has higher quality standards under normal operating conditions, it’s largely no better than normal consumer-grade software when it comes to resisting actual planned attacks, and sometimes worse. We already have computers in all sorts of things — horrible, insecure computers; our airplanes can be hijacked through the in-flight wifi network, our cars can be hijacked through the CD player, our pacemakers can be remote-controlled over wifi, and our routers are already sustaining self-replicating botnets. When these devices are on our bodies, the threats become more visible; when they are in our bodies, they become potentially fatal — not necessarily because of malice (it takes a special kind of person to actually shut down somebody’s heart by exploiting their pacemaker) but because of incompetence (it doesn’t particularly take a special kind of person to try to make a botnet out of every exploitable wifi-enabled device, including pacemakers, and then not check available memory and crash the pacemakers because he’s just written to an address that doesn’t exist).
Implants are coming, and wearables are coming first. Implants will come both faster and slower than we expect, because they won’t be used how we expect. They won’t make us live longer or read faster, but instead will let us do things we haven’t imagined yet. Let’s fix our shit before we’ve got buffer overflow vulnerabilities that’ll take actual brain surgery to patch.
This article was originally published at The First Church of Space Jesus.
By Rococo Modem Basilisk on May 24, 2015.
Failure modes of science fiction
Failure modes of science fiction
There are several popular ways to look at science fiction as a genre. I have my own preferences. That said, the major opposing perspective — what I’d term the ‘machine-lit’ school of thought — has its merits, insomuch as it highlights a set of common tendencies in science fiction. I’d like to take this space to highlight the basic premise of machine-lit, the tendencies it breeds, and why I find most machine-lit to be relatively uninteresting.
(The third major perspective, what I call the spaceship-on-the-cover style, I find wholly uninteresting and is the subject of other essays; however, this perspective is becoming historically important lately because of some drama surrounding the Hugo awards being gamed by groups who prefer this style, so it’s worth mentioning in passing.)
Machine-lit is, in a general sense, the construction of a narrative around a concept invented by the author, as a capsule intended to introduce the reader to the concept. Lots of early science fiction is machine-lit for actual machines (Ralph 124C41+ being an ideal example of how this can go wrong yet still be very influential). The works of Ayn Rand are machine-lit for the Objectivist philosophy. Big-idea science fiction novels tend to be machine-lit for the ideas they represent.
One failure mode of machine-lit is that, because the narrative is intended as a delivery mechanism for the concepts, the narrative can itself be weak or nearly nonexistent if the author thinks the ideas themselves are interesting enough. (Ayn Rand, again, and Gernsback, again — but also major dystopian novels like Zamatayin’s We and 1984). Likewise, style can be a major issue in machine-lit, with The Unincorporated Man’s borderline-fanfic-quality-prose depending upon its intended audience of libertarians to forgive lack of technical skill in writing because the ideas are sufficiently in-line with the ideology, and PKD’s writing so heavily leaning on the ideas (not to mention the amphetamines) to pull it through (outside of rare stylistically-polished books like A Scanner Darkly).
There are definitely instances where books intended as machine-lit end up having well-developed plot and characters and a coherent and polished writing style (pretty much every Neal Stephenson book meets these criteria, as does Brave New World), but to some extent, doing so depends upon a kind of imagination and intellectual honesty that brings the book into the middle-ground between machine-lit and the world-building-based style of science fiction that I tend to champion, whose most extreme and visible example is seen in the post-Neuromancer works of William Gibson.
Another major failure mode of machine-lit is that, because of the dependence upon the central conceit of the book, if that conceit is uninteresting or unoriginal, the book as a whole fails along with it. With big-idea novels related to politics (Rand again) or philosophy (a handful of PKD books that lean too heavily on solipsism or philosophical zombies, and nearly every film adaptation of a PKD work), interest in these works falls evenly along either political-ideological or philosophical-education lines — a communist is, largely, going to find The Fountainhead or Anthem uninteresting; someone who is familiarenough with the idea of solipsism to find it fairly uninteresting will likewise find The Matrix uninteresting, while someone who rejects Serle’s Chinese Room paradox and the idea of philosophical zombies as based on an erroneous deification of consciousness will find the host of films about robots being incapable of emotion or of morality to be uninteresting. When the same idea is recycled into dozens of machine-lit works, the popularity of the idea itself can suffer, because while no longer wholly novel it will often be framed in similar ways, with similar changes based on the needs of the story or premise, by nearly identical stories (The Matrix has more in common with Simula-3 and its major film adaptations, World on a Wire and The Thirteenth Floor, than it does with Plato’s Allegory of the Cave, from which all of them were derived). Today, talking about solipsism will make people think of The Matrix rather than, say, Descartes’ “evil genius” — and despite my general feeling that The Meditations failed to be adequately convincing, we as a society are favoring an action franchise with major and obvious plotholes over a fairly heavily considered work by a brilliant philosopher.
Again, if a text develops its characters and plot adequately, the central conceit can essentially be ignored — a good ghost story is good even to people who don’t believe in ghosts, while a bad ghost story will fail to entertain enough to motivate people to suspend their disbelief.
Machine-lit shares with the rest of speculative fiction a basis in a counterfactual model of the world. That is to say, we start our world-building by setting some axioms that, in our world, are not true, and work from there. The difference is that machine-lit, by definition, performs the basic world building then immediately jumps to narrative, then stops as soon as something resembling a completed text is produced. Within world-building-based science fiction, a much more complex world is built, and the narrative and characters stem from that world organically.
This requires a dedication to completeness and intellectual honesty, in part because genuinely following the logical progression of the central mechanism of a counterfactual world can point out flaws in its structure.
In cryptography, the first and most important rule is never to roll your own crypto — always use a well-known and well-tested algorithm, at the very least, and ideally also use a well-known and well-tested implementation. The reason is that flaws are never intentionally introduced into crypto by people who want the crypto to succeed, and thus fatal flaws can only be identified by other people — and the more people there are looking for flaws in an algorithm, the faster such flaws are found (and the longer it takes to find fatal flaws in an algorithm, the more likely it is that such flaws are difficult to find). Everyone who designs crypto professionally is also skilled in trying to break crypto: you learn to avoid the flaws that you have discovered how to exploit. Likewise in computer security — the research arm of the computer security community consists of people who figure out how to break security and then figure out how to patch those holes.
In fact, this is a common pattern in legitimately serious enterprises. The scientific method is exactly this: suggest a model of the world, and then recruit people to attack it. The adversarial justice system is based on two groups of people presenting different models of the world and attacking each others’ models. Even in philosophy, philosophers engage in critiques of the ideas of other philosophers, rather than ignoring any idea they don’t agree with.
Any functional member of any of these communities will attempt, before putting their ideas out into the world, to stress-test them personally — formulate simple attacks, determine which portions of the idea are weak and whether they can be strengthened without complete restructuring.
Machine-lit, by and large, fails to perform these sanity checks. Machine-lit is the domain of people who are so in love with their ideas that they cannot bear to test their mettle before pushing them out into the world.
An ideology at the core of machine-lit, if properly investigated, would collapse upon itself or mutate such that it fails to be an ideology. A utopia at the core of machine lit would, upon close inspection, become a dystopia; a dystopia, upon close inspection, would yield some happy and fulfilled people, making the message of the book ambiguous. An actual machine at the core of machine-lit, if properly and rigorously tested, would become at worst a patent application but possibly an actual invention.
I’m perfectly in favor of optimism in science fiction. Nothing is to be gained from keeping the genre grimdark as a rule, in the same way that nothing is to be gained from keeping superhero movies grimdark. However, utopian science fiction represents a failure to take the medium seriously — and a shallow dystopia or cozy apocalypse is no better. Science fiction should be a genre of ideas, but there’s no point if we allow our ideological biases and our love of shiny toys to turn it into a genre of shallow ideas shielded from unforgiving reality. The real world has problems, and while escapism is fine, a work cannot simultaneously be an escapist fantasy and a serious analysis presenting a serious solution to the problems it fantasizes about escaping from.
Science fiction always starts as machine-lit. But, machine-lit is a larval stage that adult science fiction works outgrow.
This article was originally published at First Church of Space Jesus under the title “Utopianism and sci-fi as machine-lit”.
By Rococo Modem Basilisk on May 24, 2015.
The key thing here is not that this time is different than the 60s, or that the people writing about this problem in the 60s were wrong. The state of technological optimism about AI has fluctuated over the years, but has almost never corresponded well with the reality of AI advancement. The people talking about this in the 60s were right, aside from the timeframe — because what everyone in 1959 thought would be normal in 1965 actually happened in 1995 and became normal in 2005.
Labor upsets related to technological changes do occur. There’s no rule saying that employment rates *need* to return to normal, but at least in a capitalist society, there are forces that encourage it (and capitalism is all about this kind cybernetic feedback). When a job is genuinely completely automated in such a way that, outside of the novelty or nostalgia factor of having a human make it (artisanal production), automated production is always preferable in terms of both price and quality, we can consider that a ‘functional singularity’ of that job — the machine has taken over that job, and humans aren’t going to take it back. Functional singularities happen all the time, and aren’t new. The first industrial revolution spawned luddites in part because a lot of cottage industries were undergoing functional singularities — machines can weave ornately patterned textiles better than humans, so jaquard looms really did take over the extremely specialized and skilled job of expert weavers in that context. Functional singularities happen at a smaller scale all the time, as well — think of the invention of sliced bread — and they are often viewed in a positive light as labor-saving devices, because they often take over tasks that had already been shouldered by the consumer (or by the housewife, or by servants — think sliced bread again, and the dishwasher, the washing machine, and the microwave). Indeed, the very first computers represented a functional singularity of the job of the ‘computer’ — a single computing machine took over the jobs of twenty or thirty young women with mechanical desk calculators; electromechanical telephone exchanges did the same for operators (again typically young women).
Historically, a functional singularity has been considered a labor-saving advancement by people who value the labor in question moreso than the livelihood of the people performing the labor. This is purely a market-related concern — if these people could fall back on a living wage once their tasks were replaced, it wouldn’t really be a concern that their task had been automated (because if they were doing it for some reason other than the money — say, because they got satisfaction from the work — they could continue doing it despite it no longer being profitable).
It takes time for new types of labor to appear in the wake of a functional singularity. To the extent that the ability to perform a task is specialized and full of non-transferrable skills, retraining time is time when the people who were doing the tasks are unemployed. And, even if we assume that such a thing always does happen, the rate at which it happens is clearly not strongly associated with the rate at which jobs are fully automated or the amount of skill displaced. The industrial revolution is so-called because of the vast amount of labor that was displaced, much of it skilled labor — and the various labor movements and luddite-style anti-automation movements of the era are the kind of thing that happen when so much labor is displaced. It’s important to note that the labor movements spawned by the industrial revolution took about a hundred years to get to the point where they are today, and mostly haven’t progressed since — we’re talking about a time scale wherein, historically, most of the displaced people who were the impetus for reforms did not live long enough to benefit from them, and remained displaced or operating in a diminished capacity (say, formerly highly-skilled weavers operating in the unskilled and less lucrative position of pulling a lever) for the remainder of their lives. Neither comparable new fields of labor nor reforms for improving their quality of life caught up to them in time to make any difference — theirs is a story of a lost livelihood.
We really need to be putting these measures in place preemptively, because doing so in a reactionary way will doom another generation to that generation’s equivalent of shit jobs — if even that. Many skilled jobs are on the cusp of replacement, and the unskilled jobs of today (retail, fast food) are already beginning to be replaced. Just as lever-pulling unskilled jobs were not numerous enough to replace the influx of displaced skilled cottage industry workers during the industrial revolution, there simply isn’t room to make every laid-off knowledge worker into a fry cook or grocery bagger — so, without improving the social safety net, expect a large increase in the homeless-and-hungry population. In the end, it doesn’t much matter when exactly the day fully comes, because progress toward total automation is happening, and displacement rates are already higher than reskilling rates.
By Rococo Modem Basilisk on May 27, 2015.
It’s a mistake to call Uber a tech company.
It’s a mistake to call Uber a tech company. Uber is a taxi service that uses legal loopholes to avoid having to give its employees the various benefits legally granted to employees. As a result, democrats and leftists in general are against it, because the core of its business model is circumventing labor laws.
When Wal-Mart cleverly circumvents labor laws in order to deny rights to its employees, nobody claims that it’s justified by the technical leaps that Wal-Mart makes, but why? Wal-Mart has developed more new tech than Uber has. The answer is PR — Uber has decided to call itself a tech company, and thus, it has recieved the protection of tech company boosters.
When a genuinely innovative idea is put forth, there is no problem with regulation because regulation in that area doesn’t exist — the area doesn’t exist, so it’s a non-issue. Drones don’t meet this criteron — drones are an old technology, with early versions existing as far back as the 1940s, and they operate in a regulatory space that is largely well-developed. Uber doesn’t meet this critereon, because as we mentioned, it’s a taxi service.
By Rococo Modem Basilisk on May 29, 2015.
Myths of competence and specialization
An idea has been going around for a while that science fiction, more than anything, is a literature of competence — the protagonists of science fiction are competent people who can be trusted to do the right things under the circumstances (given their knowledge of the situation), and their mistakes can generally be traced back to withheld information or the effects of external forces that manipulate their mental state (like drugs or mind control). This is true of a lot of golden age science fiction (wherein, generally speaking, the protagonists were also respectable, if not amiable — think Asimov & Heinlein), and is generally less true of new wave science fiction (think of Ellison, wherein occasionally our protagonists are mad or naive or belong to a culture with alien values) and first-generation cyberpunk (think of Neuromancer, wherein every character who isn’t mad is varying degrees of self-loathing and self-destructive). But, a fiction of competence is also the lens through which many people see the real world — and some of them are probably drawn to golden-age science fiction for this reason.
I have a friend who is, like me, a software engineer. He clearly sees the world through this lens. He sees people as, generally speaking, professionals; what I consider to be design errors he considers to be some unfortunate but inevitable product of circumstance that must have very good and acceptable reasons behind it. He acknowledges the occasional genuinely poor decision, when it’s undeniable that there’s no good excuse for it, but he considers such things rare and rarely acknowledges poor decisions made by people he respects. When faced with a problem, he prefers to theorize about it rather than probe it experimentally, and is willing to spend more time generating an elaborate mental model of a problem than experimentally discovering its contours. In other words, he has confidence in the integrity of his mind and the minds of others, and considers the production of mental models to be a generally foolproof method for exploring the world.
Although I respect him a great deal, and although I admit that his knowledge of many fields is deeper than mine, I consider his attitude naively optimistic.
My model of the world is compatible with the rule of the blind idiot god. The universe is complex enough that few elements can be modeled perfectly by human beings. Because competence is difficult to achieve, few people achieve it — incompetence and poor decisions are the rule, rather than the exception. Furthermore, even competent people have little reason to exercise their competence — the illusion of competence is rewarded moreso than actual competence, and exercising one’s competence takes time and energy that pretending to exercise one’s competence does not — and society rewards behaviors that are incompatible with the production and maintenance of genuine competence.
Human beings tend to value confidence in themselves. I consider this a major failure. Because the world cannot be perfectly modeled, all models are by definition imperfect — and confidence is faith in the predictive success of one’s mental model for situations upon which it has not yet been tested. Confidence is valued in oneself in part because confidence (i.e., lack of hesitation) is valuable in genuine emergencies — if you are being chased by a bear, spending mental effort determining whether the bear genuinely exists or is an illusion produced by a trickster god is detrimental to your expected lifespan. Genuine emergencies are more rare now than they were when the adrenal and peripheral nervous system first developed in our distant forebears, and they are less important to the survival of our genetic line — we are more likely to fail to reproduce out of a bias against children or financial instability or a lack of attraction to the opposite sex than out of actually being killed by something we could run away from (like a bicycle, an enemy, or a wild animal); as a result, in today’s world, it is generally more risky to be sure than to be unsure. The same confidence in the correctness of your mental model of the world that will save you from a wild animal will get you run over by a truck, because change blindness is part of the same set of energy-saving heuristics that allow human beings to do things faster and with less effort by introducing errors into our models of the world; the same confidence that would allow a human being in a nomadic-band-of-hunter-gatherers situation to fight effectively against another band trying to use the same resources will lead a modern person to fight and die in a religious war.
Human beings also value confidence in leaders. This is for a similar reason — if you are in a nomadic band of fewer than 150 other people, and you are being attacked by another group of approximately the same size, your odds are about even so long as your hesitation level is about even, but lack of hesitation gives you a tiny advantage. Your leader, because he is in charge of coordinating tactics, is the bottleneck — his hesitation is your hesitation. This is the context where leaders are useful — when discounting planning time your odds are 50/50, but when every second of hesitation counts against you, fortune favors fools who rush in over the ones who consider the situation carefully. But, few genuinely important situations today depend upon split-second decision-making. Unless you’re in the military, your ability to make poor decisions quickly will never be more important to your lifespan than your ability to make good decisions (although the ability to make good decisions quickly is beneficial in a wide variety of situations, it’s not really practical to develop), and unless you play professional sports the same is true of your livelihood. A good leader in typical modern circumstances is someone who takes minutes or hours to think a decision through, and who knows when to back off and reconsider a decision that has proven to be flawed — in other words, exactly the kind of person who appears unconfident to the point of neurosis. Because our heuristics are stuck in the stone age, to become a leader you must appear confident, but in order to be a good leader your apparent confidence must be an illusion.
This is not to say that I don’t believe in competence. In fact, I think competence is undervalued and under-sold. Take, for instance, the polymath.
A lot of people these days say that polymaths can no longer exist — that the world has gotten too complex. Bullshit. Our models of the world have gotten better — which means that our ability to predict the world has gotten better. It’s easier to be a polymath today than ever before, because being a polymath means being competent in a variety of fields, and great strides have been made in every field with regard to our ability to learn to become competent in them. The world has not gotten more complex, but instead, through human endevours, it has gotten slightly simpler — not because we have changed the world but because we have changed our minds, developing mental tools for organizing the massive clusterfuck that is reality into more and more useful predictive models, wherein the complexity of the model grows slower than its predictive utility.
The same narrative that claims that there can be no more polymaths tells us that specialization is desirable, or at worst an unfortunate necessity. If we can’t learn a variety of mental models because the models have gotten more complex, then we need to stick to our lane and go deep into one silo, solving the problems that fit into that domain.
But, all problems are in reality multidisciplinary. Disciplines and problem domains are inventions of human beings, and reality has no interest in them. The specialist is blind to this. The specialist sees the portions of the problem that fall into his domain, and perhaps slightly foggily sees the portions that fall into neighbouring domains; the remainder is some vast undifferentiated miasma that must be left to other people to figure out. As a result, the specialist can be very confident about his results — because he has chopped off everything in the universe that he doesn’t know how to model, and has applied a model to the tiny portion that has been left over. His model may not yield useful results, because he has ignored most of the universe, and he really can’t effectively isolate his subject that way.
The generalist, on the other hand, sees the universe and applies several different models that apply to different aspects of the subject (as well as sections of the world immediately surrounding it). The polymath, who is a generalist upgraded with the knowledge of several specialists, does the same thing with better results because he has a wider variety of useful models and the experience to determine which models are appropriate. The polymath can do this because he realises that each specialized field is a pattern recognition machine, and because some patterns can be found in the world wherever you look, many disciplines have independently reinvented the same or very similar models with different terminology. He can combine the similar models to form superior hybrid models, and when the models are exactly the same he can learn the new terminology or use the shared model to synthesize its sister models across domains. And, since models build upon each other based on shared patterns, he can use models from one discipline to more efficiently learn models from another, unrelated discipline because they essentially accidentally share patterns. Because of the polymath’s wider scope, he also is aware of common failures in various forms of various models — he is aware that the failures can compound, and so despite having better predictive results at a lower cost, he also has lower confidence; he has eliminated the artificially inflated confidence of the specialist and is left with a level of confidence more appropriate to the actual situation.
I feel like this myth of competence and confidence — the Captain Kirk character voyaging into the unknown and believing that he already knows it, confidently applying human biases to non-human situations and considering himself to be morally superior to cultures that don’t share his values — is not merely naive and optimistic, but actually regressive and dangerous. Any confident leader and man of action can be percieved, with a minor shift of perspective, as an arrogant fool who acts without thinking; any crusade against evil people doing evil things can be reframed as an intolerant bigot battling a system of values he doesn’t understand. This kind of literature transplants into the space age the kind of leader who hasn’t really been appropriate for a leadership role since the dawn of agriculture.
By Rococo Modem Basilisk on May 29, 2015.
Another great music podcast (which may or may not be still running) is Solipsistic Nation.
Another great music podcast (which may or may not be still running) is Solipsistic Nation. It focuses on electronic music, and occasionally does shows focusing on particular labels or particular artists (for instance, it did a show with Black Moth Super Rainbow), but it also does nice theme shows (one theme show that I particularly liked was the “science fiction film soundtrack” show — wherein the host chose a playlist he felt would make an appropriate soundtrack for a Blade Runner-like cyberpunk film; but, he’s also had a mashup show and other more generic themes).
If your tastes run more to the obscure, I’d also recommend El Diabolik’s World of Psychotronic Soundtracks — a show focusing on music made for 60s and 70s european grindhouse films. A lot of really interesting stuff was being done by professional musicians in the 60s and 70s essentially as library music — in other words, these musicians would make tracks and shelve them, and then film-makers would buy a license to use them in films. There were some very interesting and influential acts that only ever worked in psychotronic music for films — for example, Goblin, a prog rock band who only ever made music for horror movies and whose music you will recognize if you’ve ever seen a Dario Argento film.
If your tastes run even more obscure, I’d recommend National Cynical Network  — essentially a mashup/theme show that’s an offshoot of the Church of the Subgenius (the same joke religion slash cult that gave us Devo, Pee Wee’s Funhouse, and the Slackware linux distribution). A typical episode will be somewhere between a Negativland-style sound collage and one of the more eclectic mashup artists (think Illuminoids or Niel Cicierega), and themes include “Teeth”, “Star Trek”, and “Songs about Mary”, in addition to having cover shows (the “Not Devo Show” consists entirely of Devo covers, grouped by original songs; the Pink Floyd episode has a mashup of covers of Dark Side of the Moon).
By Rococo Modem Basilisk on June 3, 2015.
While I’ve seen lots of snappy, short, intelligent, and witty pieces of academic writing, I feel like people who criticize academic writing are really complaining about several different tendencies that rarely occur in the same piece of writing, all of which occur more often in academic writing than in most other kinds.
One tendency is to use specialized jargon. This makes perfect sense — terminology is invented in order to quickly reference a nuanced idea, at the cost of being impenetrable to people unfamiliar with the nuanced idea (and thus unfamiliar with the term). Using jargon as a shibboleth is also not without merit — explaining the meaning of these terms requires explaining the ideas behind them, which takes time and energy & is a waste of effort when the readership is already familiar with them; excluding readers without the prerequisite background to understand nuanced points is perfectly reasonable, particularly when good resources exist to bring a general audience up to speed on the ideas and terms. People working in the depths are not necessarily the best popularizers, and they don’t need to be.
Another tendency is to draw arguments out, making explanations overly long. I feel like this comes out of either avoiding jargon or not having access to the appropriate jargon.
A third tendency bears superficial resemblance to the first two, and is genuine obscurantism. This is a lot less common in academic writing than a lot of people think; it’s pretty common in business.
A fourth tendency, related to tendency number one, is to mimic the formal structures common in one’s field even when they are inappropriate for the task at hand. I don’t think that extreme examples of this are very common — academics are human beings, too, and can tell when something really isn’t working. But, how egregious examples of this appear depends heavily upon how familiar with the common structures the reader is — even something almost universal, like the format of a scientific paper with its list of references at the end and its abstract, can seem strange and awkward to someone who has never read one before. Again, the skill-set for communicating with a general audience is different from the skill-set for communicating with academics in the same field, and while some people have both skill-sets, they do not necessarily write a single document for both audiences.
(It’s also useful to note that some of the most entertaining academic work plays with the jargon and the structure in use in the field in a playful and perverse way. Single-line published papers and theorems, for instance, do this and are sometimes extremely influential. But, these things are even less accessible to a general audience.)
By Rococo Modem Basilisk on June 9, 2015.
Interacting with Fiction
Interacting with Fiction
This essay may be disorganized. Treat it as a brain dump on the material, rather than a serious analysis.
I’d like to discuss a few different kinds of interactive fiction, coming from different traditions and with different attributes. I’d like to discuss how the forms themselves play with ideas about constraint and agency, and how treating them seriously might change the way we think about fiction and fictional worlds. I’d also like to discuss how each of these subverts certain ideas about interactive fiction taken from non-interactive fiction, and make connections between these forms and other related forms that I haven’t seen made due to accidents of history and geneology.
Dramatis Personae
I’d like to introduce our fictional forms, along with their attributes, an exemplar of each form, and a few other forms that bear similarities.
Classic IF: Also called the ‘text adventure’ genre, Classic IF (which I will use interchangably with ‘IF’ in this essay) is written fiction in the form of a computer program that can be interacted with via free-form text input. The exemplar I choose is Collosal Cave Adventure. Usually, when people talk about ‘interactive fiction’, they mean this. Most of the attributes of classic IF carry over into the ‘point and click adventure’ genre, because historically, most creators of point and click adventures started out in text adventures; I am treating the ability to click on any object in a crowded scene to be of the same class of player agency as free-form text input for the purposes of this essay and using IF to refer to both forms, for reasons that will become clear in the next section. Genre conventions in classic IF include difficult puzzles and a stance of habitual contempt for the player. Player habits developed by this form include exhaustive searches of possibility space (picking up all objects, trying all verbs, clicking everywhere on the screen).
Visual Novels: Also called ‘VNs’, visual novels consist of sequences of scenes interspersed with player choices. Visual novels differ from classic IF in that player choices are strictly limited — typically no more than four options are ever given, these options are clearly presented to the user (no free-form text input), and the options chosen almost always cause meaningful narrative changes. If classic IF has a maze structure, VNs have a tree structure. I’ve chosen as an exemplar of the form Everlasting Summer, because it’s free & contains many of the genre-typical attributes and features. Genre conventions include plotted routes based on romantic pairings (being associated romantically with a particular character will give you a very different sequence of choices and events than with another character) and framing devices involving time travel. Player habits include re-playing in order to play through all possible routes (or at least, get all possible endings). Many recent twine games are similar in structure to visual novels, and so I would classify them the same way; while some FMV games are best classified as part of the point and click adventure genre, many are better grouped with VNs.
Wiki-based Choose Your Own Adventure stories: While these are not typically considered in essays like this, I think they add several interesting dimensions of possibility. My chosen exemplar is the Infictive Research Wiki Adventure. Wiki adventures have a primary method of play similar to visual novels, but differ in that players can modify scenes and options.
Fan work: Here is where we get a bit meta. Fan work, also called doujinshi, is the blanket term for any creative work related to a franchise not made by the franchise license holders. If we include fanon in this definition, we can classify it as a genuine interaction with a static fictional world that can result in apparent mutations to that fictional world. My exemplar is the fan theories subreddit.
A note on our characters: I have avoided classifying the behemoth of triple-a games as part of interactive fiction because in modern high-budget games, gameplay mechanics and visual sophistication often take priority over storytelling, and to the extent that storytelling is done it is entirely non-interactive. Unless the player character can meaningfully change the story being told (in a more complex way than winning or losing) and the story being told takes a prominent role in the experience, I would not classify it as interactive fiction. As far as I’m aware, the only recent triple-a game franchise to meet these criteria as well as the least suitable VN has been Mass Effect; however, that franchise also struggled with a percieved betrayal of the fanbase’s expectation for meaningful interaction with the fictional world during the end of the final game. Because our focus is on agency and constraint in interactive storytelling, my position is that games that allow the player character free and detailed movement in 3d space (or indeed 2d space) are, generally speaking, providing levels of agency superfluous to the goal of storytelling and potentially directly counter to it. The fact that these games often mimic the styles of non-interactive forms of storytelling like film for their storytelling elements while having primary gameplay mechanics be of no use during designated storytelling portions indicates that storytelling and gameplay are considered to be separate domains potentially at odds in this kind of game, while the genres I am focusing on have gameplay elements that directly interact with the structure of narrative.
Agency and meta-agency
In classic IF, the player is in control of a player character. His control is, genrally speaking, limited to physics — he can control the player character’s geographical location in the game world, pick up and manipulate objects, and have limited interaction with characters, based on the limits of the command parser and the variety of interactions planned by the game designer. I call this physical and limited-conversational agency: the player can manipulate the physical state of the game and initiate pre-scripted entire conversations.
In a VN, the player is also in control of a player character. However, the player’s decisions are much more limited. Rather than being able to try whatever obscure sequence of words he can imagine, the set of possible options is laid out. The responsibility for enumerating the possibilities of the world has moved from player to developer, which makes for easier play — no rules are hidden. Classic IF will appear more mysterious than a VN of similar complexity, and it is possible to have options in a VN that in classic IF would make it unplayable because the player could not reasonably be expected to guess them. In both IF and VNs, the world is crystallized and all possible narrative paths through the world have been predetermined; however, in a VN, because of the requirement that these options be enumerated, we have limited the player’s agency to actions that have meaningful narrative effects. I call this narrative agency: the player’s actions directly select which path to take through the story tree.
In a wiki adventure, we have both narrative agency and meta-agency. A player can take whatever choices he likes, but can also create new narrative paths. The story is crystallized until the user decides to change it. Furthermore, there is a social element: stories are being mutated by a group, and feedback loops cause strange attractors in the group’s psychology to manifest in the fiction.
Finally, in fan work, we have only meta-agency. Fan work itself has no protagonist; the player navigates his own mental model of a narrative and creates new narratives from it. Once these narratives are released into the world they are crystallized; but, their mutability is ensured because new versions can be created by other fans. Occasionally, fan work creates a culture significantly divorced from the original and invents a very independent narrative universe, based more on trends and patterns in the fanbase than on any genuine attributes of the supposed source material — an extreme form of the feedback loops found in wiki adventure, generating narrative simulacra.
Completeness
A common habit of VN players is to get 100% completion — to visit all routes and view all possible outcomes. On one hand, this is a show of dedication, and an in-group signaling mechanism: VNs can be extremely long, so getting 100% completion is often time-consuming in addition to requiring some careful note-taking and book-keeping. Some VN engines include features to aid in keeping track of options and routes already taken, or features useful only on re-play (such as skipping over already-seen content). On the other hand, this kind of completionism is a godlike ability to model the entire work completely — akin to viewing every alternate timeline in a Burroughs-Wheeler MWI universe. This completionism is made possible by the enumeration of responses. It is not possible in classic IF, which can have a structure of similar complexity and choices of similar granularity, unless the player determines the set of all possible options and uses them at all possible points — and while engines that can recognize only expressions of the form <verb><noun> can be iterated over using all possible combinations of recognized verbs and nouns, some engines support more elaborate language constructs including embedding, which makes enumeration of all possible recognizable strings impossible.
However, our mutable forms (fan work and wiki adventures) are incompletable on yet another order of magnitude. They change along the axis of real time as well as fictional time. While you can take a snapshot of a wiki adventure at any given time and play it to 100% completion, it can be modified the next time you play it — at any point along its timeline. Fan work is even more extreme; by its nature it forks, so any given fanwork is at any given time geneologically connected to several others that differ and are themselves mutable in real time. Fan work is the most amorpous — combining the flexibility of language with mutation along time and geographic axes, yet still operating directly upon narrative without the use of a player-character intermediary. Nevertheless, fan work is a game — a game with no author and no end, created entirely by the players.
By Rococo Modem Basilisk on August 12, 2015.
Peter Watts and p-zombies
I was surprised, upon listening to a two part interview with Peter Watts, to find him tentatively supporting Chalmer’s positions on qualia and the hard problem. Part of the reason is that Watts is a(n ex-) scientist with a background in biology and neuroscience, and also both very intelligent and spectacularly good at not avoiding unpleasant trains of thought. The other reason I was surprised is that I read Blindsight, and interpreted it as an amazingly good takedown of the Chalmers philosophical zombie idea along the same lines as Dennett’s.
This essay will contain spoilers for Blindsight, probably. Also, spoilers for the epistemology of Chalmers and Dennett. If you don’t like to learn things in orders not officially sanctioned by the establishment, I recommend you at least read Blindsight — it’s a great read, and Watts has been nice enough to put it online for free.
Chalmers presents the idea of consciousness as indicated by qualia — a representation of the subjective feeling of the outside world. His position, in my understanding, is that subjective feeling is a more difficult thing to model than other properties of the world. While I’m not sure about Chalmers himself, other people have used this idea that qualia is a “hard problem” as an excuse for reintroducing cartesian dualism into the world of epistemology — by claiming that qualia is so difficult to model that not even straight-up neurons can model it, and thus we need to bring in quantum nanotubules or some other structure as a stand-in for the soul.
A lot of people have been suspicious of the idea of qualia. After all, isn’t a representation a representation? Isn’t a subjective representation just a second-order representation? I agree with Dennett when he argues that it’s an unnecessary complication, with no evidence for it. I would furthermore argue that it’s a matter of preferring a mysterious answer to a mysterious question: complex behavior can be difficult to predict not because it’s irreducible — not because each piece is complex — but because lots of simple pieces combine in a complex way, but there’s a general tendency among people to try to keep emotional parity with explanations (mysterious things need to be explained in a way that retains the mystery or else you’ve lost the mystery; negative events can’t be explained as an interaction between purely positive intentions, or else where did the negative essence come from?) but ultimately reality doesn’t deal in emotional valences and so feelings of mystery do not need to be conserved.
Chalmers came up with a fascinating thought experiment in order to “prove” the existence of qualia. He suggested the idea of a ‘philosophical zombie’: a person indistinguishable from a regular person, but without qualia. Because qualia cannot be tested for, this person would be completely indistinguishable from a regular person.
Somehow, a lot of otherwise intelligent people thought that this was a good argument. I can’t see the invisible dragon in my garage, and therefore it must exist.
In Blindsight, Watts plays with a few variations on the philosophical zombie idea. He puts forth the idea of vampires being said to lack qualia — along with other cognitive anomalies that are of benefit to a humanoid with a very different position in the food chain. Certain optical illusions and cognitive biases don’t work on them. They have some differences in social behavior. They are largely lacking in empathy, without having the problems with impulse control that tend to be comorbid with lack of empathy in human sociopaths. A vampire, along with a split-brain patient, a personality collective, a person with extreme sensory modifications, and some other various neurodivergents take a space trip to meet a colony of intelligent starfish/squid-like aliens that are determined to have no qualia either and no sense of identity.
But, the ideas about qualia don’t line up here. I assumed it was on purpose.
Rather than ‘qualia’, each of these neurodivergent characters has some facility or attribute missing or strongly modified that is very clearly defined and very clearly not the same as qualia. And furthermore, each of these characters has very different behaviors based on their divergence from the norm. (This is along the same lines as the Rifters trilogy, particularly Starfish — we’re basically talking about circumstances where people who are psychologically and neurologically maladapted to normal life in a normal society end up being very well adapted to a fundamentally different environment.)
In other words, it’s a strong argument against philosophical zombies.
In the end of Blindsight, our protagonist gets back within radio range of Earth and can tell it’s been taken over by the vampires. Because Earth had stopped broadcasting music and entertainment, in favor of utilitarian communications. The vampires aren’t philosophical zombies, because they can be distinguished from humans. Because the particular kinds of things that they don’t experience lead them to live in a more utilitarian manner.
Indeed, no novel could deal with philosophical zombies. Because, by definition, philosophical zombies could not be distinguished from normal people. A novel about philosophical zombies could not be distinguished from a novel with no philosophical zombies in it.
Now, the argument for qualia is that, while human beings can experience something through their senses (like the color green), that experience cannot be identified in the brain itself. There is no neuron for ‘green’, and even if there was, the neuron itself wouldn’t be ‘green’ or contain the concept of ‘green’.
This argument has a handful of big flaws, some of which have been dissected elsewhere, so I’m going to dispatch it as efficiently as possible. First off, while some things do seem to have dedicated neurons (this is the ‘Grandmother Neuron’ model), most things don’t — however, this is not terribly unusual; we are very accustomed to another system for modeling the world where some configurations of state have single symbols and others have sets of meaningfully interconnected symbols: language. The word ‘green’ is not necessarily green — in fact, it might be red — and does not contain the concept of green, but instead gains its meaning from its relationship to other things. Ultimately, we can say that it gains real meaning by being in a relationship with other symbols in a manner that represents some configuration of the outside world as perceived through some people’s sensory apparatus, and gains utility insomuch as it allows us to communicate and make predictions. However, we can have syntactically meaningful configurations of symbols that could not have any semantic meaning — the colorless green ideas sleep furiously — or syntactically and semantically meaningful configurations of symbols that could not represent our universe — maxwell’s demon mounted the pink unicorn’s dragon-skin saddle and rode off at six times the speed of light in order to find some anti-entropic material and transmogrify it into orgone. Since language does this, there’s no reason for the brain to be incapable of it; since the brain makes language, the brain must be capable of doing it. It’s also not mysterious — even toy languages with heavily simplified grammars designed for computers to manipulate can do thing kind of thing (think RDF, or PROLOG).
As someone who has a background in biology and neurology, who works with words and language professionally, and who thinks deeply and clearly about most things, I would expect Watts to make these same judgments. If he has a counterargument in favor of qualia, I’d like to hear it. But, my general position is that to the extent that something that behaves similar to qualia exists, it is symbol manipulation, and to the degree that something like consciousness exists, it is something like self-simulation.
(Originally posted here)
By Rococo Modem Basilisk on August 20, 2015.
I’m not sure I buy that this is the result of increased usability.
I’m not sure I buy that this is the result of increased usability. Kids learned BASIC by trial and error without consulting the manuals on 80s home computers. The learning curve is lower if literacy is not a prerequisite, which makes the rate at which motivated children learn seem more striking.
By Rococo Modem Basilisk on September 9, 2015.
The origin of many of our most elaborate models of corporatism, advertising, and how these things infiltrate culture is — ironically enough — also the proximate origin of the very practices that theme parks embody: the situationist movement in Paris. They were equally concerned with modeling advertising and its subversion and with imagining a future urbanism where a post-scarcity city would see its primary goal as providing citizens with interesting and entertaining experiences.
The situationists would say that we can only ever win temporarily: the spectacle sees those things that subvert it, consumes them, and allows the defanged and sanitized symbols of that very subversion to become a part of itself. Just as punk was stripped of its ethos, just as Apple took an anti-consumerist minimalism and turned it into a reason to buy more things, Disney and its cohorts will find anything that opposes them and wear its skin. Nevertheless, we can continue to subvert. The situationists were marxists — they believed that capitalism would collapse under its own weight, and (presaging accelerationism) believed that constant subversion would quicken the fall of the spectacle by bloating it.
By Rococo Modem Basilisk on September 15, 2015.
Alternatives to advertising
Thinkpieces about the ethics of ad blocking are all over the news recently, because apparently things only become newsworthy when Apple stops banning them. The time to discuss ad-blocking is not now, really — after all, the largest tech companies make all their money from advertising now. The time to discuss ad-blocking was 1994, when the first banner ad was introduced.
That said, there are new (or at least new-ish) things to say about alternatives to ad-based monetization on the web, in part because during the past few years alternatives have been successfully implemented, and in part because intelligent people like Jaron Lanier have been writing at length about possible alternatives recently.
If you’re reading this, you — like most people — have probably heard the idea that advertising is justified as the sole alternative to paywalls and merchandise sales, and swallowed it completely. You probably didn’t quite realize that Kickstarter and Patreon were genuine alternatives to an ad-based revenue model. Allow this post to be an introduction to the variety of ways in which you can distribute media for free and still get paid.
Why advertising is a bad model
Some reasons why advertising on the web is bad are probably familiar to you: targeted advertising implies tracking, which eats up bandwidth and is a potential violation of privacy; advertisements are typically both irritating and irrelevant. Other reasons will be familiar to people who have hosted ads: click-through rates are incredibly low and ads have become devalued over the past ten years such that providers like Google pay fractions of a cent per click and nothing per exposure, meaning that only extremely popular sites can make more than pocket change through ads; even ads hosted through big providers like Google can be full of malware. The big one is the one you haven’t heard of, though: ads don’t work. Depending upon advertising as the basis of the internet’s economy is like tying the value of paper money to tulip bulbs during the height of the Dutch tulip bubble; sooner or later the entire system will become devalued.
A selection of alternatives
This is, obviously, not a complete list. I’m going to address the most obvious and frequently-cited ones first.
• Subscriptions: rather than releasing your content for free, you charge for access. This monetization policy is very vulnerable to piracy — if there’s an alternative source, there’s no incentive to pay. It has low discoverability — if people can’t see your content without subscribing, they have no incentive to subscribe. It’s also not particularly sustainable unless you have a lot of high-quality content being released steadily — if the quality is too low, you lose subscribers; if the content isn’t released steadily enough or isn’t voluminous enough, subscribers feel cheated. • Paywalls: an attempt to solve the discoverability problem with subscriptions by releasing a certain amount for free. Paywalls are often easily circumvented in such a way that a subscription is never necessary. Furthermore, while discoverability is improved, the average quality of content must be high if the content released for free is to convince people to pay. Finally, and most damning: paywalls don’t even attempt to solve any of the problems of subscriptions other than discoverability. • Freemium: while most content is free, a subscription system exists that provides users with either extra content or extra functionality. Freemium systems walk a delicate balance: if the free version is too functional, nobody subscribes; if the free version is too incomplete, it’s considered crippleware (and somebody else with lower costs will undercut you). • Donations: fans pay on a fully voluntary basis while content is released for free. This is very sensitive to the size of a fan-base and to how much that fan-base feels like the creator is ‘in need’ — a popular franchise with a creator perceived to be wealthy will not get donations. • Merchandise: items are sold at a high markup with images tied into the media in question. This is essentially hiding a donation inside a purchase. Because of this, it’s easy for people to undercut your prices with merchandise similar to your own and take advantage of any fans who don’t realize that their purchase is a donation. Furthermore, the primary benefit over straight donation is that fans can advertise their association with the media in question — which requires a cohesive fan-base and a cohesive set of evocative symbols to make it profitable. A popular franchise without a fanbase that sees itself as set apart from the rest of the culture cannot reasonably benefit from merchandise, nor can a popular franchise with insufficiently iconic symbols. • Crowdfunding: fans donate money to pay for the creation of media not yet created, often with merchandise thrown in to encourage larger donations. Crowdfunding is sometimes extremely effective; however, franchises with large and cohesive existing fan-bases benefit more from it than others. Discoverability is low. Repeatability is low, since repeated fundraising runs will tire fans, meaning that this is ideal for one-off new additions to existing franchises. Generally, if merchandise works for you and you need to raise money from your existing fans for something big and expensive and non-repeating like a movie or a tour, crowdfunding will be ideal. • The Street-performer protocol: you create something and then use a crowdfunding-style fundraising round to finance it before releasing it. On the surface this looks strictly worse than crowdfunding; however, because the thing is already created, you can avoid the common crowdfunding pitfall wherein time estimates for completion are under-estimated and the product that was funded was never created. Furthermore, quite explicitly, in the street performer protocol, the resulting media is released free to everyone as soon as it is paid for; as a result, it’s highly piracy-resistant: nobody has a copy before it’s paid for except the creator, and afterwards everybody does. During the first iteration, discoverability can be a problem; however, discoverability becomes less of a problem the more iterations you go through. Because this was Bruce Schneier’s idea, there’s some mathematical formalism and technical detail to how trust is established between various parties, which I won’t discuss here. • Patreon-style funding: something akin to a marriage between the street-performer protocol and a subscription system. In patreon’s model, fans subscribe to a creator such that they pay a certain amount automatically upon the public/free release of a piece of media. Unlike the street performer protocol, the media isn’t created before people agree to fund it. Patreon provides a mechanism for incentivizing higher donations by allowing various donation tiers access to specific content not accessible to other tiers, but such content tends to be rough drafts and notes — in other words, content mostly interesting to hard-core fans who might donate anyhow, thus circumventing problems with a ‘freemium’ model. • Decision bidding: fans can pay for the ability to influence the end product. Occasionally this is integrated with another model — I’ve seen limited decision bidding as an element in patreon and kickstarter campaigns. However, by treating certain classes of decisions (like what to blog about) as auctions, you gain the possibility of bidding wars. This probably will only work if you have a very dedicated fanbase. • Decision futures: fans wager on decisions and in the process make those decisions. You can treat this as a variation of decision bidding, or an extension of the kind of decision-voting that blogs sometimes have (polls about which topic should be covered next, for instance). Fans pay some small amount to vote, and those who didn’t win get a refund. You can increase fairness and revenues by allowing multiple votes up to some limit. • Pay-for-privacy: in a service that remotely hosts user content, public hosting is free while private hosting costs money. Github uses this model. • Pay-to-remix: release media for free, but expect people who reuse or recontextualize it to send you some money. This is proposed in some versions of transcopyright, and implemented in some demo-scale systems like token-coin; it is also an implicit part of music industry licensing, in which artists pay a flat fee to release a cover song on a record, and in which writers and composers are paid royalties on their work regardless of who performs it.
By Rococo Modem Basilisk on September 18, 2015.
I’d be interested in seeing how ebook sales trends match up with the shift from epaper to OLED/LCD displays on ebook readers. After all, the first few generations of Kindles and Nooks had epaper display — which in addition to having good battery life, mimics the properties of a paper book quite well; current generations (particularly post-ipad) have luminescent displays with color and faster refresh rates and are essentially just tablets. E-paper displays, because they lack back-lighting, are ideal for before-bed reading; because of their slow refresh rate, they aren’t ideal for anything *other* than reading full pages of text. By introducing tablet-style backlit displays, the new dedicated e-readers may be driving people who would otherwise use them for before-bed reading back to paper, while pushing the rest of their users toward non-book content.
(I’m also wondering if these trends you’ve mentioned extend outside of the Amazon-and-BN for-profit-ebook universe. Do the same trends exist in, say, download rates for epub files on Project Gutenberg? What about archive.org, or the pirate bay? It may be that prices or circumstances are lowering the rates at which people download ebooks that they need to pay for — and a variety of things could cause this, ranging from DRM policies to perceived reliability of the devices. After all, if you own a Kindle and all the books you own exist only on it with no capacity for backup, then bricking it would mean losing your entire library; it then makes more sense to buy the books on paper or to pirate them, since in the former circumstance it’s more difficult to lose all of them and in the latter circumstance it’s not difficult to back them up or get them again later.)
By Rococo Modem Basilisk on September 28, 2015.
A big problem here is that advertising — particularly advertising on the internet (by which I mean both web advertising and email advertising) — has already poisoned the well. The ideas you propose about vetting ads, about keeping up quality standards for types of ads, and about trying to ensure appropriate targeting are old ideas and already widely adopted — but each of them, over time, has been subverted and worn down, because advertising (since its effectiveness per impression is exceedingly low in the best of cases and almost nil in the average case) almost always becomes a race to the bottom.
Google’s entire core business model is the automatic targeting of ads along with strong quality restrictions; adsense is the biggest web ad provider in the world, and doesn’t allow advertising with any of the qualities you mentioned under a metric for bad ads. Nevertheless, outside of AdBlockPlus having an option to avoid blocking only google-hosted plain-text ads, most ad blockers still block everything coming out of adsense.
The suggestion that users will interact with a piece of advertising to indicate its quality and relevance is also pretty questionable. After all, all adsense-served ads have a button for reporting low-quality or non-relevant ads. But, years of dealing with ads that claim to have such buttons but who actually use them to steal metrics and redirect users to different ads have trained users not to trust anything put onto a page by a third party.
Even if large advertising organizations were to sign on for the kinds of restrictions you propose (and recall that Google already essentially implements all of them), that won’t really discourage ad blocking. Well-behaved ads aren’t a problem, and people who host ill-behaved ads are doing so knowingly and intentionally, because they don’t care about user experience. You can’t police them, because they’re already too shady to care about what you think and aren’t afraid to ruin advertising for everyone; they are the people hosting fake versions of the facebook login page and redirecting you to twelve different ‘around the web’ links in frames whenever you click ‘sign in’. They’re the people who are buying zero days from the russian mafia and selling them to the NSA at a 700% markup. They’re the reason that you can’t use just one ad blocker; in order to block enough ads to matter, you need to run three different ones plus an anti-tracking extension, and maintain separate whitelists for each. When advertising as a revenue model is dead, they won’t care because they will have moved on to killing kittens and selling their pelts as synthetic wool.
The biggest issue with this article is not the argument that advertising can be saved; after all, a lot of smart people have thought that in the past, and the things you propose to save it have been proposed and implemented at scale by people who knew what they were doing. Instead, the biggest issue is the false dichotomy between advertising and paywalls. Paywalls are one of maybe ten or twelve different alternative monetization strategies, and they have a bad reputation for a good reason. I recommend you do a little bit of research into the variety of alternatives. Ad hosting and paywalls are old models and remain dominant because of inertia, but they are poor solutions in terms of effectiveness, returns, and increasing customer confidence.
By Rococo Modem Basilisk on September 29, 2015.
Gelman’s characterization of his opposition is more than a little reductive.
Gelman’s characterization of his opposition is more than a little reductive. In brownian motion, the particles are wholly passive. However, the varieties of stimuli that are being studied with respect to their effect on human behavior are, in fact, primarily human-produced and can be modeled as a form of communication wherein ideas are spread subtly without analysis. It’s not controversial that this kind of communication (and this kind of spread) occurs within media, wherein we have dated archives showing various elements (arguably without rational basis) growing and shrinking in popularity in accordance with power laws; why should we believe that people do not communicate this way when they are not being recorded, or when they are outside the silos of media production?
By Rococo Modem Basilisk on October 6, 2015.
The hidden benefits of NaNoGenMo
On November 1st of 2015, NaNoGenMo begins its third year. It’ll be the third year that I’ve participated, and the third year that it’s spawned articles in legitimate paper magazines and newspapers, none of which are, unfortunately, particularly distinct from the coverage of the story generator Brutus in 1999 in the New York Times or similar projects from years prior.
Media coverage seems to circle around the spectre of wholesale automation of authorship the way that hapless space-ships circle around a black hole. (Because we, as a community, have an interest in corpora — the ability to access and analyse data makes inserting variety into generative writing convenient — we have kept track of these articles.) Perhaps, being written by journalists, these articles are justified in having a bit of a hysterical bias. After all, certain classes of news stories are already being written mostly by software, and fear-mongering about automation has been a lucrative staple of the press since the invention of the automatic loom.
However, despite its universality, the narrative of automation of authorship is a poor lens with which to look at the current state of generative text. Some forms of journalism are trivially automated, and these are precisely the kinds that are automated. However, the variety of journalism that journalists are increasingly reaching for (beautiful, literary longform nonfiction, which thrives on the web because of the comparatively low cost of distribution and which stands out heavily from the landscape of low-quality under-considered short posts) is both far from the grasp of the current generation of text generators and far from the aim of NaNoGenMo in specific.
NaNoGenMo produces, quite consistency, a flurry of extreme creativity and a wide variety of aims, styles, and implementation techniques; many people start several entries with extremely different approaches and goals, and many people do not end up producing a novel-length text despite the utterly trivial requirements, because their amazingly creative techniques were unable to produce a novel whose originality they could be proud of. In its first year, we had (among other entries) a novel composed of a supercut of similar tweets, a novel composed of a supercut of Homeric fight scenes, and a mystery novel composed of the belabored meanderings of Alice and Bob in a labyrinthine house; in its second year, we got a deeply atmospheric comic book composed by pulling images from flickr, post-processing them, and superimposing thematically related lines from detective novels, as well as a wonderful book-length piece of asemic writing. This year — who knows?
Explicitly experimental techniques appear to produce the best results. This makes sense — experimental techniques tend to be very well-defined, and the results of experimental techniques in literature as executed by human beings tend to be dominated by the attributes of the techniques themselves (meaning that, were a machine to execute those same techniques, the results would be superficially very similar). We haven’t progressed to a level of understanding of the craft of writing that allows us to automate good, readable, page-turning fiction — and I doubt that even an author of best-selling potboilers has such an explicit model. As a result, our community is less Clairion and more Oulipo. Nevertheless, each year, we produce works that inch closer and closer to readable. We produce vast novelty with the (eventual) aim of mundane novelty.
As a result, it may be most sensible to consider NaNoGenMo to be an amateur expedition into the greater control and quantification of literature.
The Royal Society of London in the 17th and 18th centuries independently rediscovered many of the things already known to professional craftsmen of the physical sciences like doctors, midwives, miners, and sailors; nevertheless, by aiming to measure and control their experiments, they became the vanguard of systematic knowledge of the physical world, which made later developments easier to isolate and demonstrate. NaNoGenMo can be seen as doing the same for the craft of literature: by producing machinery that consistently executes particular literary techniques, we can produce large amounts of stylistically consistent text; we can perform systematic mutations of text; we can isolate important elements by seeing how text affects people with a level of purity and consistency and volume not possible with human-written text.
We are also holding engineering discussions about things like plot and style. We’re determining whether, given some N major plot events, any ordering of those N events can be made sensible via transitions. We’re determining whether macro-level plot beats produce a greater impression of novelty than sentence- and paragraph-level variation in structure, and whether either of them produce a greater impression of novelty in human readers than variation in word frequency. We’re trying to figure out how much novelty is not enough and how much is too much in the context of texts of different lengths. We’re talking about how to engineer the eliza effect in readers. We’re figuring out whether or not readers can identify descriptive fluff, and whether or not they care — and whether or not Ray Chandler was lying about how he structured detective novels, whether the Hero’s Journey really is too vague, and whether the beats in Save the Cat can truly produce compelling stories with minute-by-minute granularity at a feature-film scale.
You can make the argument that this will eventually lead to the possibility of fully automated journalism. But, being possible is not a particularly compelling argument for it to be likely. Even relatively crappy chess computers can now completely outclass chess grand-masters — and so we have augmented chess, wherein a human grand-master works hand in hand with a chess-playing machine to play against another grand-master-and-machine tag-team. Items like furniture are much better manufactured by machine (in terms of quality, price, and environmental impact), and yet we pay much more for artisanal furniture made by obsolete and wasteful processes because we value the idea of a human being doing something that doesn’t need to be done and doing it the hard way.
In the same way that there is a market for artisanal wicker chairs and artisanal bread, there will probably continue to be a market for artisanal journalism — and for the rest of us, human journalists may become symbiotes joined at the hip with machines that automate the less interesting parts of the job. We already have some such mechanisms — spell check, grammar check, layout tools, note-taking and mind-mapping and automatic summarization tools. Tools for augmenting creativity in authors aren’t new either — cut-ups pre-date digital computers, as do markov chains and bibliomancy, and oblique strategies are now half a century old. Cutups and markov chains actually produce text for you, to which you must act as editor; but all these ‘writing machines’ that augment creativity do so by acting as a source of semantic randomness, much as mind-warping drugs do. We accept the use of all these mechanisms already — we don’t criticize Thom Yorke or William S. Burroughs for using cut-ups any moreso than we criticize John Lennon or Hunter S. Thomson for using LSD. Automatic methods for the production of text will, if they gain acceptance among writers, gain as much acceptance among readers as spell check and LSD.
NaNoGenMo probably won’t produce the future journalism-symbiote I describe, in the same way that NaNoWriMo has never produced the great american novel; but, just as NaNoWriMo produces novelists (and published novels), NaNoGenMo will produce some of the figures and technologies and domains of collective knowledge and culture that will inform text generation in creative fiction in the near future.
By Rococo Modem Basilisk on October 8, 2015.
To be honest, the closest thing to an interest feed as you describe it is tumblr — not because it doesn’t try to be a ‘social network’ but because it fails so hard at being a bidirectional communication medium.
Like twitter, tumblr has asymmetric following and a reblogging feature that constitutes most of the content. Unlike twitter, most active users keep separate accounts for separate topics. While you are still *following* *accounts*, it’s more like following one person or group’s curation of a topic than it is like following a person, and some portion of the network actually only pays attention to tags (meaning that they don’t follow anyone and don’t look at their feed, but instead watch a topic-centered feed via the search function). So it’s twitter without character limits.
By Rococo Modem Basilisk on October 9, 2015.
The formula is stupid, simple, and thus far pretty reliably successful as far as literally anything in the domain of startups goes: take an existing service, fire all the employees and replace them with untrained contract workers, spend an afternoon on a smartphone app or mobile site, and call yourself a tech company. Since the tech involved is extremely simple — your fourteen year old nephew can do it after school — this is sort of a brave re-branding. But, non-tech companies in the valley don’t get ten billion dollar valuations, do they? And if you have a ten billion dollar valuation, who cares if you make a profit?
By Rococo Modem Basilisk on October 21, 2015.
But, again, what reason do we have to believe that other humans have interiority if we ignore the indications of interior life that dogs share with humans? Humans produce more complex communications, but those communications are not necessarily better explained by interiority than by mere generative complexity.
This is not to say that I don’t believe humans feel pain, but instead, to say that if *you* believe that humans other than yourself feel pain then you should probably also believe that dogs feel pain, since there is roughly equal evidence; likewise, with intentionality, we can estimate it by looking at planning effectiveness. The closer a set of behaviors is to being the ideal path toward some goal, the more likely it is that those behaviors were planned by a goal-persuing system. Proving intentionality with a 100% success rate is not possible, just as proving interiority with a 100% success rate isn’t possible: a system with strong intentionality that is working off flawed axioms or flawed data or that is very limited in how many steps it can plan ahead will be almost indistiguishable from a system that performs behaviors semi-randomly based on simple rules without memory. But, communication is neither necessary nor sufficient to prove either interiority or intentionality: after all, limping and yelping are, effectively, communication insomuch as they are behaviors that provide information to humans and other animals about how likely the dog is to be in pain, and yet we can ignore those signals based on the assumption that all attribution of inner state to non-human animals is pure anthropomorphism.
By Rococo Modem Basilisk on November 9, 2015.
You’re complaining that a free VR device came with your newspaper, even though by no means do you need to use it in order to gain the same enjoyment you usually do out of your newspaper, because it reminded you that computers exist? I hate to think of what will happen when you discover that the New York Times has had a tech section for thirty years.
By Rococo Modem Basilisk on November 10, 2015.
I’m not sure that decision anxiety is the sole or even primary source of ‘prestige television’.
I’m not sure that decision anxiety is the sole or even primary source of ‘prestige television’. After all, a shift toward a preference for binge-watching (and the type of show that gets better when marathonned rather than getting worse) dates to the first instances of widespread time-shifted home viewing. Buffy replaced Kolchak in the public consciousness because Kolchak was too formulaic to watch in season-long chunks and networks wanted to hype new seasons with marathon reruns of previous seasons — and because Buffy was getting released on DVD. Netflix and other streaming services upped the ante in a UI way, but not via poor UI design: Netflix doesn’t update their catalog every time an episode comes out but instead every time a season comes out (sometimes with a multi-year delay), so binge-watching is the only way to watch that doesn’t involve either using another service that updates faster or forcing yourself to keep to a weekly schedule; however, unlike buying a DVD boxed set, queuing up a series on Netflix doesn’t cost any more than not doing so. Thus, a mainstream audience learned what the anime bootleg fansub community learned ten years earlier: when you can watch a whole season of a show in one sitting with no penalty for dropping it in the middle, doing so is for a large subset of shows — those shows that gain rewatch value by focusing on complex, intricate, and detailed plots and character arcs rather than going for a casual prime-time-TV audience with formulaic structures, running gags, and ripped-from-the-headlines topicality — will be far more enjoyable than watching an episode a week.
You’re absolutely right about Netflix’s UI, though. It’s awful.
By Rococo Modem Basilisk on November 19, 2015.
This is not ‘no UI’ — it’s ‘text-based UI’.
This is not ‘no UI’ — it’s ‘text-based UI’. Human written language is a set of technologies that have been under active, heavy development for four thousand years — compared to GUIs, which were under active and heavy development at Xerox PARC for about five years and have barely changed since. I agree that text-based UIs are the future — the competent ones among us never switched away from them in the first place, because they have features and nuances that GUIs would not be able to compete with even had they been truly under active development for the forty years of their existence.
However, this doesn’t mean that traditional UI concerns are irrelevant to text-based UIs, or that there are no special UI concerns to consider. It’s very easy to screw up a text-based UI — compare the MS-DOS command shell with a modern UNIX shell like zsh, and the vast gulfs of difference are obvious; even so, MS-DOS is by far not the least competent text-based UI in existence.
Conversational interfaces, by catering almost exclusively to new users and eschewing efficiency, nuance, and a rich feature set in favor of a shallow learning curve, are almost universally unusable for genuinely complex real-world tasks and gain their popularity from the novelty element of a computer behaving like a person. A well-designed conversational interface would eventually, in the hands of a habitual user who has become comfortable with it, resemble a traditional command-line interface: it would efficiently execute unambiguous and richly expressive queries while making use of shortened mnemonic forms of those queries. However, the novelty of a conversational interface to a new user depends upon a cynical assumption about that user’s willingness and ability to learn new skills: the conversational interface must accommodate a user who knows nothing and has made no effort to learn, and to interpret ambiguous requests as their simplest possible evaluation — to complain about the ambiguity or request it to be resolved would be to suggest the user learn a programming language and to interpret any resolution other than the simplest would be to suggest that the system is either unreliable (like a human) or sensitive to subtle details (like a programming language). Assuming users are stupid and unwilling to learn how to perform simple tasks is part of good UI design in GUIs, but it works against you even moreso in text-based interfaces; conversational interfaces that do not take advantages of UI advances in command line interface design can never become any more than toys.
By Rococo Modem Basilisk on November 19, 2015.
Novel UI systems have a lot of the same problems as other novel pieces of media.
Novel UI systems have a lot of the same problems as other novel pieces of media. The existing widgets, even when they are objectively terrible, represent a familiar visual language to users — in the same way that samey blockbusters combine familiar franchises with familiar special effects, familiar story structures, familiar characterization cliches, and familiar forms of cinematography for an ultimately conservative work that is unlikely to lose money, and in the same way that bubblegum pop combines familiar artists with familiar rhythms and familiar harmonies to produce minor variations on a familiar style that is unlikely to be rejected for being too extreme. When a UI is big business and lots of money rides on it being usable for a wide audience, that UI will be conservative, and in the rare exceptions to that rule often we end up with something universally reviled (Windows 8, the MS Word ‘ribbon’ interface, Microsoft Bob, the Macintosh finder’s ‘galaxy mode’ from the early 90s, Google Wave, literally every UI change on Facebook or Tumblr). Because UI is a form of creative media that people have to use every day — an experimental UI for an important and widespread product is like having Steve Reich and Daphne Oram compose the soundtrack for a large supermarket chain.
However, since the situation is comparable to more traditional forms of media, we can borrow a bit of media’s solution. The solution to Hollywood sequelitis and horrible blockbusters is the slow and careful importing of ideas, techniques, and talents from experimental film, just as visually striking import giallo and the rise of auteur directors in the 60s and 70s revived the 50s slump in creative moviemaking (along with big names like Hitchcock grabbing talent and ideas from other fields like experimental animation — compare North By Northwest’s cinematography with that of Vertigo), and just as the constrained compositional environment of punk revived stagnant arena-rock (itself descended from blues- and folk-derived attempts to use experimental electronic hardware to inject vitality into overly commercial 50s rock/r&b, itself an attempt to bring in new influences to revive a stagnant crooner-centric ecosystem, etc., going back long before Mozart). And, we certainly used to do this: PARC was so influential because they were an isolated group of geniuses quickly iterating on UI design. To some extent, we still do this: video games often have unusual UI ideas, most of which don’t work, and Alan Kay is still playing with UI innovation with Squeak. The thing is, for the most part, fringe UI designs haven’t gotten pulled into the mainstream since the early 80s, when “GUI” became synonymous with “Alto clone UI” and anything that didn’t use a mouse began to be considered as not ultimately related to UI.
Some UI experimentation continues, here and there. The biggest example of UI experimentation being pulled into the mainstream in the past five years or so is, shockingly, in text-based UIs, wherein conversational user interfaces based around messaging systems have begun approximating the kinds of features and nuances that command line interfaces in the UNIX world have shipped with since the late 70s. SIRI is a cross between Eliza and Autocorrect, trying blindly to approximate a cross between ZSH and Google while remaining fixated upon being accessible to new users, and it’s an interesting enough experiment; special-purpose slack and twitter bots are more innovative because the cost of failure is low.
My recommendation: release smart people in the UX field into a situation where they can iterate quickly on interesting projects of their choosing, none of which will ever be marketed. Then, get a couple canny con-artist types like Steve Jobs to drop in, take a look at the end result of five or ten years of iteration, and distribute a simplified version to the mainstream.
By Rococo Modem Basilisk on November 30, 2015.
The thing about cultural niches is that they are created by and for ‘mass media’ as a novelty generator. Having an isolated group of people experimenting on the fringes of culture without the necessity of appealing to large numbers of people or dealing with expensive equipment means creating an alternate media universe with new and interesting ideas and techniques each with its own associated small audience, and cultural niches can have their ideas and parts of their audiences imported into the mainstream on a temporary basis in order to keep the perception of the mainstream ‘new’: whenever something gets imported, it seems alien and novel to mainstream viewers precisely because they have historically been unaware of or isolated from the cultural niche in which it is considered an inevitable and incremental step. By importing proven ideas from fringe groups, mainstream media creators get the best of both worlds: they can seem creative while ultimately being extremely conservative with investor money.
This particular way of looking at niche media isn’t new; at the latest, it dates back to the french situationist movement of the 50s and 60s (and specifically to The Society of the Spectacle, in which it is a central theme), although it’s probably older. I’m going to suggest that — at least since the 60s but probably starting earlier — this has been an explicit part of planning, rather than (as Debord suggests) a fully autonomous process inherent in appropriative capitalist media structures.
Niches come in a hierarchy of sizes, and these sizes are related to cost and audience — two factors that were a lot more closely correlated before widespread internet access made the marginal cost of expanded distribution drop precipitously. Nevertheless, distribution is not wholly free: distribution on ‘mainstream’ scales is expensive even on the internet, and while it’s cheaper than television and newspapers, it still justifies (for instance) the internet advertising ecosystem. Furthermore, there are often production costs that are essentially ultimately scaling costs: more professional-looking things often are considered more accessible or desirable by a larger audience, and professional things at minimum take more time and/or experience to create even when they don’t produce greater material costs (and this is the problem that people who create media for youtube run into: youtube isn’t charging them to upload videos, but at a certain point they feel like in order to expand their audience they need better quality cameras and microphones, makeup, lighting, and eventually professional actors and makeup artists and animation teams, which is how the Vlogbrothers video ecosystem became an insanely conventional-looking media establishment despite zero distribution costs).
Art-house films and experimental music, despite hipster cred, are great examples of niche media ecosystems. And, if you have familiarity with either, you can trace the influences of the niche forms into mainstream, as happens over and over. But, both of these are explicit niches. They aren’t so much ‘long tail’, particularly historically: film production was, until the advent of inexpensive high-capacity digital video cameras and video editing software, absurdly expensive even in its cheapest forms, and the same is true of both conventional acoustic instruments and pre-PC synthesizers of both analog and digital variety. You might instead cast experimental film and music as a kind of skunk-works, like Xerox PARC: get the smartest and most creative people together to make things, throw money at them, and never try to actually market the results because the real value is in which ideas and techniques you can appropriate later when they’ve become mature enough to be safe bets. There are similar kinds of skunk-works situations on different levels of niche-ness: the BBC radiophonic workshop was fundamental in the genesis of electronic music and was essentially a government-funded audio special effects studio; low-budget BBC TV shows in the 60s and 70s like Doctor Who and The Avengers were as influential as they were eclectic and despite being popular and having national or even international distribution their low budget nature made them less conservative: the cost of mistakes was low. Television in general was, once upon a time, a skunkworks for film, and public access television continues to be a testing ground for people looking to get into producing film or television. As an example of another level in this hierarchy, comic-book-based and SF-based films have become mainstream while westerns have become fringe — a complete reversal since the mid-70s, and one that allows time for the rich loam of gunslinger mythos to lie fallow and compost itself into a more fertile genre while formerly ignored and ridiculed genre fiction in the SF sphere injects its hard-won fruits and seeds into the mainstream, which, vampire-like, returns from the dead by sustaining itself on the blood of living subcultures.
The particular economic shift of internet distribution — wherein even if scaling up distribution has minimal marginal cost, scaling *out* distribution geographically has truly zero marginal cost barring the extra steps of translation — has meant that each culture operates as though it’s a media niche. Really, this isn’t very accurate in the scheme of things. Gangam Style was never niche: the South Korean music industry is huge and profitable and represents its own mainstream, and both the jpop and kpop industries are considered overly commercial and conservative to people who are in those countries, the same way that bubblegum pop is considered commercial and conservative in the united states. What’s happening is a greater-scale version of what often happens when import barriers are lifted: a niche group in one culture defines itself in part by bits and pieces of another culture’s mainstream. Just as the influx of american gangster movies into post-second-world-war France was the antithesis (and french identity the thesis) creating the synthesis of the french new wave movement in cinema and the influx of american science fiction films into post-war Japan was a major influence for japanese tokatsu/SFX fandom (and as how, earlier, the import of Disney cartoons spawned anime), americans latching onto jpop and british tv imports has created new subcultures. Gangam Style’s widespread familiarity outside South Korea is a slightly more extreme form of the same phenomenon that made Yellow Rose of Texas a huge hit in China, Frank Sinatra’s My Way a huge hit in Japan, Blueberry Hill big in Russia, Jerry Lewis in France, David Hasselhoff in Germany, and pretty much any other unexpectedly popular import. “Big in Japan” is literally a stock phrase in the music industry because the combination of good media relations between Japan and the english-speaking world with a boatload of subtle but complex cultural differences makes for a huge set of english-speaking acts who undergo unexpected success once exported to Japan. But, today, anywhere with widespread high-speed internet access has the same relationship to anywhere else with it that the United States and Japan had in the 1980s. Every piece of mainstream media has the capability of being exported and recontextualized and becoming “big in japan” in some unexpected place.
By Rococo Modem Basilisk on December 1, 2015.
How to become Steve Ballmer: implement stack ranking, throw chairs at people during meetings, yell a lot.
How to become Steve Jobs: make fun of your employees until they cry and/or become suicidal, and then later take credit for their work.
Why do so many people want to emulate famous assholes from industry?
By Rococo Modem Basilisk on December 3, 2015.
Jobs is interesting insomuch as he’s a figure who is inextricably associated with industries that, on the technical level, he had no particular familiarity with, and on the management level, he was not particularly successful with. I can’t help but imagine that, for the most part, Jobs is famous for a combination of charisma and luck: he was willing to present himself as a figurehead and invite the suggestion that he had a greater hand in the products he is selling than he really does, but he would be forgotten if it wasn’t that his post-1997 decisions for Apple were largely just as profitable as his 1980–1985 decisions were unprofitable.
In this sense, he’s similar to other larger-than-life figures particularly in the financial industry: a professional gambler in a sense whose personality and convictions about skill borne from luck have led him to become interesting as a character.
After all, many of Jobs’ positive contributions were minor and aesthetic (the beveled corners of early Macs) or were positive only due to a series of clearly unpredictable accidents, while others were unambiguously terrible ideas both at the time and in retrospect (avoiding any expansion ports on the mac ostensibly to save cost, despite the mac still being double the price and half the performance of the Amiga 1000 and comparing even worse price-and-performance-wise to the Atari ST, both of which had plenty). Any attempt to suggest that Jobs was skilled relies upon attributing psychic powers to him.
Of course, this is a wonderful story to tell a tech industry in the midst of a bubble. Elevate a non-technical guy in management for a tech company to a godlike status and claim that all his good choices were the result of skill and all his bad choices were good choices, thereby suggesting that the universe is orderly and that charismatic people have magic powers that allow them to excel in business without trying. It’s no wonder that the Jobs Hagiography is so popular: like the Prosperity Gospel, it promises monetary success and public adulation and moral justification to the masses while justifying itself by claiming that those who succeed were destined to do so while those who fail have only themselves to blame.
By Rococo Modem Basilisk on December 3, 2015.
Yes, but not all of us physically assault our employees and then are lionized for it.
Yes, but not all of us physically assault our employees and then are lionized for it. That’s reserved for a special class of people whose luck has ruled them immune to criticism.
By Rococo Modem Basilisk on December 4, 2015.
I keep on being reminded of “Sirius Cybernetic Corporation” — the company in the Hitchhiker’s Guide series that keeps producing robots who are irritating because they are too human. To a certain extent, it makes sense to treat corporations and media properties more like machinery — not insomuch as you kick them when they’re not working (to be honest I wouldn’t trust anybody who hurts a machine on purpose) but insomuch as your transactions with them are expected to be limited in scope. You don’t need to smile and say “good morning” to a vending machine but you are expected to do so to a cashier, which is not desirable for you *or* the cashier (because you’re both put in a situation where potentially both parties are supposed to feign happiness and friendliness); media entities on the internet are worse when they break out of the transactional mold, because they benefit less from thousands of people sending them cheerful messages (or, you know, hate mail). Ultimately, we want our robots to work like robots, and we want our computer-mediated commercial interactions to hide the human being behind the screen and just show us the robotic interface.
I see why this trend exists, of course. Intimacy means replacing a dunbar slot. If some very lonely person has 150 of their slots filled by corporations and the rest by actual family, those corporations have a very good defense against competition for that particular customer: because to the customer, Coca Cola is a best friend and Pepsi is a creepy stranger. Of course, this polarizes the effect when everybody tries to do this, and it polarizes the effect even moreso for people who have rich social lives and have most of their slots filled by human beings: suddenly every celebrity and corporate entity is a creepy stranger trying to con you out of money by being overly familiar.
By Rococo Modem Basilisk on December 7, 2015.
There are a couple of models for self-directed learning resources that are useful to look at, because historically they’ve been extremely effective.
One is the reference-material model. Its extreme end is a system like wikipedia: few things are more self-directed than a huge set of extremely detailed articles all hyperlinked together. If you have a goal in mind, you can search and probably find what you’re looking for if you know the right keywords; if you have no goal, you can idly wiki-walk and sate your curiosity indefinitely. (For media, tvtropes may be better than wikipedia, but tvtropes also has elements of the next model I’m discussing — the collegiate model — and may fit better into that category for various reasons. However, for very particular domains there are better resources than wikipedia: memory alpha, for instance, has better coverage of star trek trivia than wikipedia does.)
Another is the collegiate model. I call it that because this is the kind of thing that universities try to produce in both students and research faculty, and it’s also one of the arguments for think-taks and open-plan offices. I would argue that the most extreme form of it is in collaborative open question-answering communities like Quora and Stack Overflow, with comment-centric news sites with voting mechanisms (reddit, hacker news) and any other discussion system with large numbers of extremely active members (imageboards like 4chan for instance) coming in second. The point of a collegiate model is that you have people from different fields of interest interacting with each other in a dynamic way. Everybody learns, everybody takes the role of both teacher and student, and often the community itself produces new ideas and names them. A reference-material model is extremely effective at allowing a motivated learner to access existing well-documented ideas, but a collegiate model produces new ideas and allows motivated learners (even if they are beginners) to participate in the process of producing those ideas. That said, the reference-material model imposes some rigor on ideas (mostly by transplanting the rigor of existing established systems like peer review, academic publishing, canonicity, and journalistic standards), while the collegiate model’s flaws and benefits come from its flexibility. A collegiate model system will produce many ideas, and only a few of them will be good; furthermore, any bias in membership is likely to perpetuate itself (male-heavy communities often over time become misogynistic and then become even more male-heavy as they shut out women; the same is true of sample biases in terms of economic situation, race, religion, and even otherwise innocuous differences like tendencies toward logical positivism versus epistemic agnosticism or belief in free will versus determinism. For extreme examples, look at 4chan’s various boards, many of which are incredibly creative and have had a huge impact on the culture of the internet but each of which have consistent and absurdly biased cultures and habits that often don’t translate well between boards).
Attempts to automate self-directed learning have historically relied upon sticking a layer of gamification on top of the reference-material model — everything from hand-held trivia games to twenty questions to duolinguo do this (and memrise does this while sticking a tiny bit of collegiate model in a bag on the side by allowing people to create and vote on mnemonic devices).
By Rococo Modem Basilisk on December 7, 2015.
I don’t buy the idea that these kinds of restrictions will ever become universally (or even nearly universally) enforced.
Why?
1. Because advertising, like many other forms of capitalism, is a race to the bottom that subverts any attempts to ensure quality. Advertising optimizes for metrics because it is paid based on metrics, and the least-effort means to provide comparable metrics will be used. Bad ads can risk being totally ineffective because it’s a safe bet that the people paying for them won’t realize they’re ineffective until the bad ads have already made enough money to justify the effort. 2. Because the largest ad providers in the world (Google’s Adsense and Doubleclick) already enforce quality standards for ads and already enforce many of the constraints you’ve suggested (to the extent that they can). By doing so, they yield part of their potential market to middle-men who specialize in low-quality scammy ads. As long as we have ads, we’ll have scammy, grey-legal ads, and the attempt by the largest providers to improve general quality has led to a bifurcation in the ad market between up-market ads and explicitly crap ads. Some ad-blockers allow up-market ads (those approved by Adsense) to be shown with some constraints; this just means that the scammy ads get shown exclusively to the less-savvy users who were more likely to click them anyhow. 3. Ads don’t really work particularly well in the best of cases. Having an ad-based economy on the web is, essentially, a bubble. Any ad is inherently an annoyance, so getting rid of the absolute worst ads won’t keep people from using ad blockers. The widespread use of ad blockers will just make it clear to companies formerly using advertising that the utility of advertising (particularly on the web and particularly through media that are clearly ads) is minimal. Already, there are plenty of ways in which the traditional web-based ad ecosystem is being circumvented: patreon and similar crowd-funding mechanisms are supplementing or replacing ad-driven content because ads don’t pay without enormous volume; large companies are sponsoring high-quality content that is largely unrelated to them (GM sponsoring Backchannel & the Cracked Podcast, for instance); Google Contributor uses the existing Adsense infrastructure to directly pay ad hosts with money uploaded by users without involving the advertisers at all; while that standalone bitcoin-mining machine intended to pay for content passively doesn’t have its economics straight yet, a similar system is perfectly feasible in theory; various attempts at automating the music royalty system seem likely to yield or inspire a general-purpose transcopyright-style micropayment infrastructure for derivative works & content reuse. Some or all of these things will eat up enough of the web ad ecosystem to make the changes you propose pointless long before enforcing them becomes practical.
By Rococo Modem Basilisk on December 15, 2015.
Something not mentioned in the article is that OpenAI is in direct competition with several other non-profits whose goals are (or include) to ensure provably friendly AI — MIRI being the obvious example. I’m a little worried that all of these seem to be run on the west coast of the US and funded by the same group of ‘california ideology’ folks — if there are any hardcore marxists around who want to ensure provably friendly AI, I recommend that they set up some competition too!
By Rococo Modem Basilisk on December 15, 2015.
One peeve I have with this analysis is: even if you consider a dysfunctional government to be similar to a dysfunctional corporation, Trump is not a skilled businessman: he just plays one on TV. For all her past failures, Florina has a better record in that regard.
By Rococo Modem Basilisk on December 17, 2015.
Scene/Sequel: Post-Mortem of a Fiction Generator
Abstract Fiction generators based on goal-directed planning in a simple state machine can produce reasonably human-like output without explicit modeling of multiple characters by treating the planner as the narrator and protagonist.
Background Fiction generation using world models is not new. The same kind of planning used in SHRDLU (Winograd, 1972) drove early planner-based fiction generators like TALE-SPIN (Meehan, 1976). This class of fiction generators is a middle ground between ‘simulationist’ models, wherein a large number of variables are modeled carefully and a story is extracted from the progression between states by dropping most of the information modelled, and ‘texture’ models, wherein large pieces of preexisting flavor text are pieced together. (An example of the former would be the history of the world produced by Dwarf Fortress, and an example of the latter would be BRUTUS (Bringsjord, 2000).) TALE-SPIN-like planner-based fiction generators have, historically, like TALE-SPIN, modeled a set of characters and modeled their interactions. The most interesting stories generated by these generators remain the “mis-spun tales”, wherein assumptions by the generator are flawed in a way that produces absurd violations of common sense; the remainder of tales are fairly mundane transcripts of interactions between characters. As demonstrations of a functional model, these transcripts are serviceable; as stories, they lack interesting forms of structured conflict.
Author Jim Butcher presents a model of plot construction I call the ‘scene- sequel model’ that diverges from the idea of the world as a perfect simulation and focuses on the kinds of stories readers find interesting. While Butcher’s interest is in human-written stories, his ideas lend themselves to generative fiction more so than other seemingly more mechanical models of plot construction like Plotto and the BS2. Quickly: within the scene-sequel model, a story has a single protagonist and consists of scenes (wherein the protagonist attempts to achieve his or her goal) and sequels (wherein the protagonist reacts to the content of the scene emotionally and performs his or her planning); scenes have scene-specific goals that are either achieved or not achieved and each scene may introduce complications that change the protagonist’s state.
Current work For NaNoGenMo 2015, I present a fiction generator based on a simplified scene-sequel model for caper novels. In this model, there is only one character modeled: the protagonist, who is also the narrator. The content of the story is generated by the planner with the help of flavor text.
The story is generated by the planner from a ‘world model’, which is a state machine with associated flavor text. The world model consists of a list of state objects; each state object has a list of other states it can transition to (along with a probability for this transition to occur and optional flavor text for describing attempted, successful, and failed transitions). It furthermore contains a set of potential complications, which themselves are state names paired with probabilities.
The planner is provided with a world model, a weight for state transition success rate, a weight for complication accretion rate, a starting state, and a goal state. The planner keeps a ‘goal pool’ consisting of a set of state names and weights, and this goal pool is initially set to the goal state. Typically, the goal state is the overarching goal of the protagonist. As complications are accrued, they are added to the goal pool.
At each step, we perform a sequel followed by a scene.
The sequel consists of information produced by the planner, which (starting from the current state) performs a walk of the state space, depth-first, searching for the goal state. Because the world object contains cycles, the tree-walking portion of the planner is provided with a recursion depth limit. The planner, multiplying and adding probabilities, determines the immediate next state with the greatest composite likelihood of reaching each state in the goal pool — in other words, the likelihood of reaching each goal is determined, adjusted by its goal weight, and then a composite is produced for ranking purposes. During this tree-walking and goal-weighing process, at an adjustable rate, information about the planning process and the decisions being made is emitted in the style of first-person narration of planning in a caper novel:
So, I figured, if I tried to steal them jewels by trying to pass as a museum employee I’d have maybe a 35% chance of succeding. I’ll try to remember that.
If I’m trying to get a museum uniform, what if In order to steal them jewels, I tried to pass as a museum employee. That has about a 1 in 10 chance of working. So, I figured, if I tried to steal them jewels by trying to get a museum uniform I’d have maybe a 10% chance of succeding. I’ll try to remember that.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
Once the planner has produced a candidate state, it uses the likelihood of success for the specific state transition and the global state transition weight to determine whether or not it succeeds, producing the appropriate flavor text (or a default if undefined), and then produces the set of complications to add. If the state transition is successful and the new state is the end goal state, the program stops here; if the state transition is successful and the new state is in the goal pool, that entry is removed from the goal pool.
Here is an example of the full output, using a world model about jewel theft:
This is the story of that time I decided to try and steal them jewels.
So, I figured, if I tried to steal them jewels by trying to pass as a museum employee I’d have maybe a 35% chance of succeding. I’ll try to remember that.
If I’m trying to get a museum uniform, what if In order to steal them jewels, I tried to pass as a museum employee. That has about a 1 in 10 chance of working. So, I figured, if I tried to steal them jewels by trying to get a museum uniform I’d have maybe a 10% chance of succeding. I’ll try to remember that.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
I failed to get a museum uniform while trying to go about it the obvious way. Bummer. Now I have to get a smaller gun. I still need to steal them jewels.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
I failed to get a museum uniform while trying to go about it the obvious way. Bummer. I still need to get a smaller gun. I also still need to steal them jewels.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
I failed to get a museum uniform while trying to go about it the obvious way. Bummer. I still need to get a smaller gun. I also still need to steal them jewels.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
I totally succeeded in my attempt to get a museum uniform by trying to go about it the obvious way. Yay! Now I have to get a smaller gun, again. I still need to steal them jewels.
So, since I’m trying to get a museum uniform I decided to steal them jewels by trying to pass as a museum employee. So, I have to pass as a museum employee. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to get a museum uniform.
I failed to pass as a museum employee while trying to get a museum uniform. Bummer. I still need to get a smaller gun. I also still need to steal them jewels.
So, since I’m trying to get a museum uniform I decided to steal them jewels by trying to pass as a museum employee. So, I have to pass as a museum employee. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to get a museum uniform.
I failed to pass as a museum employee while trying to get a museum uniform. Bummer. I still need to get a smaller gun. I also still need to steal them jewels.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
I totally succeeded in my attempt to get a museum uniform by trying to go about it the obvious way. Yay! Now I have to get a smaller gun, again. I still need to steal them jewels.
So, since I’m trying to get a museum uniform I decided to steal them jewels by trying to pass as a museum employee. So, I have to pass as a museum employee. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to get a museum uniform.
I totally succeeded in my attempt to pass as a museum employee by trying to get a museum uniform. Yay! Now I have to escape the museum. I still need to get a smaller gun. I also still need to steal them jewels.
So, since I’m trying to pass as a museum employee I decided to steal them jewels by trying to steal them jewels. So, I have to steal them jewels. I also have to get a smaller gun. I also have to escape the museum. Right now, I’m trying to pass as a museum employee.
I totally succeeded in my attempt to steal them jewels by trying to pass as a museum employee. Yay! Now I no longer need to steal them jewels. Now I have to heal my arm wound. Now I have to heal my chest wound, too. I still need to get a smaller gun. I also still need to escape the museum.
THE END
Here is the world model that produced that story:
world={} world[“go about it the obvious way”]={“get a museum uniform”: {“probability”:0.5, “complications”:{“get a smaller gun”: { “probability”:0.9 }}}, “go to the ninja supply store”:{“probability”:1}, “go to the gun store”:{“probability”:1}} world[“go to gun store”]={“get a smaller gun”:{“probability”:0.7, “complications”:{“find my stolen wallet”:{ “probability”:0.2}, “success_descr”:[“The gun store carried an antique gun intended for defending women on bicycles against dogs in the late nineteenth century. “, “The gun store carried a half-scale airsoft dart gun version of a Walther PPK, and poison darts. “]}, “descr”:[“The gun store was a tiny brick building by the side of the highway, in the bad part of town. “], “success_descr”:[“The owner glared at me, and then at my ID, and then back at me. Then he grunted, accepted my cash, and handed me the new gun. “], “failure_descr”:[“After banging on the locked door for ten minutes, I noticed a tiny sign at the lower left hand corner of the window embedded in the door. It had hours. It turns out, this store is closed on Tuesdays. “]}, “goal_reqs”:{“or”:[“get a smaller gun”]}} world[“get a smaller gun”]={“go to gun store”:{“probability”:1}, “go about it the obvious way”:{“probability”:1}, “pass as a museum employee”: {“probability”:0.3}} world[“get a museum uniform”]={“pass as a museum employee”: {“probability”:0.3, “complications”:{“heal my leg wound”:{ “probability”:0.2}, “escape the museum”:{ “probability”: 0.7} } }, “descr”: [“The costume shop was tucked into a strip mall down town, between a laundromat and a chinese take-out place. It smelled like soap. “], “success_descr”:[“There was a perfect museum employee uniform sitting on the rack to the left of the entrance. “], “failure_descr”:[“After looking through the racks several times, I finally decided to ask the cashier — a wrinkled but plump old woman with a puff of curly white hair — if she carried museum employee uniforms. She shook her head, and I left, dejected. “]} world[“pass as a museum employee”]={“steal them jewels”:{“probability”:0.7, “complications”:{“heal my leg wound”:{ “probability”:0.3}, “heal my arm wound”:{ “probability”:0.3}, “heal my chest wound”:{ “probability”:0.3}}}, “go to the hospital”:{“probability”:0.9}, “reqs”:{“or”:[“get a museum uniform”]}} world[“go to the ninja supply store”]={“get a smaller gun”: {“probability”:0.4, “success_descr”:[“In the glass display case, there was a poison dart gun that looked like a fountain pen. I bought six! “], “failure_descr”:[“The cashier claimed that they had a moral aversion to projectile weapons, and thus did not carry them. “]}, “purchase a black leather catsuit”:{“probability”:0.7, “success_descr”:[“A beautiful black leather catsuit greeted me from the rack to the left of the doorway. “], “failure_descr”:[“All the black leather catsuits they had in stock were sized for literal cats. “, “All the black leather cat suits they had in stock were way too big for me. “, “All their black leather catsuits were covered in shiny chrome studs and buckles, and wouldn’t help me disappear into the night at all. “]}, “purchase a grappling hook”:{“probability”:0.7, “success_descr”:[“There was a grappling hook with two hundred feet of rope sitting right behind the counter, on display. “, “I spent twenty minutes looking through the discount bin, before finding an absolutely perfect grappling hook for thirty cents. When I went up to pay for it, the cashier waved me off — no charge. “],”failure_descr”:[“\”Are there any grappling hooks in stock?\” The cashier, impassive behind his mask, shook his head slowly in response to my question. Then, after a moment of staring at me, he threw a smoke bomb at his feet. I found myself outside the shop, which was now locked. “]}, “descr”:[“The ninja supply shop was in the middle of the second floor of the mall, between a Hot Topic and a Zappo’s. It was dimly lit, and the scuffed floors had a fake tatami-pattern print. There was a wad of gum stuck to the doorway. “], “failure_descr”:[“The shutter was shut, and a great big lock hung from the side of it. “, “The door was shut and locked, and a sign said \”Back at 2:00\”. It was four. I waited until six. “],”success_descr”:[“As I entered, a machine emitted a little beep to indicate that customers were about. The cashier appeared out of a plume of smoke behind the counter. “]} world[“purchase a black leather catsuit”]={“sneak into the museum at night”:{“probability”:0.8}, “go to the ninja supply store”: {“probability”:1}} world[“purchase a grappling hook”]={“sneak into the museum at night”: {“probability”:0.9}, “go to the ninja supply store”:{“probability”:1}} world[“sneak into the museum at night”]={“steal them jewels”: {“probability”:0.8, “complications”:{“heal my leg wound”: {“probability”:0.3},”heal my arm wound”:{ “probability”:0.3}, “heal my chest wound”:{ “probability”:0.3}}}, “go to the hospital”: {“probability”:0.9}, “reqs”:{“or”:[“purchase a black leather catsuit”, “purchase a grappling hook”]}} world[“go to the hospital”]={“heal my leg wound”:{“probability”:0.9}, “heal my arm wound”:{“probability”:0.9}, “heal my chest wound”: {“probability”:0.7}, “escape the museum”:{“probability”:0.7},”goal_reqs”: {“or”:[“heal my leg wound”, “heal my arm wound”, “heal my chest wound”]}} world[“heal my leg wound”]={“go to the hospital”:{“probability”:1}, “get a museum uniform”:{“probability”:1}, “get a smaller gun”:{“probability”:1}, “go to the ninja supply store”:{“probability”:1}} world[“heal my arm wound”]={“go to the hospital”:{“probability”:1}, “get a museum uniform”:{“probability”:1}, “get a smaller gun”:{“probability”:1}, “go to the ninja supply store”:{“probability”:1}} world[“heal my chest wound”]={“go to the hospital”:{“probability”:1}, “get a museum uniform”:{“probability”:1}, “get a smaller gun”:{“probability”:1}, “go to the ninja supply store”:{“probability”:1}} world[“steal them jewels”]={“steal them jewels”:{“probability”:1, “complications”:{}}} endGoal=”steal them jewels”
By Rococo Modem Basilisk on December 17, 2015.
Absolutely in agreement with you, here. This is a major pet peeve.
The way I usually phrase this position is: “For any definition of creativity not formulated specifically to exclude them, computers are already capable of creativity, and have been since the 1950s at the latest.”
By Rococo Modem Basilisk on December 21, 2015.
Why you should care about generative text
Generative text gets some press. Unfortunately, like many other technical fields that attract shallow coverage, generative text has been the subject of minor variations on the same article every few months since the mid-1950s.
The typical article on anything related to generative text (particularly generative fiction) has the following pattern:
“[Insert quote here].”
You would think that was written by a human, wouldn’t you? Well, reader, you are a dumbass, because that was written by a machine!
But don’t feel too bad, because here is an example of the machine writing something comically terrible and absurd: “[insert quote here]”.
“[Insert quote here],” says one of the three researchers in the field that we crib quotes from previous interviews from every six months.
In conclusion, I’d like to reassure you that computers won’t be writing all the novels soon. Or will they? Shock!
If you haven’t read articles about generative fiction before, no matter; they’re all like that. If you have, you know what I mean. This article is not that article. I’m going to tell you all of the interesting things about generative text that those articles didn’t cover.
You should care about generative text if you care about:
Video games
Video games have been trying to expand their clout as a narrative medium. While there are a lot of ways to do narrative in an interactive medium, one of the most obvious is to have interactive dialogue. After all, dialogue usually drives narrative in films, books, comics, and TV.
Unfortunately, dialogue trees don’t scale well: after all, they grow exponentially, and inconsistent or inadequately varied dialogue is extremely obvious. As a result, dialogue-driven interactive narrative is typically limited to big game studios, and truly complex dialogue trees are rare even in game genres that have that as their primary technical focus (such as VNs and adventure games).
The same way that, five years ago, indie game developers started looking toward procedural world generation as a way of creating large maps that allowed them to compete with large development houses’ sandbox games in terms of scale, indie game developers now are beginning to look at the various ideas in the field of text generation for ideas about automatically generating varied dialogue and dialogue trees from models of character and narrative. The next Minecraft might derive its scale from procedural expansion of NPC dialogue instead of procedural expansion of the map.
Journalism
Right now, because of the use of advertising for monetization of content, a lot of internet ‘journalism’ is clickbait — in other words, content optimized for page view counts rather than for sustained attention. Clickbait is generated at low cost by content farms; it’s poor-quality because being high-quality is a net loss, and it’s short because that’s cheaper than being long. Clickbait optimizes for two things: number of ads on a page and number of people who will click a link. However, generative text is extremely promising for content farm owners: after all, even if you’re paying content farmers pennies an hour, you’re still paying more for these humans than you would for machines, who can generate far more content far more quickly with only slightly lower average quality. The kind of A/B testing that content farms use for optimizing their headlines is, furthermore, a perfect match for existing methods by which relatively simple AIs can use feedback to improve their headline generation — and AIs are already pretty good at generating clickbait headlines. In other words, machines might well easily replace the lowest end of internet journalism.
At the same time, text generation is already beginning to supplant the lowest end of traditional paper journalism, with various organizations automatically generating minor financial and sports stories. This frees up human writers to be put on more interesting stories, or to alternately be fired, depending upon the skill of the writer and the financial situation of the news agency.
Both of these effects are truly huge potential economic shifts in these industries. And, they have the potential to be truly positive, as well. Consider Buzzfeed, which makes its money off inane clickbait content and then turns around and funds wonderfully deep serious journalism by serious journalists about interesting subjects with all that shit-click money: if they automated their low-quality high-lucre content, they could shift more of their workforce toward high-quality journalism. Alternately, the slightly lower quality of machine-written articles versus content-farmed articles might accelerate the devaluation of clickbait and cause alternatives to ad-based monetization to become more popular more quickly.
Psychology
The effectiveness of generative text is the result of an interplay between the design of the generator and the human audience. The best generators lean heavily on the human element, using rich associations and loaded structures to convince the reader to project meaning onto the text, which itself is very often structurally simple. All of this is to say that a large part of the design of text generators is psychology. Invert this, and it’s not at all surprising that text generators are being used by experimental psychologists to probe the human mind.
Just recently, there’s been press coverage of a study using a new-age BS generator to study personality traits associated with the projection of meaning , as well as of a group of older studies using joke generators to study the mechanics of humor. Text generation allows psychology experiments to scale up and to have extremely fine control over the material they use; when text generators used in psychology experiments have their source made available, later experimenters can tweak the generator in various ways in order to easily test variations on the original experiments, and text generators can be hooked up directly to systems like Mechanical Turk that allow experimenters to expand their studies outside the college campus.
Spam
The spam industry is the only segment of the tech world to really take text generation seriously. Spammers have been using techniques like text spinning to trick both humans and AI filters for more than a decade. As technology improves, spam will get better. Any advances in text generation will probably be employed by spammers first.
Ebooks
A few years ago, Amazon had a problem with machine-generated reference books of poor quality. These reference books would be produced based on search queries, which were fed into Wikipedia and the resulting pages combined, initially into a print-on-demand book but later into ebooks. It’s a little unclear to me how Amazon fixed this problem, but it doesn’t seem to be such a big deal anymore.
However, this is not the only con in Amazon’s ebook ecosystem. Today, the big money is in creating hyper-targeted erotica[1, 2, 3]. Consumers of erotic fiction often don’t care very much about prose quality, or are prevented by fear of social stigma from being vocal in their criticism of poor-quality prose in erotica. Erotic fiction is extremely popular, and hyper-specific subgenres have their own categories on Amazon, which means that it’s fairly easy to get to bestseller status in a single category (and thus have a boost in sales resulting from a listing on bestseller pages).
Again, text generation is a very good fit for erotica. It is easy to generate arbitrary amounts of poor-quality erotica. Erotica is either effective or comical: even bad erotica is good. While existing cons for Amazon erotica ebooks often involve taking public domain erotica and publishing it with a few easily automated changes, there’s no reason that brand new erotic fiction couldn’t be generated in various categories at the kind of rate that only machines can keep up with.
UI design
Partly because of the hype behind Slack and Siri, conversational UIs have become trendy recently. Those of us of my generation will recall the failure modes of conversational UIs. (Remember SmarterChild?) Understanding how to perform good text generation and take advantage of the Eliza Effect will help future conversational UIs feel less static and more lively.
Determining whether or not forming an emotional attachment to an AI-driven corporate mascot is a good thing is left as an exercise for the reader.
The arts
There’s a long history of musicians and authors employing ‘writing machines’ to help produce inspiration from old content. These ‘writing machines’ vary in details, from forms of traditional bibliomancy to dadaist or surrealist writing games to oulipo-style constrained writing to the use of computer programs for scrambling text. Phillip K. Dick used the I Ching to determine the plot of his award-winning novel The Man in the High Castle; William S. Burroughs, Thom Yorke, and David Bowie all used cutups to inspire their work (going so far as to sometimes use the text produced by cutups directly); Doctor Seuss’s unique style was largely determined by heavy constraints on his vocabulary.
Text generation technology presents a new set of ‘writing machines’ for authors, poets, and musicians to collaborate with and build upon. The difference is that, where previous mechanisms primarily created stylistic affectations or merely inspired narrative tangents, more recent text generation technologies are capable of producing a variety of engaging and interesting narratives by themselves.
Outside of the more traditional forms, pure text generation has come into its own in the form of text generator driven twitter bots. A variety of pitch bots use simple templates to produce amusing and evocative ‘pitches’ by combining familiar forms with mismatched corpora. Bots exist that automatically generate biting satire of overused, shallow, or damaging trends.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
By Rococo Modem Basilisk on January 13, 2016.
If you find the others on the road, kill them
If you find the others on the road, kill them
The last part of Tim Leary’s famous quote is often forgotten: “turn on, tune in, drop out, and find the others”. Leary claimed it was the most important part: after all, all of the outsider weirdo creativity goes to waste if it’s stuck in your skull. Unfortunately, nothing has prepared us for the strange times.
There is no counterculture anymore, because there is no mainstream culture anymore. (Maybe there is for you, if you live in a theocracy with state-regulated media and heavily limited internet access. If you are, congratulations on reading this post, and also why are you reading this post?) The spectacle has consumed and absorbed the early-90s utopian cultural pluralism of the John Perry Barlow set, just as it absorbed the culture/ counterculture division of the 50s and 60s, which is why people like Stewart Brand are rich de-facto plutocrats now. And in the global village, no matter how freaky you are, you can surround yourself with precisely the same kind of freak and live in your little filter bubble. Finding the others, in this case, is a bad thing.
Don’t find the others. Find the Others. Stick apart. If you agree with someone, take that as a warning sign: do the two of you agree for good reasons, or are you just incidentally the same kind of freak?
The function of the lunatic fringe is not to become a comfortable space for you and your like-minded friends. There are other places for that. The function of the lunatic fringe is to thrust half-baked ideas into a violent orgy of death and copulation until they become more fully baked or die the death of warriors.
By Rococo Modem Basilisk on January 19, 2016.
Paranoia as a design choice
Paranoia as a design choice
A lot of people have been making a lot of noise about this post about bitcoin. Even before that, people were making noise about the bitcoin community being full of goldbugs. The thing is, bitcoin is a cryptocurrency whose primary design goal is paranoia, and all other factors are secondary. Nothing about the bitcoin design or the bitcoin community will make sense if you don’t recognize the intentional placement of paranoia in its design.
Bitcoin isn’t the only technological system built explicitly around paranoia. In the crypto and computer security communities, building systems around paranoia is normal, in the same way that it’s normal in a late-capitalist environment to build systems around profit maximization. SSH is designed around paranoia (and this is why you no longer can enable ‘cipher=none’ in SSH at compile time); the WWII-era ‘double-cross’ espionage structure that underlies cold-war spy thrillers is a social technology built around paranoia; the hospital policy surrounding disposal of drugs is based around paranoia.
Systems that are intended to be secure but are not designed around paranoia tend to become security theatre if they function at all — consider the HTTPS certificate system, the TSA, gun lock technology (and indeed much of regular lock technology), community policing, credit card and check based payment mechanisms, website password protection prior to widespread access to two-factor authentication, and highway toll stations. Security theatre is not entirely ineffective — after all, most people follow the rules most of the time and security theatre hints at the idea that the rules are important enough to be enforced — but it typically combines most of the inefficiencies of a truly secure system with most of the insecurities of a truly effective system.
When we’re talking about bitcoin, we’re talking about a conception of money wherein the government is the enemy. We’re also talking about a conception of money wherein banks and corporations are the enemy. We’re talking about a system where the assumption is that hours of clear time is a small price to pay for proof that double-spending is nearly impossible, and wherein the ideal community of users is a global network of wealthy individuals who never communicate except by exchanging money for goods or services and who have no friends or loyalties. Bitcoin is a currency designed for people who think of other people as potential enemies first.
There are circumstances where that view of the world is accurate.
This doesn’t really explain why large banks are investing in bitcoin, or why bitcoin startup companies exist. Even mining pools are too communal: the moment that the interactions of the bitcoin universe fail to consist solely of pristine blocks of pure value reeking of the scent of poorly-veiled animosity and apprehension, we are on the slippery slope to collusion-based attacks to debase the currency. Mining pools have gotten close to 51% before and have backed off out of the good in their hearts; however, whenever the good in people’s hearts matters, this represents a failure of any security model based around paranoia.
Of course, at least for the startups, we can look to the california ideology for why bitcoin was adopted by actual organizations. We can look to strange hybrids like ESR. The thing about the paranoid position is that it can be easily distracted from old threats by new threats; this is why libertarianism is a right-wing ideology now while it was a left-wing one fifty years ago. If you distrust the government because they are the one with all the guns and money, you’re right — they have both those things and they may or may not be on your side; but, if you support them wholeheartedly as soon as another party attacks, you’re gonna have a bad time. Bitcoin’s paranoia is against any kind of collaboration, but the bitcoin community’s mix of the generally paranoid and those paranoid only against the bugaboos of the moment (along with various collaborations, scams, and internal politics) have caused a mix of fracturing within the community and collusion with forces from without it. Bigotry is fractal, and as soon as the policy of paranoia in bitcoin became twisted, the community shattered.
This is a pretty common failure mode. Think of mole hunts in the CIA under James Jesus Angleton. Think of Jim Jones. Think of Ayn Rand’s last amphetamine-fueled years. A failure of appropriately applied paranoia cannot be remedied with an excess of poorly applied paranoia.
By Rococo Modem Basilisk on January 19, 2016.
To be sure, these hollywood formulations are male-centric.
To be sure, these hollywood formulations are male-centric. But, to suggest that those aspects will translate is to assume that it’ll even work. The requirements of hollywood UX are very different from real UX, and this is most extreme with conversational interfaces — to the point that the kinds of interfaces that work in films do not work IRL and vice versa.
After all, in a film, any computer interface must be immediately recognizable and understandable to people seeing it for between a few seconds and a few minutes. A real computer interface, on the other hand, can be initially a bit confusing if that makes it more usable in the long run. To be more concrete: a film’s UI must have bright colors and a giant font and only clearly show plot-relevant information (otherwise the viewers wouldn’t know where to look), while a real UI needs to show whatever information the user might find useful (life has no plot) and giant fonts and bright colors would annoy long-term users. This kind of problem extends out to conversational interfaces in films, which are driven by exposition and spectacle. People in films don’t even talk to each other the same way as they do in real life, because dialogue is optimized for showing off plot and character rather than engaging in the kind of social game-playing that dominates real dialogue.
A real conversational interface ideally resembles the interaction between two technical professionals: in other words, over time a shortened and optimized jargon is developed for more efficient communication, so that the kind of conversation that begins looking like an interaction between strangers eventually ends up resembling a command line interface.
A conversational interface based on mimicing conversational interfaces from movies won’t support the patriarchy, because it will be almost completely useless — worse in all ways than a simpler system based on individual commands. Anyone who uses such an interface is merely screwing themselves over.
By Rococo Modem Basilisk on January 19, 2016.
The idea of a Bowie
The idea of a Bowie
I was surprised that I was affected so much by Bowie’s death. Despite being a fan, I don’t have a huge appreciation for the level of musical craft in Bowie’s music — I see him as an experimenter, and I appreciate him in the same way as I appreciate Steve Reich or Skinny Puppy: I liked the way that he wasn’t afraid to alienate his audience in pursuit of some pure expression. Nevertheless, I feel like a void has opened in the pit of my abdomen, slowly sucking my entrails out from the inside. I used to make fun of people for mourning celebrities, but I’ve been shown firsthand that the kind of connection between an artist and his audience is a real one, and the pain that occurs when it’s severed is real too.
The worst part is, Bowie isn’t even dead. David Jones is dead. Bowie was never alive in the first place.
This is not an idle distinction. David Bowie is a fiction-suit. David Bowie has no more to do with David Jones than Ziggy Stardust does, and we killed Ziggy a long time ago.
What David Bowie is, ultimately, is not a human being nor an icon but a set of patterns, practices, and behaviors. David Bowie is a way of life. Buried in the coverage of Jones’ death, clues about how to Bowie are being uncovered and re-aired: information about the creative process from old interviews, and some of the theory behind it.
I didn’t know David Jones. But, to a certain extent, I, like other fans, know Bowie, because Bowie is a construct made exclusively to be known by fans. Here is my attempt to put together the beginning of a list of things that made the Bowie fictionsuit interesting.
1. Bowie is a container for other personalities. Even as Bowie is not Jones, Bowie also constructed and discarded other identities, acting as a buffer between these identities (who are ultimately the real rock stars) and Jones himself. Each of these identities is a mythic figure with an epic arc. Each of these identities has a different angle on both the world and Bowie. Ziggy Stardust and Halloween Jack are warped Christ-figures: in a gnostic manner, they become minor messiahs for a group of disaffected youth while corrupting themselves in the process, and in the end they are bodily destroyed by invisible powers greater than themselves whose bidding they were unknowingly doing and whose goals were ultimately selfish; Aladdin Sane and Cracked Actor were more meta: a window into the fracturing of personality. An album was a story of another Bowie fictionsuit, and albums iterated on previous albums, rewriting their story: Outside rewrote Scary Monsters, which rewrote Diamond Dogs, which rewrote The Rise and Fall of Ziggy Stardust, which itself rewrote sections of Space Oddity (consider Memory of a Free Festival). Every Bowie egregore is created, set on its path up the arc of the monomyth, and killed off and discarded at the peak of its cultural power. It is in this way that Bowie (and Jones himself) avoided the rock star’s mythical downfall. 2. Bowie took creativity very seriously, and availed himself of mechanical means to expand upon ideas and styles. He used cutups for twenty years, both to expand inspiration via juxtaposition and to create actual content. References to Thelema are indications that Bowie was familiar with the english cabala and with Tarot — which is to say, he is familiar with the history of using bibliomancy as a mechanical means of obfuscating existing patterns to generate new insights. When he said that the cutup method is “a very western Tarot”, this indicates the depth of his insight: after all, Tarot is not (historically) eastern — even the people who make dubious claims as to the long history of Tarot only put it as far east as Egypt, while actual historians would say that the game of trumps originated in fifteenth-century Italy and the use of Tarot in divination originated in nineteenth century England under the Golden Dawn — so what does it mean to be a “very western” Tarot? Geographically, the cutup method in its modern form was developed in the international zone of Tangiers in Morocco — so, west of Egypt and west of Italy but southeast of England — and its predecessors in the form of surrealist and dadaist writing games were european. Instead, we can say that the cutup is a culturally western Tarot: it is a purely mechanical means of symbolic rearrangement developed by an american heir to a calculator fortune, and it eschews the kind of monastic memorization of static correspondences that Tarot relies upon. It is a more culturally western (by which I mean empiricist-pragmatist-positivist) occult tradition than the Western Occult Tradition. By the time that Bowie was writing Outside, he had graduated to an even more mechanized and even more western form of cutups: a computer program developed in California for the Macintosh that chopped sentences into five-word columns before shuffling the columns. 3. Bowie wasn’t afraid to appropriate other people’s ideas and scramble them. In some sense, Bowie himself was like a cutup machine. At various points, in interviews, he eschews any sense of intended meaning behind his words and actions. To an older crowd, this is sometimes seen as the mark of a pretentious poseur; but, this postmodern attitude toward meaning and toward the function of media is in line with bibliomancy historically. Bowie was a catalyst for other people’s growth, because he put things out there that other people had to fit together, and because they fit those things together using pieces of themselves. Sometimes, this cabalistic attitude toward art merely affected the way identities were presented; other times, it had drastic effects on the media he was working in (such as Bowie’s outsized affect upon the glam, punk, and rivethead genres). Juxtaposition of genres creates new genres while expanding those genres that have been juxtaposed. Bowie did this as a habit. 4. Bowie didn’t stop. He didn’t have to. By sacrificing his fictionsuits, he saved himself and was able to work another day. As a result, he was able to release albums at a fairly steady pace from the mid-60s to 2016. Part of his legacy is the fact that he was so productive for so long, and part of his legacy is that he never stopped innovating. 5. When it was time for David Jones to leave, Bowie made a show of casting him off too. The body that had held the idea of Bowie is gone, but in addressing directly the same way he had with Ziggy, Bowie took control of this situation. One way of reading Blackstar and Lazarus together is to consider it a fight between Jones, who wants to keep being Bowie, and Bowie, who wants to cast off Jones in order to live on in another body. (I am treating Lazarus as being from the perspective of Jones and Blackstar as being from the perspective of Bowie.)
Obviously, all of this was combined with a lot of talent, hard work, energy, and heaps of pure physical sex appeal. Invoking the Bowie godform is never going to be easy. But, neither is it impossible.
Bowie is dead. Long live Bowie.
By Rococo Modem Basilisk on January 20, 2016.
A cutup of Palin’s Trump endorsement speech
part why holy all more  and and around works but  they of Its youll The  theyve he the a well  for keep youre hero us  has ha to hard And  jobs as more Mr to  private conservative relax isnt was  go very Well Senate try  theyre Right other code day  who our they the all  infrastructure to the rogue for  are and multibillionaire they would  to to are Not of  theyre is we much them  to that hang changey going  We so the his the  known passionately tell our he  these racebaiting Heads division would  you proves vets Iowa sides  something ethic reform and hes  down him Donald busted quiet  a important they Theyre great  you brought with but than  or to then cooks and  are goodness you Its You  right team president why of  friends a you where gravy  He some told America America  a couple go and suck  others not for every spinning  sector am of and enemy  say fulltime from our I  tried to them Greek You  Well coming week debated political  their vest being tiny Parenthood  train part for bit and  to then to blank towns  to team multibillionaire When this  deserve horse so both ISIS  Ive voters to the private  unify selfiesticks to main Trump  the refreshing gonna things you  those So establishment let supporters  its and Trump contest fabric  and jobs would under look  see to keep to finally  and that Thank take titles  man of of Iowa to  of involved really is of  on machine on that able  of movement veil he values  issues packs ready the beautiful  to clingers and people be  they success the care on  even a with You hardhats  of but captors with were  a supporters again to overseas  passionate thats is teachers Trump  only was its attack were  and get of transformation year  teamsters helps we you watching  not deals his basic one  the break can out so  hands to as just for  until are new run command  going for so respect he  were it drill kick high  Democrat a I the hes  of eating enough and must  VP leader status off able  he out you know All  political to That needed and  thats knows tells in say  establishment promise too the that  whole over to dont is  to angry off and and  Look we on let ABCs  issues the its promised can  When ready me be families  media the and would let  share us is that beat  member up hopey Now Because  theres capture and they libertarian  more House Donald were exposed  our You actually folks races  been be trade is quit  And God the able States  this strict stage would there  putting Ill will not freedom  America stage because in Not  these that to his rest  for the No on other  table their tragic again to  Chicago havent Tru deals street  what tear that theyre in  this things community Doggone competitive  behind give of is proSecond  find worked we Fighters America  work your says again elitist  that very squirmishes borders and  to well for no troops  got America frontrunner great havent  budgets rollers thing to quo  I we Donald you nations  know to weaker it the  ass Ill And and to  had theyre A all other  betcha are this a this  up changes man remember commonsense
By Rococo Modem Basilisk on January 21, 2016.
Right on.
Basically, if you don’t need interactivity, there’s no excuse for having it. If the point of the website is to display text, it should just *display text* and neither javascript nor css is justified (let alone a CMS).
To be honest, your site isn’t quite minimal enough for my tastes. You could have gotten away with pure HTML and no formatting, and it would have been better. But, at least you’re on the right track.
By Rococo Modem Basilisk on January 21, 2016.
I wonder if your experience is typical.
I wonder if your experience is typical. I have a lot of projects on github (and have for some time), and I’ve gotten something along the lines of four issues/ pull requests total. Having entitled trolls bothering you about your code doesn’t seem like it’s something common on github (although clearly they could be giving you the tools necessary to handle this better), and is (paradoxically) an indication that your project is extremely popular.
Part of the point of a system like github that makes forking projects easy is to eliminate pressure on upstream developers to implement features. In other words, while on an older project management system like sourceforge and google code having requests for features directed to the original authors of some code would be normal, github goes out of its way to shift the pattern such that people who want features are expected to create a fork and implement them themselves, before optionally issuing a pull request to submit the patch back to the source.
I wouldn’t blame github for this. Instead, somehow, you were targeted by a group of assholes who are too incompetent to implement the features they desire and too lacking in empathy to realize that you have more important things to do than to follow their whims. It’s absolutely appropriate to expect them to do their own homework, so to speak, and it’s a shame that they don’t seem inclined to do so (or to observe the basics of decorum).
By Rococo Modem Basilisk on January 25, 2016.
Read Intuition Pumps by Dan Dennett.
Read Intuition Pumps by Dan Dennett. It’s by far the best book on thinking tools I’ve read. (Compare directly to lesswrong’s “sequences”, which are similar in form but ultimately much inferior in both style and content.)
By Rococo Modem Basilisk on January 28, 2016.
I find this post significantly funnier than I should.
By Rococo Modem Basilisk on February 11, 2016.
MMOCs/MOOCs are probably the wrong match for the kind of learning technology presented in The Diamond Age, which while fictionalized, was really a small extension of existing trends in *educational game technology* that existed in 1995. In other words, rather than fitting into trends related to the extension of traditional academic curricula with ed-tech (as MOOCs and video lectures do), it fits more closely with the Papert/Kay ‘mathland’ concept (and thus with things like LOGO, Croquet, MindStorms, and other systems built to encourage autodidacts to explore systems).
The middle ground — and a very useful and popular middle ground it is — between typical formal-instruction MOOCs and ‘mathland’-style environments is the new breed of quiz-focused gamified ed-tech services like Memrise and Duolinguo. Despite being truly useful for a fairly limited set of fields (i.e., those fields where the number of right answers tends to be limited and people can reasonably expect to learn from quick quizzes with immediate feedback in the absence of initial instruction — in this case, vocabulary, grammar, and lists of facts), they are very useful in these fields.
Both The Diamond Age and Ender’s Game reference and borrow from educational games from the 80s and early 90s, adding context-awareness and extra flexibility as their primary technical advancement; for the most part, the actual passages described from the educational video games in both of those books are cribbed from real games, and there’s no reason why such games could not still be made.
We have yet another very popular field of educational games: simulations (Sim City, Civilization) — we simulate a complex existing model of a system and the player is expected to become competent at understanding the behaviors of that model in order to execute his or her tasks. Closer to the Croquet model is Minecraft, which is educational insomuch as necessary tasks embed a great deal of fairly esoteric information about geology and about historical methods of producing common goods: Minecraft makes explicit the tech tree that Civilization uses as a progress bar, and requires players to demonstrate a familiarity with the general idea of what tools and materials are required for a given technology along with the previous technologies required to produce those tools and materials, at a much more granular level than Civilization.
Yes MOOCs rely too much on bad video. But, MOOCs are thirty years behind the curve on ed-tech and have never even attempted to be cutting-edge or maximally effective. If they had, they would at least focus on testing mechanisms rather than on methods for replacing books with AV presentations — because if engagement is sufficiently high, books are just fine.
By Rococo Modem Basilisk on February 26, 2016.
Talking about Dribble is undercutting exactly how stupidly static the western idea of ‘good design’ is. Instead, we can look at how dominant themes in design go back to being influenced by Apple’s design standards from the early 1980s, which themselves are basically a simplified and dumbed-down version of late-1940s and early-1950s trends in modernist minimalism.
While there were plenty of interesting ideas in design from 1950 to 1980, because of Jobs’ reality distortion field we basically threw all of that out and have been increasingly influenced by this static throwback FrogDesign BS that was pretty questionable in 1979 (along with the underlying Jobsian ideology that consumers are too dumb to deal well with choices and must have decisions made for them — a necessary element in the application of extreme minimalism to ostensibly general-purpose machinery).
Today’s idea of good design is essentially: flat, spare, minimalist graphics with low information density and few user choices. Never use color if you can use white; never use two colors where you can use one; never use two buttons where you can use one and never use any buttons if you can get away with not having any; never give the consumer an option if you can please more than half of them by choosing first; if a user disagrees with a designer about usability then the user is wrong. These rules make some sense for a house or a toaster, and are defensible for a coffee machine or a toilet; they make no sense when applied to computers and even less sense when applied to computer software.
By Rococo Modem Basilisk on March 11, 2016.
Talking about Dribble is undercutting exactly how stupidly static the western idea of ‘good design’ is. Instead, we can look at how dominant themes in design go back to being influenced by Apple’s design standards from the early 1980s, which themselves are basically a simplified and dumbed-down version of late-1940s and early-1950s trends in modernist minimalism.
While there were plenty of interesting ideas in design from 1950 to 1980, because of Jobs’ reality distortion field we basically threw all of that out and have been increasingly influenced by this static throwback FrogDesign BS that was pretty questionable in 1979 (along with the underlying Jobsian ideology that consumers are too dumb to deal well with choices and must have decisions made for them — a necessary element in the application of extreme minimalism to ostensibly general-purpose machinery).
Today’s idea of good design is essentially: flat, spare, minimalist graphics with low information density and few user choices. Never use color if you can use white; never use two colors where you can use one; never use two buttons where you can use one and never use any buttons if you can get away with not having any; never give the consumer an option if you can please more than half of them by choosing first; if a user disagrees with a designer about usability then the user is wrong. These rules make some sense for a house or a toaster, and are defensible for a coffee machine or a toilet; they make no sense when applied to computers and even less sense when applied to computer software.
By Rococo Modem Basilisk on March 11, 2016.
There’s a problem with people recommending ‘smart guns’: they clearly haven’t looked at the existing market for mechanical trigger locks, the best of which are still comically insecure enough that a six year old with a paper clip and two minutes can disable them. The market doesn’t have sufficient incentives to make gun locks as secure as bike locks, and it shows. Producing a more complex system for in-firearm locking, featuring electronics and biometrics, will not be profitable if people aren’t willing to pay a few dollars for a trigger lock that actually has tumblers.
There’s a system in place for gun owners who want to secure their firearms: gun safes. Gun owners who do not want to secure their firearms will not pay extra for an add-on or a special more secure gun, and people will quite reasonably resist any legislation that requires new firearms to have such features (Apple can’t keep its own products working reliably, and fingerprint readers aren’t very good at eliminating either false negatives or false positives — how could you expect Mossberg to do a better job, or for someone to want to add a battery pack to an object whose job it is to sit unused for its entire lifetime except in the rare situation that a home intruder appears?). Even if new firearms all had such a system, new firearms are not the majority of firearms on the market: unless the sale of older firearms was banned or older firearms were gathered and destroyed (two very unpopular ideas), the only result would be a sudden drop in firearm sales and a lot more import sales from overseas of older weapons; and, this is if we only account for legal sales.
One must also consider the culture of firearm owners. Firearm owners in the united states skew a bit to the right these days (although any group not firmly in the center and any group with legitimate concerns about the reliability of government-provided policing are more likely to own guns: consider the black power movement as an example of a decidedly leftist cohort with a pro-firearm stance based on a completely legitimate expectation that police would not protect their community), and are going to be suspicious of any mechanism that could get in the way of using a firearm when they decide they need to, particularly if that mechanism is coming out of a government they don’t trust. There’s also, at least within the subset of gun owners who are essentially functional, a culture of gun safety: children are taught how to be safe around firearms long before they are allowed to put their hands on them, and mistakes like poor trigger discipline or poor muzzle control are treated not merely as dangerous but as social faux pas that invite mockery (anyone who puts their finger inside the trigger guard is considered a buffoon from the hated out-group who doesn’t even know how to shoot a gun, and as a person whose presence is dangerous due to stupidity and incompetence). Legislation that interferes or duplicates the function of that acculturation comes off as tone-deaf and disconnected to people with that acculturation, even as it may protect the family of gun owners outside of gun-owning culture (who are at greater risk of accidental discharge because they are unaware of the kind of safety procedures that are drummed into infants in gun-owning culture); this legislation comes from the out-group and is for the out-group and thus seems unnecessary.
By Rococo Modem Basilisk on March 16, 2016.
> But the reality is most people don’t want just a universal basic income.
Source, please.
Certainly, I agree that SV is biased by extremes of wealth and privilege, and that their ‘solutions’ often don’t work outside of the context of rich white/ asian males living in California and working in tech. However, a stopped clock is right twice a day: regardless of the reasoning used by one group of supporters, UBI is desirable to a number of very different groups for a variety of different reasons — not least by the precise opposite of SV, current minimum wage (or less) earners, many of whom would very much like a modicum of flexibility without the perverse incentive structures provided by current safety net systems.
The suggestion that people would reject UBI as charity is a delusion of people who understand neither UBI or actual want. After all, by definition, with UBI *everyone* would get the same income. This is not a matter of SV folks giving money to the poor; instead, this is a matter of everyone from billionaires to the homeless getting the same living wage.
By Rococo Modem Basilisk on March 17, 2016.
I’m less concerned about actual scientists falling into this trap, because often they don’t (and those that do often don’t matter). However, science popularizers fairly frequently fail to adequately inspect their biases & disregard the philosophy of science without understanding anything about it; science popularizers aren’t always even scientists (Bill Nye is a retired aeronautical engineer, and while Dawkins and Sagan were at one time research scientists they stopped their research in favor of writing for a general audience about science), yet they have a great deal of control over the popular view of science.
There’s a lot of variation here in terms of how reasonable they are, as well — Sagan almost certainly had heard of Karl Popper, and wrote about what one could reasonably consider to be the philosophy of science with a pro-science bias but with enough emotional distance not to completely disregard important limitations; meanwhile, Nye seems to think that philosophy is all about Plato’s Cave, and Dawkins doesn’t understand that the objective truth of scripture is a tiny and unimportant part of religion. I’m not quite sure where NDT falls here, but I feel like from what I’ve seen, he’s closer to Dawkins and Nye than to Sagan.
Ultimately, if actually active scientists have foolish beliefs about the philosophy of science, it doesn’t much matter: it doesn’t impact their ability to perform lab work and do reasonable analyses, generally. But, when you replace Bertrand Russel with Bill Nye, you’re going to have a bad time: suddenly, large chunks of humanity disregard important ideas and avoid using useful cognitive tools for, essentially, theological and tribal reasons. (And, to a certain extent, even Feynman is guilty of this: the macho arrogant physics-centrism he popularized has really screwed over a couple generations of otherwise promising people by giving them an easy way to dismiss important ideas.)
By Rococo Modem Basilisk on March 21, 2016.
I’m going to have to point out that this survey will be biased.
I’m going to have to point out that this survey will be biased. At least this is a Stack Overflow survey rather than a Y Combinator survey (which would be far worse), but even here — I use SO all the time but have never been asked to fill out this survey; many if not most people who use SO in the course of their jobs or for help with personal projects don’t even have accounts (because SO works better as a passive repository of historical good answers to common questions than as a means to have a novel question answered in a reasonable length of time — I’ve asked five or six questions and I’ve only ever gotten one response, months after I posted the question). In other words, only people who have an SO account and frequently ask or answer questions will be represented — and then, only those who feel like they have enough free time to fill out a survey.
By Rococo Modem Basilisk on March 22, 2016.
OK, OK, but a lot of these things are still toys.
OK, OK, but a lot of these things are still toys. The mouse is still a toy, and the WIMP GUI is still a toy; the original mac was a toy for reasons related to hardware costs and hardware budget being eaten up by industrial design rather than by RAM, but the general policy that computers should be strictly limited to performing the tasks the designers thought of beforehand in the way the designers thought they should be done has continued to plague the line since 1982. Facebook was never intended to be more than a toy, and really isn’t: it’s a fun and profitable toy with very wide appeal, but it’s not as though anyone trusts it for genuinely important communications. Being profitable does not make something not toy-like, nor does becoming large in scale.
By Rococo Modem Basilisk on April 4, 2016.
A quarter page of cliches about how bubble gum media is bad pitched at an imaginary audience who has never heard the term “bubble gum media” is the pinnacle of bubble gum media, and exactly the trend in medium posts that I hate. If you’re going to write some unoriginal content, at least make your post thirty pages long and entertaining like Cuepoint and Backchannel do.
By Rococo Modem Basilisk on April 8, 2016.
Dear Recruiters,
Since you lot seem to be both very eager to spam me and pretty out of touch with reality, let me clarify a few things.
1. I do not live in New York City. Yes, I know my LinkedIn profile says “greater New York City area”. That’s because the entirety of New England is considered part of the “greater New York City area” by LinkedIn. I am not going to commute to New York City. I am not going to move to New York City. There is no benefit to tripling my rent in order to live in a broom closet and work for a hedge fund. 2. Emailing me out of the blue is probably not going to work out very well for you. If I’ve heard of the company you represent, I probably either dislike them or suspect that you don’t actually represent them. If I haven’t, why would I care? I have a job right now, as you can see from my profile. Sending me more emails is also not going to help. 3. What the hell makes you think I want to work in finance? Nothing about my profile mentions it. No, I don’t want to work in web design or for a startup either. 4. If I receive an email from you, I figure odds are that you aren’t actually a recruiter, or aren’t actually representing whoever you claim to be representing. Why would a big company go trawling LinkedIn and emailing random junior developers who are already employed? If you want my attention, prove to me that you aren’t just a con artist. And do it in the subject line, because I’m not going to open your email. 5. On second thought, maybe just stop? You’re wasting minutes of your time and seconds of mine.
By Rococo Modem Basilisk on April 8, 2016.
It’s kind of funny that your way of dealing with branching narratives of this type is to play through only once and treat your path as canon, since the norm (at least in VNs, which are generally shorter and contain fewer mechanical challenges) is to get 100% completion (to the extent that, in many games, the ending the creators consider ‘canon’ doesn’t even become unlocked until all the ‘normal’ paths are visited — see Everlasting Summer, for example, whose core plot is only vaguely hinted at until you slog through all the formulaic dating sim BS, or Sharin, which does something similar by unlocking whole new dimensions of meaning in throwaway lines from previous play-throughs in what is essentially an easter-egg route that nevertheless is necessary to play through in order to get completion).
I wonder if triple-A games, because of play time and budget and aspirations towards ‘cinematic experience’, don’t play this way — but games focusing on branching narratives as their primary or sole mechanic are often built with the assumption that players will engage with multiple routes, and reflect this thematically (with plotlines involving time loops, time travel, alternate universes, and so on — everything from Steins;Gate to Higurashi does this). This is a very different way to engage with a game. It’s not precisely leaning on the fourth wall, but instead, taking greater advantage of an already existing mechanic for thematic reasons, telling a story that is much more difficult to tell in non-branching media. (There are attempts to tell this kind of story in film. Run, Lola, Run, for instance, or Tatami Galaxy. They feel more like formal experiments than direct experience because the choices aren’t directed by the audience, and this detracts from their effect; less competent attempts are even less memorable because of this.)
By Rococo Modem Basilisk on April 11, 2016.
The bigger the bot hype grows, the more apparent it becomes that when it comes to bots, for now at least, being interesting and making money are mutually exclusive. The bots that are being productized now are just special-purpose CLIs with none of the UX effort that goes into making CLIs actually useful.
By Rococo Modem Basilisk on April 14, 2016.
Bot capitalism will fail
There’s been a lot of hype surrounding chat bots lately. I love bots, and usually I’m all for getting excited about the things I love, but I think the recent hype is very misguided. The reason is that the people contributing to the current hype bubble surrounding bots are not natural persons but corporate persons: they are excited about bots as products. Usually, when the spectre of money enters into a domain previously commercially nonviable, a lot of people get super excited about making money and miss the point spectacularly (pun kind of intended), and this case is no different. However, it’s worth talking about this specific case nevertheless, because with bots, the commercial focus of yuppies and suits is fairly likely to ultimately convince everyone outside the core art-bot community that bots are something worse than useless: that they are supremely uninteresting.
The first thing I’m going to discuss is conversational interfaces. The reason is that the sudden increase in interest in bots is related to the fact that several companies have been shipping speech-based conversational interfaces, and a lot of current commercial bots are intended to be the equivalent of these speech-based interfaces run over existing text-based communications protocols.
I am largely unimpressed with conversational interfaces. They have a long history; many early and influential AI systems would be classified as conversational interfaces, and very simple conversational interfaces were a staple of books on learning to program for the BASIC set since the introduction of the first 8-bit home computers.
Ultimately, conversational interfaces fall into two categories: interfaces that try to keep up the illusion of intelligence and personality by ignoring most input and searching for particular keywords, and interfaces that are effectively special-purpose command lines with snarky or otherwise unprofessional error messages. The former type is epitomized by the search engine “Ask Jeeves”, which achieved its “flexibility” by implicitly inserting an OR operation between each term in the input (thus causing the term with the greatest TFIDF score to rise to the top of the results and become a de-facto keyword), although other examples include Alice and Eliza. The latter type is epitomized by the adventure game “Zork”, which had a somewhat english-like and very limited programming language it could understand and would mock you if you attempted to perform an invalid operation. Siri, Cortana, Echo/Alexa, and Google Now are all combinations of these two forms.
The thing about conversational interfaces of the former type is that they are inflexible and difficult to predict. A given input will produce some output, based on pattern-matching, but in order to prevent seeming as though the machine is not intelligent, the machine will be inclined to always respond with something — ideally, something somewhat randomized. A human being, without access to the source code, will eventually build up a folk-model of what patterns do or do not produce the desired result, but such a model has no guarantee of accuracy or completeness. A user may have some operation they desire that is built-in, but the complete list of available functions (though it is necessarily small, since each one has to be hand-written by a human being and given rules for invocation that don’t conflict with other functions) will never be distributed to prospective users because that would ruin the “magic” of a somewhat human-like interface. The ideal end-game for such an interface is for a user to memorize a handful of commonly used patterns in their most consise form and otherwise use it for its novelty value as a conversational partner — a world of people barking “MOVIE SHOWINGS BROOKLYN DEADPOOL” at their phones instead of typing the same query into google.
The thing about conversational interfaces of the latter type is that, by being english-like, the language understood by the interface will never be sufficiently minimal for a non-casual user, and by being ‘entertaining’, the error messages will never be sufficiently specific for a casual user to be able to trivially determine what he or she is doing wrong. The ideal end-game for such an interface is for a user to memorize a needlessly verbose and limited programming language and be able to type “FEED TROLL TO TROLL” with the expectation that doing so will cause the troll to eat himself and be defeated.
Taken to infinity, the ideal form of the first type is a search engine. Taken to infinity, the ideal form of the second type is a unix shell.
Commercial bots, to the extent that they are expected to reliably perform potentially dangerous operations like making purchases, editing calendar entries, and controlling home automation systems, are going to remain quite close to the second form. This is a shame, because there is absolutely nothing revolutionary about a shitty command line, and it doesn’t do justice to bots in general to imply that all of them are like that.
Consider the non-commercial bot: the art-bot. The art bot is varied in its form. The art bot, because it is a bot, is able to tirelessly perform intellectual tasks. The art bot, because it is art, focuses on tasks relating to recontextualizing ideas, words, images, and perceptions. The art bot is a meaning factory, producing brand new thoughts out of the interference pattern between a PRNG and an audience.
The art bot doesn’t buy anything, or if it does, the fact that you can’t reliably tell it what to buy is part of the point.
The art bot can write music, or poetry, or paint pretty pictures. If you don’t like the art that the art bot produces, too bad. The art bot doesn’t care. The art bot will produce a thousand other pieces while you are deciding whether or not you like that one.
Some art bots tell you to do things. Do you want to take commands from a robot? Maybe. Sometimes the things it tells you to do can’t be done.
Some art bots are funny. Some are even intentionally funny. Any image macro, snow clone, or formula joke is the potential domain of an art bot, who will scour a dictionary to produce millions of variations.
An art bot is not a shitty command line. Or, if it is, it’s intentionally shitty. An art bot might mock you for everything you tell it to do. Or, it might systematically break down your hopes and dreams. Or support them.
Freed from the shackles of needing to please a core user base and be immediately useful without ever screwing up, art bots are allowed to be interesting.
By Rococo Modem Basilisk on April 19, 2016.
Bots are only interesting when they aren’t expected to be consistently useful or make money.
Bots are only interesting when they aren’t expected to be consistently useful or make money. The constraint of productivity saps away the only interesting thing about interacting with bots: serendipity.
The current boom in interest in commercial bots is just another example of the Spectacle trying to look hip by consuming something that can’t be effectively productized.
By Rococo Modem Basilisk on April 25, 2016.
I’d like to point out that Project Xanadu (which, btw, is still going on) is, at its core, an attempt to solve this attribution problem permanently (at least for all good-faith actors) by focusing on quotation as a primary factor in composition (seeing editing as a form of selective quotation and rearrangement from previous drafts, for instance). Every so often I see articles like this (or articles on similar topics like music credits) that point out how necessary this tech still is — this clearly-skilled guy would still have his job if, by default, copy and paste between applications implicitly kept source attributions.
By Rococo Modem Basilisk on April 25, 2016.
How Bots Were Born From Spam
How Bots Were Born From Spam
[1]
The first commercial spam message was sent in 1994—at least that’s the general consensus. Lawrence Canter and Margaret Siegel had a program written that would post a copy of an advertisement for their law firm’s green card lottery paperwork service to every Usenet news group — about 6,000 of them.
Because of the way the messages were posted, Usenet clients couldn’t filter out duplicate copies, and users saw a copy of the same message in every group. At the time, commercial use of internet resources was rare (it had only recently become legal) and access to Usenet was expensive. Users considered these commercial-seeming messages to be crass—not only did they take up their time, but they also cost them money.
In reaction to the “green card” incident, Arnt Gulbrandsen created the concept of a “cancelbot,” which compared the content of messages to a list of known “spam” messages and then, masquerading as the original sender, sent a special kind of message to “cancel” the original message, hiding and deleting it. Two months after the original spam postings, Canter and Siegel did it again — upon which the combined load of spam and cancel messages crashed many Usenet servers. Anti-spam measures, it seems, had themselves become spam.
[1] A Usenet client, showing message groupings. A message cross-posted to multiple groups would only appear once. Image credit: Public domain
While this was the beginning of commercial Usenet spam, this was not the beginning of Usenet spam in general. Prior to April of 1994, a poster known as Sedar Argic would automatically reply to any message containing the word “turkey” with a lengthy rant denying the Armenian genocide. This, of course, made discussions of Thanksgiving celebrations difficult.
The thing about all of these early forms of Usenet spam is that the messages were always identical. Cancelbots worked because the messages they were canceling were either identical or changed very infrequently — they could be compared to a human-maintained list of spam messages (a “corpus” of spam).
But even during this era there were Usenetters using a new technology that would upset this and future countermeasures: Markov chains, which are a popular tool among modern bot-makers. Invented in 1913 by Russian mathematician Andrey Markov, a Markov chain works by combing through text, looking at which words tend to follow each other, and assembling new sentences, paragraphs, and pages using the resulting statistics. Want to try it? Here’s a website that generates filler text from Shakespeare, Jane Austen, the Nixon Tapes, college essays, and even the Bible.
It didn’t take long for spammers to realize that cancelbots could be stumped by adding random junk to the end of messages. At the same time, spammers were moving beyond Usenet into email, just as regular people all over the United States and western Europe were suddenly learning what a modem was and signing up for web access.
By this time, people dedicated to identifying and fighting spam (a problem that had barely existed six months earlier) had already started creating “honeypot” email accounts. These were accounts that no human being would have any reason to send messages to, created for the purpose of accumulating a large corpus of spam, with the goal of researching spammer behavior and developing new spam-fighting techniques. With so many spammers carrying different messages, and random junk being newly added to the end of messages (or beginning, or middle), spam-filtering technology had to get smarter. Programmers began to look at word statistics and Markov models to identify the spammers.
But spammers quickly figured out that they could use the same Markov chain technology against the filters: By creating Markov chains out of clearly non-spammy material (usually derived from Project Gutenberg, a collection of out-of-copyright e-books), spammers could add legitimate-sounding but nonsensical phrases to the end of their messages, making the job of the filters harder. That technique is called “Bayesian poisoning” and is the origin of spam poetry.
Unfortunately for spammers, Bayesian poisoning tends to make messages too unconvincing: Long strings of unrelated words don’t sell. But there’s another way to get around blacklists based on a corpus of text—a technique that became all too common when people started including comment sections on the nascent web. In the spam community, it’s called “spinning.” The rest of us know it as “generative grammar.” Spinning uses variations on phrases in an existing message to create large numbers of semantically identical but distinct messages. Like Markov chains, it’s popular within the bot-making community, and you can try it for yourself here.
[1]
Shortly after email and web browsing became the norm, instant messaging followed. While chat services date back to the early 1970s, large-scale internet-based chat systems like IRC appeared in the late 1980s. As people started growing up with internet access in their households, commercial services like AOL Instant Messenger boomed in popularity.
On IRC in the 1990s, a lot of what had happened on Usenet repeated itself. People wrote Markov chain bots for amusement; other people wrote bots to paste pre-written diatribes in response to particular keywords. Some spambots existed that posted advertisements automatically. But the IRC community, like Usenet, quickly developed technical countermeasures.
Commercial instant messaging services, on the other hand, skewed young and nontechnical. Whereas IRC and Usenet were used and run mostly by programmers, AOL was targeted toward families. When bots appeared on AOL Instant Messenger, AOL had no incentive to stop them; when bots began sending misleading messages to AOL users, the company didn’t have enough experience with spam to be cautious of where this might lead.
Meanwhile, some AOL bots like SmarterChild and GooglyMinotaur were officially sanctioned. Despite the commercial angle, these bots wouldn’t message people without provocation and, thus, arguably were not spambots. Still, their underlying technology was identical, and their attempts to act human represent not merely a precursor to similar systems like Siri, but also a less sinister version of what instant messaging bots at the time did to trick naive teenagers.
[1] A conversation with SmarterChild. Image credit: TheFirstM
If you’ve ever used Twitter, many of the spam techniques I’ve mentioned above will be familiar. You already know that users who post links only are unlikely to be human, particularly if they have a supermodel avatar. At some point, you’ve probably inadvertently mentioned some buzzword (iPad, Bitcoin, etc.) only to be swarmed by tangentially related ads.
Other applications of these spam techniques on Twitter, however, are more interesting. Some, like RedScareBot, are subversive. Others, such as StealthMountain, are educational. Some use normally questionable time-wasting techniques for the greater good by redirecting abuse — such as an ELIZA implementation that engages people using Gamergate-related tags, causing naive trolls to flame a bot rather than a human.
But there are many other modern use cases for these technologies, too. In the academic world, in response to a series of scandals related to fraudulent conferences, a tool called SCIGen was developed that used spinning to generate nonsense papers as a way of ensuring that journals and conferences were doing peer reviews. In 2014, IEEE and Springer, two major academic publishers, adopted the use of a tool for automatically detecting nonsense papers generated by SCIGen after it was revealed that more than a hundred such papers had gotten around peer reviews.
In 2010, Amazon opened its eBook store to self-publishing only to be flooded with e-books made automatically by web scrapers. While content farms for clickbait sites are mostly run by poorly paid humans, The Associated Press is using spinning techniques to generate sports and finance articles, and others are building bots that can write clickbait.
[1]
What this all leads to is unclear. Science fiction author Charlie Stross suggests, in his 2011 novel Rule 34, that the competition between spam and anti-spam technology might drive forward future AI research. In his novel, a superhuman AI evolves from an experimental spam-filtering technology and, as a side effect, has no internal sense of self: It projects its consciousness on some arbitrarily chosen user, because its intent is to determine what that user would consider spam.
Hugh Handcock, another science fiction author, suggests in a recent blog post that the future of chatbots may have more in common with the spambots and mass-trolling of Anonymous and early-1990s IRC than with Siri. Chatbots, by design, might be more desirable to interact with than humans are — they could perpetuate rather than break down filter bubbles, becoming something to interact with without ever leaving one’s comfort zone. They might swarm around dissenting opinions. Handcock presents a world in which a human might know that all his friends are bots trying to sell him something—and simply not care.
Meanwhile, 1990s virtual reality pioneer Jaron Lanier, in his 2010 book You Are Not a Gadget, presents his concerns about current trends in publishing and media where the monetary value of artistic expression is tied to advertising. In his 2013 follow up, Who Owns the Future, he offers a possible end game for an advertising-driven society: one wherein physical spambots provide goods and services for free to people in their target market, while leaving everyone else to starve.
The second episode of the TV series Black Mirror, “Fifteen Million Merits,” dreams up a similar society—an economy based on the twin poles of entertainment and physical labor that extracts money from laborers and funnels it into the entertainment complex by using aggressive advertising that can only be accepted or dismissed using micro-transactions.
Lanier suggests that voluntary micro-transactions might be a way for artists to take back control of their work from the advertising industry and avoid an imminent fall of media from a middle-class to a lower-class position. The Black Mirror episode shows how as long as the entertainment industry is centralized, however, micro-transactions can be a tool for perpetuating class divides and systematically excluding people from participating in the creation and sale of art.
Personally, I suspect that with the new emphasis on conversational interfaces, we’ll begin to see hybrid spambots: Existing conversational interface systems like Siri and Echo, because they serve up data from third parties, might begin to be manipulated by some bot-equivalent of SEO to respond to certain queries with advertisements. In this environment, no automated methods exist for filtering out ads — and since conversational interfaces are often run by retailers, there is no incentive to do so. Rather than trying to outwit spam filters, the creators of these bots would need to be subtle enough to avoid alarming users.
As the landscape of the internet changes, and as countermeasures are put in place, one thing remains constant: So long as spambots can remain profitable, they won’t go away.
This post is part of How We Get To Next’s Talking With Bots month in May 2016, looking at how chatbots will change our lives. If you liked this story, please click on the heart below to recommend it to your friends.
Read more from How We Get To Next on Twitter, Facebook, or Reddit, or sign up for our newsletter.
By Rococo Modem Basilisk on May 6, 2016.
If you’re going to replace regex half-way with UML and expect people to use a graphical editor, why not just use real UML and generate regex from it? Anyone who finds your visualization more accessible than regex is already more familiar with UML than with regex, after all, and probably already has a preferred UML editor; anyone who uses a console-based editor (which is to say, a whole lot of serious developers) won’t be able to use this (and will be philosophically unwilling to anyhow).
By Rococo Modem Basilisk on May 16, 2016.
This isn’t a feature of some new revision of UML.
This isn’t a feature of some new revision of UML. This is one of the things UML was being used for in the late 90s. Most Java textbooks, for instance, use UML to describe Java syntax.
By Rococo Modem Basilisk on May 17, 2016.
Another possibility: do nothing, and allow ad quality to degrade.
Another possibility: do nothing, and allow ad quality to degrade. Much as email spam & phishing messages are intentionally unconvincing to factor out the possibility of intelligent and savvy people caught in the net and causing trouble, one can intentionally ensure one’s ads are removed by ad blockers and then specifically target groups that are unlikely to understand the very concept of ad blockers (seniors, children, the terminally unaware).
I’m not convinced that advertising as an industry can ever be saved from its downward quality trajectory: this trajectory is built in to the concept of advertising, and while the drop can be slowed or even backed up slightly, it cannot be stopped. We are living in the era of Late Advertising (by analogy to Late Capitalism), and like the Accelerationists, our best bet is to make advertising as bad as possible in order to hasten its total irrelevance.
By Rococo Modem Basilisk on June 6, 2016.
The author of this piece managed to write an entire article about the concept of slack without once mentioning the Church of the Subgenius, and the long history of interplay between the philosophies of half-serious irreligions and hacker culture. Hell, he didn’t even mention Slackware.
In analyzing what slack means and why people care, it’s important to understand the other aspects of the slack memeplex: specifically, it’s important to understand the Church of the Subgenius, what aspects of the world it specifically satirizes, and its interplay with Discordianism. One reason is that, if anything, the CoS’s satire is becoming more and more relevant — and was always a great deal more on-point and meaningful than more mainstream variations like pastafarianism.
Where Discordianism sets its sights on control freaks by mixing eastern philosophy with greek mythology and creating a religion venerating chaos, the Church of the Subgenius sets its sights on capitalism and The Spectacle by combining evangelical christianity with UFO cults and creating a systematic veneration of laziness (which they call slack).
The messiah of the Church of the Subgenius is L. ‘Bob’ Dobbs, an idiot-savant con-man who can “sell anybody anything”, and he was tasked with leading the descendents of yetis to a pleasure planet. Membership costs thirty dollars, and eternal salvation is guaranteed “or triple your money back”. This is the matrix of ideas from which the concept of ‘slack’ emerged, and this is why it’s important: slack is an essentially subversive concept that stands simultaneously with and opposed to all the tacky late-night-tv Ed-Woods glory of ironic ad-worshipping hipsterism that substitutes bigfoot for Chuck Norris as an icon of ancestral masculine virility.
By Rococo Modem Basilisk on June 13, 2016.
One small complaint: you mention Moore’s Law as one of the reasons ML is having a rennaisance; while it’s true that there’s an exponential growth trend involved, the trend in question is not actually Moore’s Law (which ended sometime between 2005 and 2007 depending on who you ask). Moore’s Law has to do with the number of transistors that can be fit on a single die — a number whose hard limit we’re bumping up against (because of the size of atoms); mechanisms to get around this limit don’t involve transistors as such and so the moniker doesn’t apply to them.
In popular science writing, Moore’s law has become a shorthand for all forms of exponential growth that affect performance-per-dollar or performance-per-inch of computer hardware. CS people sometimes use it this way, but (when it matters) make distinctions — and there are a number of other “laws” that are essentially similar in nature but apply to different metrics: Kryder’s law for storage density, Koomey’s Law for performance per watt. And then, there’s Englebart’s Law (which all these other accelerating performance laws are arguably a special case of): that the performance of human beings increases exponentially in all sorts of contexts and domains.
By Rococo Modem Basilisk on June 23, 2016.
Regarding the ‘preferred music service’ example, it’s a bit funny that Amazon is not extending the (Android/Palm) concept of ‘intents’ with simple fallback logic (“alexa, play me song X” can be handled by this list of three skill providers in preference order, so search the first and if it can’t play this song then fall back to searching the second).
By Rococo Modem Basilisk on June 23, 2016.
There’s a difference between the Lake Woebegone effect and the Dunning Kreuger effect.
There’s a difference between the Lake Woebegone effect and the Dunning Kreuger effect. While the Lake Woebegone effect means that we tend to rank ourselves above average more often than is possible, the Dunning Kreuger effect has to do with scale: the degree to which we overestimate our abilities is inversely proportional to our actual skill level.
By Rococo Modem Basilisk on June 23, 2016.
Novelty, Perversity, and Randomness
I recently read a blog post (never mind which one, since this is a pretty common position) railing against “rules of thumb” for writers that make generalizations about readers. This post made the argument that a readership, because it is heterogeneous, cannot be generalized about. Such an argument is untrue (one can take the mean, median, and mode of extremely diverse data sets and still get somewhat valuable information from that), but more crucially, it’s untrue in a boring way. It’s true that rules of thumb that make generalizations about readership are problematic, but this is the case for a much more interesting reason that I’ve never seen articulated.
First, I’d like to get something out of the way. These rules of thumb and generalizations about readers exist for a reason: specifically, they are useful to two different groups of authors. One group is expressly commercial: people whose primary or sole goal is to maximize their sales will want to optimize for a large readership, and will therefore want to model their readership and appeal to this model. The second group is the aspiring amateur: someone with no experience and no model at all of a readership or of the way in which one goes about writing benefits greatly from the confidence provided by any direction at all, even if the direction in question is anecdotal, misleading, limiting, or false; arbitrary advice benefits these people.
The real reason why rules of thumb are a problem is that readers read in order to satisfy a hunger for perversity. By this I mean that readers are looking for a special kind of novelty: within a constrained system (such as a fictional world, a genre, a style of argument or of writing, or a set of themes), a path through this domain is created that is plausible but surprising. The author is an engineer of subversion, creating expectations and then delighting the reader by perverting them. Rules of thumb, whether or not they correspond to tendencies desirable to readers, are best understood (as TvTropes notes) as a set of rules or expectations in the mind of the reader — in other words, as the starting point of subversion.
However much a reader may love some trope, the reader will love an interesting perversion of that trope more.
Writers, like readers, are human beings, unfortunately. Human beings are a bit too good at categorizing and finding connections: this is a liability when it comes to producing novel ideas or determining how novel an idea is once produced. Randomness-driven “writing machines” like cut-ups, bibliomancy, Cards Against Humanity, and similar techniques can introduce novelty unlikely to be produced by a human mind unaided; constraints like those in Oulipolian writing games or those used by Dr Seuss can force novelty from the reader’s perspective by warping the environment the writer is navigating: the most straightforward choice of words for a writer unable to use the letter ‘e’ will seem very strange to a reader not accustomed to playing the same game.
It is important to note that the novelty of a work is a property of the mind of the reader, not a property of the work itself. In other words, an experienced reader has more discerning taste as an inevitable consequence of that experience: anything is novel to a tabula rasa and nothing is novel to an omniscient being. As a result, the game of writing can never be won. Every set of expectations is the end result of someone subverting previous expectations. Furthermore, we can divide up groups of people by what expectations they have (and thus, what they find novel), essentially based on what they’ve already been exposed to.
A work like The Last Ringbearer is in dialogue with Lord of the Rings, but more importantly, it is a perversion of expectations created by Lord of the Rings; its value is lost on anyone who isn’t familiar with the original work. To a lesser extent, a work like Neuromancer is explicitly a perversion of the expectations set up by golden age SF: where Asimov would have given the protagonist role to someone in a role of power or authority, Gibson gave the protagonist role to someone incapable of changing even his own destiny, explicitly on the verge of being killed by minor criminals with the implication that he wouldn’t even be remembered, whose role in the story is that of a convenient tool for powers beyond reckoning; where Heinlein would have given us a wise-cracking Bill Murray protagonist and Asimov would have given us a Spock, Gibson gives us Case as Shinji Ikari; where any golden age author would have described only the important parts of the environment and shown us a gleaming, streamlined future, Gibson fixates on the tiny dents in the table of a diner and the smell of decaying newspaper piled in the entryway of a dilapidated storefront; where golden age governments are noble or evil, Gibson shows a world where government ranges from minor corruption to complete irrelevance. Similarly, Dune goes out of its way to pervert all our expectations: we have an aristocracy whose power is based on access to resources instead of an aristocracy based on the idea of genius loci and the divine right of kings as in most extruded fantasy product; we have faster than light space travel and a multi-planet civilization in a world without computers wherein most fighting is done using knives; our far-future society’s great new social and political movement is driven by an offshoot of Islam invented by an ecologist and practiced by desert nomads who ritually consume psychedelic drugs and drink their own urine. While these works can be consumed by people without the understanding of the context they are in dialogue with, the enjoyment gained by such readers is limited to that of intratextual perversion (i.e., plot twists), free-floating novelty (“what a cool idea!”), and expectations based on other domains like the real world; rather than being a lazy new trend by people trying to push cinematic universes, intertextual perversion is the primary defining factor in whether or not a creative work is considered seminal or culturally important within a genre. To use marxist dialectic terminology, a seminal work is one that is the antithesis of all that has come before it, and forces the genre thereafter to synthesize it. Every work in a genre is a reaction to each seminal work in that genre: either supporting it or reacting against it.
We can consider a genre to be defined by the parameters of its conventions: in other words, what attributes do its seminal works have in common? The domain of any interesting work in that genre, then, is in probing the unexamined assumptions of readers who have internalized these conventions: what things are these readers currently incapable of considering, and how can we surprise them by causing them to consider these ideas unexpectedly? Authors, being human, are probably also going to have a hard time considering unknown unknowns; they are advised to use machinery to aid them, since machinery has no problem being creative and original.
It is not that following the “rules” of a genre is foolishly stifling the wonderful underlying free creative spirit of the writer: any author who thinks that their underlying free creative spirit is something special never to be tamed is encouraged to look at their pile of rejection letters and reconsider a career outside the arts. Instead, following the “rules” misses the point entirely: the “rules” exist in order to make the game more interesting. Breaking the spirit of the law without breaking the letter, and vice versa, is the very manner in which writing is creative.
By Rococo Modem Basilisk on July 1, 2016.
As much as I hate to rain on your parade, I’m afraid I can’t agree at all with your premises.
As much as I hate to rain on your parade, I’m afraid I can’t agree at all with your premises. This article itself argues against you.
You’ve written a short article that shallowly rehashes ideas that were already long past their expiration date in the 70s, when they were being summarized in Future Shock. You cite studies that don’t support your findings, and complain about trends that are mostly imaginary. Ultimately, you’ve projected your own subjective feeling of burn-out onto the world at large, channeling it into a pastiche of what is essentially now a popular subgenre of theoretically non-fiction op-eds: the anti-internet thinkpiece.
Have you constructed a media diet for yourself that negatively impacts your life? Clearly. Is this common? Sure. Is it new? Not remotely. You’ve taken perfectly good tools and used them to cut your limbs off, and now you’re complaining that the tools are dangerous.
The only part of this article that is non-trivially correct is the part about multitasking & unfinished task cues. But, the idea that this mechanism should be used as an excuse to do some kind of internet detox is absurd. Instead, be mindful of your own cognitive biases, and take advantage of this effect as incentive to perform the tasks you intend to perform.
Easy access to information makes shallow understanding possible but does not encourage it; after all, it also makes much deeper understanding possible. If this access makes you tend toward shallowness, that’s an indication of your own intellectual laziness, not a reflection of the innate tendencies of the tools at your disposal. Practice some discipline, and these tools will help you gain more depth. (Without much discipline and with only a little foresight, it would have demonstrated to you that there is nothing original in this essay; in Medium’s daily recommendation email alone, I see three of this type a day, though they are often shorter and have fewer reviews or recommendations. You could have saved time by recommending some of those, rather than writing this yourself.)
By Rococo Modem Basilisk on July 5, 2016.
There’s another path here that you haven’t mentioned: take advantage of content generation technology and combine it with your own efforts. (This isn’t a new idea: a few months ago, at least two or three ‘creative autosuggestion’ projects were floating around; Burroughs edited the output of cutups, as did Bowie; after Deep Blue became the world’s best chess player, top-tier players started playing ‘centaur’ or ‘cyborg’ chess wherein they collaborated with chess programs.)
That said, being a part of the generative writing community, I think you slightly overstate the state of the art. Generative poetry is pretty good, in part because poetry is pretty flexible and unusual stylistic choices are typically seen as meaningful: generative writing does well in forms that are either extremely experimental or extremely formulaic. Machine-generated short articles performing numbers-driven reporting of factual events (such as sport or financial stories) are more or less indistinguishable from human-written stories of the same type, but also represent a fairly uninteresting application of this technology (in part because, while the AP and other news wires started using this technology only recently, it’s been possible for even a mediocre programmer to implement this kind of thing since the 50s). Long-form narrative-driven works don’t tend to be readable, although in the past few years there’s been a lot of progress (for instance, there was an entry in National Novel Generation Month in 2015 called MARYSUE that generated pretty convincing pastiches of bad Star Trek fanfiction, most of which is not much worse than human-written bad Star Trek fanfiction).
There are certain things that generative writing systems are very good at. These systems are capable of being intensely creative (in the sense that they can generate juxtapositions of ideas that could never occur to a human being), but lack any good approximation of human taste: as a result, writing systems can be used as a creativity prosthesis, where human judgement isolates good ideas from bad ideas while the machine performs the heavy work of producing ideas. Likewise, these systems are good at being exhaustive: it’s trivial to take a couple corpora and a format and produce every possible variation on a theme, if someone is willing to wade through the output looking for the interesting material. Human editing makes machine-generated writing much more feasible for even long-form work.
By Rococo Modem Basilisk on July 6, 2016.
Have you determined how much non-book content you are reading during the day?
Have you determined how much non-book content you are reading during the day? During a typical day, I end up reading several books worth of articles, not to mention emails. I’m loath to value books over other forms of written content impulsively: after all, books are often crap and articles are often quite good.
Is it valuable to be self-aware about the way in which notifications and consumption of short-form content performs operant conditioning on you? Sure; this is why I have notifications off on my devices, avoid broadcast television entirely, and try to avoid reading any article that takes less than five minutes (with articles taking 20 minutes or more to read taking precedence).
But, a “book” has a lot of cultural and structural baggage: books are associated with traditional publishing pipelines (meaning that a book is written about a year and a half before anybody who isn’t a professional author, editor, or reviewer reads it), with a particular structure (several multi-page sections called chapters, organized either by topic or chronology, numbered) and attributes (even ebooks are usually paired with paper equivalents, and so features like internal hyperlinks, interactive sections, and direct feedback should be avoided; updates, to the extent that they exist at all, are limited to later editions which must be bought separately; a book is rarely less than 90 pages long and rarely longer than 500, because otherwise binding it would be prohibitively expensive for an approximately seven USD price point). How many of these attributes of a book are, in of themselves, valuable? How many of them, for a particular subject, are desirable constraints rather than undesirable ones? When we consider that books might also need to be audio books, we add more constraints: no diagrams, no extensive use of homophonic puns or homoglyphic puns, write nothing that is inherently unpronounceable.
I love books. But, when we uncritically value the book format we sacrifice ourselves to a kind of shallow reactive traditionalism that ultimately runs counter to precisely the kind of attributes that book-loving people ascribe value to: deep consideration and intellectual bravery. A book is a tool, and to consider it preferable for jobs for which it is unsuitable for reasons of habit and social signalling is to devalue it.
By Rococo Modem Basilisk on July 8, 2016.
Bots are wonderful.
Bots are wonderful. But, for a variety of reasons that should be so obvious as to not merit mentioning, if you’re trying to make money by creating a bot you’re making a huge mistake. (Because people haven’t managed to get the memo, I’ve gone over these reasons three or four times on Medium alone. But, spending a few minutes thinking about the situation should suffice.)
Of course, if somebody expects to make money off of phone apps they’re already laboring under a huge set of delusions. Rather than disabuse them of specific ones, I would recommend they instead switch to a more lucrative career in problem gambling and/or attempting to be struck by lighting for the insurance money.
By Rococo Modem Basilisk on July 11, 2016.
A lot of the ideas about OO that are mentioned in Mr Scalfani’s essay were pushed and popularized by academic materials for teaching Java and C++ as first languages; to the extent that they are true in those languages, they are mostly true of toy examples. These materials hammer home these benefits as ideals while also pushing OO as a panacea, & are aimed at naive beginners, who lack the experience to argue coherently against them.
Using OO when and where it is appropriate, and using languages like Smalltalk that do OO ‘right’, is fine. It’s not fine to tell 15 year old newbies that OO is the best tool for every job, and that all Java code is OO — and that’s what practically every introductory Java textbook will do, because it’s literally part of the standardized APCS curriculum & part of the accreditation process for university computer science programs.
It’s not that OO should be attacked because it’s inherently terrible. Instead, there’s a fantasy idea of OO promoted by a whole industry, and that fantasy needs to be attacked because it is systematically producing incompetent programmers who write bad code.
By Rococo Modem Basilisk on July 25, 2016.
General purpose, in my view, is an overstatement.
General purpose, in my view, is an overstatement. (That said, maybe I work with an atypical set of problems.)
OO is appropriate when the least complicated way of modelling a problem is in terms of agents with internal states communicating. In other words, something like a physics engine is a natural fit for OO.
I mostly work with processing large amounts of text data between formats, occasionally doing analysis. (This is, as far as I can tell, a pretty typical programming job.) This kind of work lends itself well to multiple parallel stages with relatively little state, much of which is short-lived. In other words, it fits well with the UNIX pipeline model, and both OO and FP would be a poor fit. (I’ve had to work with other people’s attempts to shoehorn this kind of process into an OO framework; when I can get away with it, I replace a few thousand lines of Java with two or three lines of shell.) In other words, this is the ‘general case’ for my line of work.
I occasionally come across a circumstance where OO makes sense. I wrote an OS in D, and because D’s object system is similar to C’s structs and unlike C++’s objects, I was able to make excellent use of inheritance & polymorphism for wrapping abstraction around memory-mapped devices like VGA memory; I also made heavy use of object orientation when modeling some novel data structures for Project Xanadu. However, in none of these cases was it convenient or sensible to go full-OO — all of these cases were ‘mixed paradigm’ with a lot of procedural and functional code, and even when I wrote OO, I violated encapsulation whenever I needed to & barely took advantage of inheritance.
I understand why pedagogy around OO is so popular. After all, human beings deal with objects with internal state in real life all the time, so these concepts are familiar to non-programmers. Likewise, inheritance hierarchies mimic the form that every naive cataloguer produces. But, like John Wilkins in his Essay Towards a Real Character and the imaginary Chinese philosopher in Borges’ Celestial Emporium, we quickly find that reality doesn’t neatly conform to either a hierarchy of relation or a conception of objects with attributes.
In other words, OO is a convenient stepping stone to other paradigms, and is useful in of itself when a mapping of the problem space to a set of objects is either immediately obvious or (in the case of physics engines) provided in the literature, but its most useful attribute is the way in which it demonstrates its own limits. Despite this, because most formally trained software engineers have only ever had serious experience with so-called OO languages & with material that promotes OO to the exclusion of other paradigms, there is the (false, but widespread) belief that the problems introduced by trying to shoehorn the wrong problems into OO are problems of programming in general and are irreducible. The idea that there is such a thing as a general purpose programming paradigm that is ‘good enough’ for anything, while convenient for casual developers who would like to avoid learning more than the basics of a single language, supports this essentially artificial set of problems.
The current boom in interest in functional programming is hardly unexpected: you teach people that the one true paradigm is a bastard mix of OO and procedural programming for twenty years and that it’s the end of history, and as soon as they are exposed to an alternative they’ll jump all over it: most problems introduced by OO are artifacts of OO and dissolve as soon as you break out of it, but FP has similar artifacts.
Poor CS pedagogy is a bit of a hobby horse for me. OO is as it is, and I don’t have anything against it as such, but the idea that OO is generally applicable (or, more generally applicable than some other given paradigm) is getting in the way of professors and authors realizing that all serious developers need to have a full toolbox. The current miserable state of the development world is a result of every developer having a pair of vice grips & nothing else — and while you can solve pretty much any problem after a fashion with a pair of vice grips and a few hours of fiddling, you end up with a lot of mangled screws and bloodied fingernails.
By Rococo Modem Basilisk on July 25, 2016.
I always make this argument, and it usually falls on deaf ears.
I always make this argument, and it usually falls on deaf ears. But, this is a cultural and industry-norm-driven pattern — and Netflix is in prime position to upset industry norms (as it’s already done by releasing whole seasons at once).
In the anime industry, outside of a handful of long-running high-profit and usually every episodic shows aimed at younger audiences, there’s a very different attitude toward extending series. In the US, we tend to make new seasons of a show until it gets so bad that it gets cancelled; in Japan, because cancellation is such a big deal (it’s so rare that I’ve only ever seen one cancelled show), you have a very different pattern: a single season is planned out and made, and if it’s sufficiently popular and the source material allows for it, a decision is made about whether or not to create a sequel, usually several years later. The average show is between 12 and 26 episodes long; most shows have exactly one season, and when new seasons appear, whether or not they are a continuation of the same story depends heavily upon whether or not the show is an adaptation of a continuing story in another medium. As a result, a season is like a miniseries: good shows are finely crafted single-season stories that are never revisited, and a show with more than three seasons is almost always considered mediocre at best.
If Netflix adopted this style, they could push a change in the industry. After all, television has already changed: serious character-driven stories with complex ongoing narratives have become valued only in the past fifteen years or so, displacing the normal highly-episodic series well-adapted to syndication by channels that only license a small portion. If the expected form of television changed in this way, we’d see fewer shows that follow the pattern of Buffy — you know, an excellent first season followed by increasingly mediocre and convoluted follow-up seasons designed to pander to an obsessive core fanbase.
By Rococo Modem Basilisk on July 26, 2016.
I don’t know where you got any of the ideas in this article.
Copyright is not a protection against communism. The concept of copyright predates communism by a few centuries, and was initially formed as a means of mediating conflicts between publishing guilds and thus making censorship of printed materials easier. When the intellectual property framework of the united states was being designed, it became reframed in the same terms as patents: as a means of guaranteeing a temporary monopoly in order to encourage people to donate ideas to the public domain. This would be the 1790 formulation you were talking about — fifty years prior to the first ideas we could reasonably consider part of ‘communism’. In fact, we can consider copyright (along with other time limited forms of IP like patents, as opposed to trade secrets) to be a tool in support of ‘communist’ ideas in the sense that it is specifically intended to feed the pool of publicly owned ideas.
Copyright has definitely changed. For instance, in countries that are signatories to the Berne Convention (which the United States has been since 1957), all copyrightable materials are automatically implicitly under copyright — copyright registration is purely for the sake of expediting a suit by providing evidence from a trusted third party. During the middle of the twentieth century, implicit copyright was introduced to the United States in several different ways, and there was a short period wherein unregistered works were only implicitly copyrighted if they had a copyright statement attached (which is why Night of the Living Dead is in the public domain); this is no longer the case. In 1998, the Digital Millenium Copyright Act introduced the notion of safe harbour provisions in order to grant partial protection to websites hosting arbitrary third party content; this is a major break from all prior copyright legislation. When anti-circumvention clauses were added to the DMCA, that’s another major change. The idea of the FBI actively investigating breach of copyright is another recent change not in line with the historical spirit of IP law (which, since it is civil law, should theoretically only be investigated in the case of a suit).
I understand that you’re trying to push a product here. But, before trying to push a product relating to copyright law, try doing at least a modicum of research. I agree that copyright is currently benefitting mostly large corporations; however, this is not because of the cost of registration (because registration is not necessary) but because of a general lack of understanding on the part of regular people about the nature of IP law; this post perpetuates the worst of these misunderstandings.
By Rococo Modem Basilisk on August 1, 2016.
It’s important to note that Wikileaks itself doesn’t choose what information to grab & host; in terms of editorial control, it sits somewhere between pastebin & the New York Times: it mostly receives unsolicited leaks, and it will spend extra effort cleaning up & making accessible things that have wide appeal, but will more or less host anything. Looking at the raw list of leaks makes this clear: there are a lot of items along the lines of the membership list of Condoleeza Rice’s sorority (which is fairly uninteresting).
Does the wikileaks administration reject some submissions? Probably — after all, there are plenty of people who, given a place that will host anything, will use it to pirate hollywood movies & child porn. But, if somebody offers wikileaks an email dump, whether or not they will host it is not even a question: of course they will host it, because that’s what they see as their mission.
By Rococo Modem Basilisk on August 3, 2016.
tl;dr verison: original article is a dumb rehash of stuff that was proven wrong thirty years ago, and the author’s shallowness has nothing to do with the internet & everything to do with being lazy.
By Rococo Modem Basilisk on August 3, 2016.
Spoiler alert: I don’t hate you for being immune from spoilers; in fact, to the extent that this has actually been empirically tested, the general consensus appears to be that spoilers improve the enjoyment of media.
What is lost to spoilers? Only the shock value of sudden plot twists — in other words, the cheapest and most evanescent emotional effect of the laziest narrative constructions. Media that can be “spoiled” by spoilers is media that isn’t worth repeat viewing, continued thought, or discussion. For everything else, spoilers improve enjoyment.
By Rococo Modem Basilisk on August 17, 2016.
Caffeine takes about 20 minutes to metabolize to the point where it can suppress adenosine reuptake. This is how people do “coffee naps”.
Twenty minutes is plenty of time to write down 10 original ideas. Why not chug your coffee (or use caffeine pills), then spend the following 20 minutes writing your ideas? Alternately, sip your coffee while writing your ideas — and take advantage of the much slower ramp-up in the effects.
By Rococo Modem Basilisk on August 22, 2016.
Caffeine takes about 20 minutes to metabolize to the point where it can suppress adenosine reuptake. This is how people do “coffee naps”.
Twenty minutes is plenty of time to write down 10 original ideas. Why not chug your coffee (or use caffeine pills), then spend the following 20 minutes writing your ideas? Alternately, sip your coffee while writing your ideas — and take advantage of the much slower ramp-up in the effects.
By Rococo Modem Basilisk on August 22, 2016.
I don’t think Wikileaks is choosing their timing.
I don’t think Wikileaks is choosing their timing. They are not, and have never claimed to be, a journalistic outfit: if they were, they would assert control over their material and choose particular material to release (which they do not).
They are a repository for leaked material. The value of the leaked material is occasionally useful as a PR strategy to get more support, but they have no particular problem hosting uninteresting leaks — which is most of what they supply.
Don’t compare them to the New York Times; instead, compare them to MegaUpload.
By Rococo Modem Basilisk on August 22, 2016.
It’s a mistake to consider Wikileaks as a media organization, because they are generally anti-curation. There’s little rhyme or reason to the leaks they host, because they put up anything that’s submitted that they have reason to believe is legitimate. This makes them closer to a repository for leaked information — which is, of course, what they’ve been claiming to be the whole time. To the extent that they perform any kind of extra work — from redaction to supporting internal search (as they did with the cables) — it’s because they had something of such great importance that they felt the need to improve accessibility even if it damaged the correctness in minor ways. This should be seen as a divergence from their goals, rather than part of their core goal set.
By Rococo Modem Basilisk on August 22, 2016.
It’s a mistake to consider Wikileaks as a media organization, because they are generally anti-curation. There’s little rhyme or reason to the leaks they host, because they put up anything that’s submitted that they have reason to believe is legitimate. This makes them closer to a repository for leaked information — which is, of course, what they’ve been claiming to be the whole time. To the extent that they perform any kind of extra work — from redaction to supporting internal search (as they did with the cables) — it’s because they had something of such great importance that they felt the need to improve accessibility even if it damaged the correctness in minor ways. This should be seen as a divergence from their goals, rather than part of their core goal set.
By Rococo Modem Basilisk on August 22, 2016.
I’ve always been suspicious of how much companies that brand themselves as “tech companies” are drinking their own kool-aid, and how much is a calculated PR/branding move chosen specifically to provide an “in” for avoiding regulations. Uber can hardly be considered anything other than a taxi service — but by claiming to be a tech company they have an excuse to use this Randian mythic form to reframe any circumvention of labor regulations as a battle of smart renegade heroes of industry against an oppressive government, rather than the narrative of a large corporation systematically abusing its employees (which would be more natural if Uber identified as a tech company).
Given the trend toward creating service companies employing mostly contractors and claiming them as “tech companies” because of their use of employee management software, the suggestion that this is some kind of organic misunderstanding rather than a canny manipulation seems increasingly absurd — although it’s not unusual for intellectually incestuous communities to become prone to episodes of truly monumental mass delusion, so we shouldn’t expect the silicon valley VC industry to fare any better than the judicial system of eighteenth century Salem, nor for those forced by circumstance to implement those delusions to fare any better than the poor of any community in the middle of a witch- or werewolf-hunt.
By Rococo Modem Basilisk on August 22, 2016.
Saving the magic system of Familiar of Zero from itself
Saving the magic system of Familiar of Zero from itself
Familiar of Zero isn’t just a 2006 anime — it is the most 2006 anime I’ve ever seen. It’s the kind of uninspired extruded-fantasy-harem-crap we always complain about, in its purest and most unadulterated form. Its protagonist is without any particular traits, other than an over-the-top lecherousness that while it sticks out now was more or less par for the course a decade ago for harem shows. The romantic lead is the kind of poor-little-rich-girl tsundere we wouldn’t see properly fleshed out and given characterization until Toradora. There’s a large supporting cast of generic archetypes. The most interesting and original character may be Colbert, a plot device of a balding, kindly but boring professor who is also prone to chasing after myths — kind of like if your high school social studies teacher spent his free time trying to find Atlantis — but he is mostly notable for how little he resembles Stephen Colbert. It’s a spectacularly forgettable show, notable mostly for how stupid every single character appears to be. But what it does have is an interesting and well fleshed-out magic system.
That is, until the plot starts.
You see, the way magic works in this world is that each spell works with one of the four classical greek elements. Even simple spells that are theoretically identical are completely different between elements: the spell to illuminate the tip of a wand would have nothing in common in the mechanics of casting between a method using air and a method using earth, even though the result would be identical. A ward or seal created by fire magic can only be undone by fire magic. This creates a pretty stable set of parameters. Imagine this from the perspective of game design for some MMORPG: even if every player character is a mage, you would expect a party to by necessity contain a member specializing in each type of magic. To the extent that attack spells are used, they are undone by attack spells from an opposing element, usually — fire can be defeated by water or redirected by wind, etc.
And then, we’re introduced to the fifth element, void, which is incredibly powerful and can beat anything. Because that’s the only way our protagonists can be put into interesting situations: by deus ex machina.
Ignoring void magic, we have the possibility of multi-classing. The idea is that extremely talented people can reach the limit of advancement in one form of magic and then start from the bottom in another form. If you can master three of the four forms of magic, you become a triangle mage, and can cast spells that weave together the three elements in such a way that it would be very difficult for three high level mages of the individual elements to undo; only a triangle mage specializing in the same three elements, or a square mage who has mastered all four, can undo it.
Void magic is in-born, can’t be properly studied, operates mostly by intuition, and is incompatible with all the other forms. So it breaks everything.
The world in which this show takes place is a kind of extension of feudal europe, in which all those capable of magic are considered a part of the aristocracy, and where rival kingdoms are in politically precarious situations, with webs of secret alliances, spies infiltrating each other’s kingdoms, and aristocrats trying to foment populist uprisings among the peasantry. In other words, a fairly realistic depiction of a society where a whole class of people have magic powers passed on along the bloodline. We have plenty of call-outs to actual historical events, including a revolution in the britain-expy called Albion wherein a guy named Cromwell kills a prince named Wales and turns the country into a dictatorship. Adding intrigue are various magical devices that can temporarily animate corpses or cause people to act against their will while in line of sight.
And then, the void magic is powerful enough to kill all the conspirators. Accidentally. With the help of a world war two era fighter plane. This show keeps generating promise and then screwing it up.
Without the main characters, we have the setup for a really interesting game or story here. We have a complex but internally consistent set of rules for magical warfare — one that rewards teamwork at the lower levels and rewards achievement at the endgame, encouraging people to max out their level up to four times. We have interesting sets of alliances and conflicts: nations at war with each other and themselves, spy networks, the double-cross system. We have devices that have interesting abilities and limitations: the ability to temporarily control a corpse, the ability to control a living being at short range and under line of sight. We have an interesting combat system, with familiars tanking for mages and an interaction between elements. I could imagine something similar to a cross between WoW and Planetside — an ongoing war of small-scale conquest along the edges of domains, powered by a steady supply of fresh blood from strongholds where mages in training follow quest lines to develop their skills.
All we need is somebody to take advantage of this, instead of breaking it in the name of making a generic hero’s journey story.
By Rococo Modem Basilisk on August 22, 2016.
The trademark issue is pretty cut and dried: a trademark holder must show due diligence in protecting its trademarks or else it loses control of the trademark permanently; as a result, a trademark holder has every incentive to send nasty messages to anybody they might have reason to believe might be seen by future courts as infringing upon their trademark, lest they be suspected of not protecting it well enough. (Basically, the law incentivizes trademark holders to be assholes & go overboard, which is why Disney sues day care centers for having Disney-owned characters on murals.)
As for linking: you may not remember, but the court case regarding the legal status of links was a *big* deal, and old media companies are likely to still be pretty sore about it. It was only twenty years ago that the question of whether or not deep linking was a form of infringement; similar mechanisms (like the use of frames to wrap ads around other people’s sites) were ruled against. It doesn’t really matter if the NYT is planning to make a bot: they didn’t want to give up on suing Google & anybody else who does deep links, and now that they have to, they have every intention to prevent that verdict from being generalized to slightly different circumstances.
Furthermore, you can make the argument that use of the API might, in certain cases, lower the number of ad views they get or screw up their progressive paywall system. While I’m not convinced that either system is making them bank in the way that they might hope (and both are easily & regularly bypassed), they have a financial incentive to label and shun anything that might conceivably be used to circumvent these mechanisms — particularly if the product is not commercial.
By Rococo Modem Basilisk on August 22, 2016.
Zizek (along with a handful of other figures in media criticism that are full of ideas but short on consistency, like McLuhan) are, I find, best viewed from the same lens as divinatory bibliomancy or generative or surrealist writing-games. In other words, the juxtapositions they create can help externalize and elucidate insights that come within us, but we should not attribute those insights to the ostensible authors, because for every ‘true’ insight we get from those texts, the direct opposite can be supported equally well by the same material. Rather than a string of words containing clear meaning, I see these authors as creating a string of vague associations with strong resonances — and we as readers can cherry-pick particular ideas and complete them ourselves, just as Burroughs did with his cut-ups, or a tarot card reader does with his spread.
By Rococo Modem Basilisk on August 22, 2016.
OK. There’s a pretty big gap between scheduling the announcement of a leak to maximize press (and thus maximize donations to your non-profit organization) and trying to manipulate the election of a foreign country in favor of your ideological enemy, though.
Why would a collection of mostly european anarchists want a Trump presidency? I don’t see any traces of accelerationist rhetoric going on; if anything, Wikileaks appears to take the attitude that transparency in general will improve governance, rather than the idea that discrediting the idea of government in general is desirable. Julian Assange is not Nick Land; his outlook is a lot more utopian.
By Rococo Modem Basilisk on August 24, 2016.
An alternate wizard/cleric distinction
I’ve been reading Playing the World, Jon Peterson’s scholarly history of the origins of Dungeons & Dragons. Peterson elucidates the circumstances around the wizard/cleric distinction well, explaining that such a distinction had very little basis in fantasy literature (and none in Chainmail, Gygax’s predecessor to the D&D ruleset) but instead appears to come from Gygax’s religious background & various complaints about the absence of religion in medieval & fantasy wargaming. As a result, while magic users are fully irreligious, we have a separate class that is essentially a specialist magic user whose domain of abilities is based on a credulous reading of christian miracle-work — along with a dynamic involving power level being limited by a fall from grace. This is wholly at odds with the essentially pagan attitude elsewhere in the extruded fantasy product tradition that D&D in many ways codified: a universe where a thief can be lawful good is not in any way a christian universe, even in terms of the secularized pseudo-christian morality that binds clerics. Instead, since we’re talking magic systems, we might look at historical occult traditions and classifications: specifically, the thelemic conception of left-hand versus right-hand path.
Talking to magick practitioners about the left hand and right hand paths is like talking politics at a family gathering: it’s a recipe for broken hearts. For the sake of this essay, I’m going to use a strict and simplified division: left-hand magic is performed using the abilities of the magic practitioner directly, and comes down to a set of rules and mechanics; right-hand magic, on the other hand, relies upon no special abilities in the practitioner, but instead a knowledge of means by which the practitioner can convince supernatural entities to perform tasks. This is certainly not the only way to interpret left-hand vs right-hand traditions, but the major alternative (which applies a moral gloss, considering left-hand to be synonymous with black magic) is less useful for the purpose of role playing systems & ultimately redundant; I would furthermore claim that conflating the two (as many traditions do) is at best an indication of anti-secular bias.
Untangling left-hand and right-hand mechanisms in historical occult practices is difficult, in part because non-secular traditions perform theological gymnastics to reclassify seemingly left-hand practices as right-hand practices, thereby avoiding anti-left-hand bias. This is particularly common in monothesistic contexts: the use of kaballistic formulae whereby the magician manipulates letters or numbers representing the state of the world in order to manipulate the world appears fairly left-hand, but by identifying the material world with an aspect of the divine and invoking predestination, we can treat it as a form of prayer — that most representative form of right-hand magic. For the sake of this essay I would like to avoid these kinds of tricks entirely, and consider only those magical techniques that directly interact with a supernatural entity as right-hand.
And so, what does this give us? It gives us a richer domain for the cleric, reallocating some of what would otherwise be the domain of wizards. In a clearly polytheistic context, the low-level cleric also takes on some aspects of bards, being capable of granting boons to particular attributes by prayer or sacrifice to gods associated with those attributes. Contracts with gods, as in voodoo, are the domain of clerics; control of elementals, summoning of & contract with demons, and invocation and evocation of any non-humanoid ‘monster’ likewise. Rather than needing to keep up a kind of pseudo-christian morality, a cleric would need to merely act in a way in line with the gods he or she actually interacts with. (Many polytheistic pantheons contain a god of communication who acts as a gatekeeper: Legba in voodoo, Ganesha in hinduism; any cleric would need to act in a way that pleases the gatekeeper deity, or risk losing access to the whole pantheon, but what this means is very different between pantheons: what is fine with Mercury is unlikely to be acceptable to Ganesha.)
This would require moving many of the healer & support roles to another class; after all, the cleric redefined in this way would be significantly more powerful as an offensive class at higher levels and would not have special healing abilities at lower levels.
By Rococo Modem Basilisk on August 26, 2016.
I have to agree with all of your points here.
I have to agree with all of your points here. But, at the same time, on the scale set by the other TOS reboot films, this one is the closest to Star Trek — in the sense that it has an interesting and complex plot, ensemble cast drama, & elements of the utopian subtext that characterized TOS. The other two have leaned much closer to generic-sci-fi-action — though Star Trek movies have always been a lot more action-oriented and a lot less talky than the shows.
Was the jokeyness a bit out of character for a Trek film? Sure, particularly for the reboot franchise; on the other hand, the other films in this group have been particularly over-serious grimdark fare (even as they used absurd plotlines that made “Spock’s Brain” and “Move Along Home” look normal), and so expanding upon the only redeeming quality of previous scripts — one-off jokes — is justified. (This is hardly out of line for Trek films; after all, the best — or at least most interesting — Trek film is The Journey Home, which is also the silliest.)
I more or less agree with MovieBob’s review here: Star Trek Beyond is, as a sci-fi action film, passable; as a Trek film it’s mediocre; as a Trek film in the reboot franchise, it’s surprisingly good, and would have been indicative of a worthwhile series had it been the first film. Because it’s the third, and because we’ve already lost some of the cast, the reboots as a whole probably won’t be able to recover, because even if the general improvement in quality continues, it doesn’t have enough remaining installments to become worthwhile as a whole.
By Rococo Modem Basilisk on August 29, 2016.
“ASAP as possible” is redundant.
“ASAP as possible” is redundant. (I’m surprised you didn’t notice this typo when you went to stick it in a pull quote.)
By Rococo Modem Basilisk on August 30, 2016.
ASAP stands for As Soon As Possible.
By Rococo Modem Basilisk on August 30, 2016.
I’m unfamiliar with Schiffer.
I’m unfamiliar with Schiffer. Her post seemed typical of tech-related posts on medium with the exception of a fairly common typo being emphasized in a pull quote, so I figured it was an honest mistake — many authors on medium are not native english speakers.
By Rococo Modem Basilisk on August 30, 2016.
Nope! Changing ‘responsive’ to ‘reactive’ doesn’t explain why we’d specify a server-side Microsoft stack in the context of a client-side problem on a blog about CSS, nor does it explain why we would drop words & punctuation in this context. It still seems more likely to me that the author meant “ASAP” in its conventional sense. Since she appears to be active in this thread, perhaps she can clarify.
By Rococo Modem Basilisk on August 30, 2016.
On second thought, the article is slightly above the average stupidity of a tech article on medium.
On second thought, the article is slightly above the average stupidity of a tech article on medium. The epoch should have tipped me off :)
By Rococo Modem Basilisk on August 30, 2016.
So, you think the tail end of that pull quote was intended to be “we need to stop building responsive web pages active server active pages as possible”? If that was the intent, then there’s still a typo here — we’re missing a conjoiner & some punctuation. In other words, “we need to stop building responsive web pages using ASAP, as possible” would make sense (even if it’s very awkward phrasing), but it would imply that only one set of technologies (the Microsoft ASP stack) is at fault. This is a very strange interpretation, considering the rest of the article is complaining about reactive web design in general.
By Rococo Modem Basilisk on August 30, 2016.
The gatekeeper idea you mention is one of these elements that I feel is self-reinforcing.
The gatekeeper idea you mention is one of these elements that I feel is self-reinforcing. Despite filmmaking equipment getting progressively cheaper, even indie budgets are growing, & the whole industry is getting more and more insular, with low acceptance for spec scripts & a lot of big-budget low-risk franchise flicks. From the 70s straight through the 90s, even though film cameras were an expensive specialty item & editing equipment was difficult to use & to afford, we had people dropping lots of money trying to make (mostly awful) films. Today, when it’s possible for someone in the lower middle class in the west to make a feature length film with only the phone & the computer they already have (paying nothing for software), we still have big-budget theoretically-indie youtube channels, rather than an explosion in amateur narrative films.
I used to run something called the “No Budget Film Contest”. We’d take submissions of films, rank them, & give small (less than $10) cash prizes to the top 3. For a film to be eligible, the creator of the film could not have paid for anything specifically for the film, with the exception of camera rental — no paid actors, no set pieces or props, no expensive editing software. The goal was to demonstrate that one could make reasonably interesting movies using no resource other than time: in other words, that small scale productions can be valuable, and that you don’t need to get studio involvement or a successful kickstarter to make a film. It’s been a while since I hosted one of those, but it might be time again.
By Rococo Modem Basilisk on August 30, 2016.
What?
It’s difficult to tell if you’re serious. Are you honestly unfamiliar with the colloquial use of “ASAP”, or are you just failing to make a joke?
By Rococo Modem Basilisk on August 30, 2016.
That’s sort of my point: there’s no technical reason why feature length film budgets should be increasing rather than decreasing; instead, it’s a purely social reason: specifically, people (falsely) believe that a big budget is necessary to make something with any kind of audience. Demonstrating that a worthwhile feature-length film can be made with a budget in the tens of dollars instead of in the millions is probably the best way to counter this.
In the 90s, plenty of pictures were made for ten thousand dollars or so. That number has stuck in people’s minds as the low-end estimate, but in reality, that refers to the cost of a low-end film camera, plenty of film, and film editing equipment — or a low-end camcorder, a bunch of tapes, and a video toaster. But, today, most americans have video cameras in their pockets that knock the mid-range 90s camcorders out of the water. We need to redefine the idea of the “shoestring budget” for film from $10k to twenty bucks.
By Rococo Modem Basilisk on August 31, 2016.
A Qualified Defense of Jargon & Other In-Group Signifiers
This essay is in part a response to an article I read this morning. I won’t link to the article, in part because I don’t want to diminish it by talking over it, and in part because the story that this article tells is pretty common — plenty of people have complained about exactly the same thing, myself included. The article in question told the story of a self-taught programmer whose unfamiliarity with certain terms and historical figures theoretically irrelevant to her work marked her as an outsider & made her uncomfortable in a professional environment. I sympathize — after all, I too was once a self-taught developer in over my head, and I’ve gone through the process of trying to work in an environment where my unfamiliarity with the shibboleths marked me as an outsider. I also don’t want to imply that her situation is justified — accidents of biology & history like gender, ethnicity, and childhood cultural background can make the bar for fitting into particular groups much higher than it should be. What I’d like to avoid is the shallow idea that we can just reject shibboleths altogether and solve all problems.
Jargon is a clear cut example of when a tool that’s instrumental for competency also acts as a shibboleth. Most fields with complicated forms of technical knowledge also have specialized vocabulary or specialized meanings for particular words; the general consensus surrounding the meaning of a particular word stands in for a vast chunk of shared knowledge that would be complicated to explain in non-specialized language. When someone says “O(n²)”, they are making a claim that, in order to be understood, implies a familiarity with the calculus of derivatives, instruction counting, how loops work, and the idea of worst case versus expected case outcomes. Working with someone whose familiarity with foundational concepts in your field is at the level of an outsider is nearly impossible: you need to act the part of professor more often than you do real work; if someone lacks familiarity with the terminology, your handle for figuring out their knowledge level is no longer usable. Autodidacts are already at a disadvantage here: the set of things they know is not the same as that taught by a degree program, and they can have large conspicuous gaps in their knowledge even when it comes to relevant material. When an autodidact is unfamiliar with a term, it’s hard to tell whether or not that indicates a gap in their competence that needs to be filled, taking up time and effort that could otherwise be applied to the ostensible end goal.
Even when a piece of jargon isn’t of direct relevance, familiarity with jargon is indicative of a shared cultural experience that reflects shared values and habits — some pieces of which can be highly relevant. Using a term like “dot file” when referencing a hidden configuration file indicates familiarity and comfort with the unix environment, which might extend to a familiarity with pipes, standard unix tools, shell scripting, basic unix system administration, the history of the free software and open source movements, unix-style development toolchains, a preference for classic text editors like vi & emacs over graphical IDEs, an understanding of the unix philosophy of small modular programs working together on text streams, a familiarity with and preference for irc over other similar communication systems, a fondness for beards, and the ownership of toys shaped like penguins or tennis shoe wearing demons. Someone who just says “config file” will not open the floodgates of spurious cultural associations. (Likewise with “home directory” versus “user directory” or “my documents”.) All of these associations are fuzzy: not everyone has equal immersion in the cultural ephemera related to some technology. However, there’s a set of expectations associated with use of terminology: kinship with a set of tribes with clear centers and fuzzy boundaries.
Similarly, familiarity with certain historical figures has cultural relevance that extends to philosophy with meaningful applications in work environments. Somebody familiar with Djikstra who only learned about him in school probably only knows about him in the context of graph traversal, but someone familiar with Djikstra’s mythology is likely to be familiar with a couple quotes: “asking if a computer can think is like asking if a submarine can swim” and “premature optimization is the root of all evil”. Even if misattributed & misunderstood, both those quotes have impacts on the way someone develops software that go far beyond familiarity with graph traversal algorithms. Someone who quotes Postel’s Law, similarly, has a particular philosophy that’s easy to identify. All of these philosophies have impact, though the value of this impact depends on the application: Postel’s Law was instrumental in both the widespread adoption of the world wide web and the horrible security shitshow that the world wide web can’t extract itself from.
This is all to say that culture matters and signifiers matter. If you’re working alone, it’s fine to focus only on the precise technical ideas that stand between you and your immediate goal; as soon as you start working with a group, an inability to code switch means your coworkers can’t predict your behavior, communicate with you efficiently about important things, or figure out what you need to know in order to do your job effectively. Hiring an autodidact is a big risk, which is why lots of places don’t bother — it’s not that a degree implies a high level of competence, but instead that a degree is a hedge against a low level of competence that would cost the company a lot of money; hiring an autodidact with big gaps in their understanding of jargon and culture is not merely a risk but an almost-guaranteed cost, even if the person in question has significantly above-average technical skills and work ethic.
All of this is a huge problem, because the tech industry has a number of problems related to a lack of diversity — particularly cultural diversity. We need to shake this industry up with outside ideas, and part of that is bringing in autodidacts who have experiences unlike those systematically manufactured by degree programs or accelerators. We need people who can point at the inconsistencies and stupidities that are passively accepted as normal by the monoculture. But, we need those people to speak the language.
If we don’t throw away shibboleths entirely, what are we to do?
My recommendation is: outsiders, know your enemy. If you’re learning to code on your own, pay attention to the mythology. Read the Jargon File. Read The Devouring Fungus. Read Hackers & Where Wizards Stay Up Late. Make sure that you have enough understanding of the history and culture to pass as a precocious newbie.
As for insiders: be conscious of the way shibboleths are being used. Just because someone doesn’t know the term you used doesn’t mean they are totally unfamiliar with the concept. Make sure you aren’t leaning too heavily on spurious associations: playing Tempest doesn’t really have anything to do with understanding depth-first searches, Stallman’s association with Linux isn’t much more than a series of accidents that don’t meaningfully impact the syntax of awk, and LISP’s vast mythology is mostly the product of it being really popular at MIT fifty years ago rather than some deep truth about lambda calculus. Make sure that, to the extent that you are using shibboleths as a proxy for competence, you are not doing so in a way that unfairly prevents competent outsiders from contributing meaningful work. And, especially, give the benefit of the doubt to anyone who isn’t a cultural fit for reasons beyond their control: assume that women & people who are neither white nor asian are competent, because everyone else in their lives will assume they are incompetent.
By Rococo Modem Basilisk on September 1, 2016.
A device and method for escaping the universal truth
ABSTRACT
A device and method for escaping the universal truth. The devices comprises a panoptic system, an inverse mirror, a universal system, a whole edifice, a whole system, an original book, an uninterrupted circuit, an artificial mosaic, a nervous system, a single gadget
BRIEF DESCRIPTION OF THE DRAWINGS
Figure 1 schematically illustrates the critical obsession with its aura.
Figure 2 is an isometric view of The other aspect of this process.
Figure 3 schematically illustrates no strategic stakes at this conjuncture.
Figure 4 is a block diagram of an unforeseen twist of events and an irony.
Figure 5 is a perspective view of The machine-readable form of the input.
Figure 6 is a block diagram of any other symptom in classical medicine.
Figure 7 is a block diagram of the discrete charm of the gravity.
Figure 8 is a schematic drawing of the logical evolution of a universal system.
Figure 9 is a schematic drawing of the second sentence of the epigraph.
Figure 10 illustrates the same time by the cartographer.
DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS
The present invention witnesses the end of the negative form. The present invention separates one pole from the very swing. The invention seals the end of the abolition. The invention is the slope of a false problem. The invention is the truth of this new age. The device signifies a setback in the sense. The invention makes any sense of a conventional , restricted perspective.
The present invention is an escalation of the whole edifice. The present invention produces nothing but the discrete charm of the gravity.
What is claimed is:
1. A device for escaping the universal truth, comprising:  an original book; and   a universal system.
2. The device of claim 1, wherein said original book comprises the discrete charm of the gravity.
3. The device of claim 1, wherein said universal system comprises an unforeseen twist of events and an irony.
4. A method for escaping the universal truth, comprising:  a single gadget;   a universal system;   an inverse mirror; and   an uninterrupted circuit.
5. The method of claim 4, wherein said single gadget comprises the discrete charm of the gravity.
6. The method of claim 4, wherein said universal system comprises the logical evolution of a universal system.
7. The method of claim 4, wherein said inverse mirror comprises The other aspect of this process.
8. The method of claim 4, wherein said uninterrupted circuit comprises The machine-readable form of the input.
By Rococo Modem Basilisk on September 20, 2016.
A method and device for becoming a property owner himself
ABSTRACT
A method and device for becoming a property owner himself. The devices comprises an old system, a mere instrument, a militant working, a Polish edition, an international product, a present system, a big factory, an other working, an eight-hour working, a Danish edition, an indispensable cloak, a first edition, a democratic newspaper, a English edition, a German edition, a whole system, an entire surface, a European working, a feudal system, a social stronghold, a revised edition, a chief load, a German working, a whole article, a bribed tool, a last resort, an own working, a transcendental robe, a modern working, an essential product, a 23-page pamphlet, a Russian edition, a mighty weapon, a second edition, a collective product, a present edition, a respective working, a subsequent edition, a great factory, an other article, a mechanical loom, a poor stock-in-trade, an own house, a sentimental veil, a parliamentary stronghold, a new machine, a representative system, a heavy artillery, a previous edition, a gigantic broom, a common platform, a European system, a capitalist system, a rough sketch, a whole working
BRIEF DESCRIPTION OF THE DRAWINGS
Figure 1 is a diagrammatical view of the progressive historical development of the proletariat.
Figure 2 is a schematic drawing of the first revolution in which the working class.
Figure 3 is a schematic drawing of a certain stage in the development.
Figure 4 is a schematic drawing of a corresponding political advance of that class.
Figure 5 illustrates the first conditions for the emancipation.
Figure 6 is a diagrammatical view of the influential bourgeoisie during the French revolution.
Figure 7 is a cross section of an independent section of modern society.
Figure 8 is a perspective view of all political action on the part.
Figure 9 is an isometric view of an exaggerated form of the ancient struggle.
Figure 10 is a diagrammatical view of the main consequences of the abolition.
Figure 11 is a block diagram of the same pace at which the progress.
Figure 12 is a perspective view of the working-class parties of every country.
Figure 13 is a cross section of the rural producers under the intellectual lead.
Figure 14 is a cross section of the vanished status of the workman.
Figure 15 is a schematic drawing of the great factory of the industrial capitalist.
Figure 16 is a diagrammatical view of the inevitable impending dissolution of modern bourgeois property.
Figure 17 is a cross section of the necessary offspring of their own form.
Figure 18 is a schematic drawing of the miraculous effects of their social science.
Figure 19 is a cross section of the historical movement as a whole.
Figure 20 is a diagrammatical view of the old system of manufacture or industry.
Figure 21 is a block diagram of the miserable character of this appropriation.
Figure 22 schematically illustrates the whole range of old society.
Figure 23 is a schematic drawing of the political form at last discovered.
Figure 24 schematically illustrates the icy water of egotistical calculation.
Figure 25 is an isometric view of a national bank with state capital.
Figure 26 is a diagrammatical view of the holy water with which the priest.
Figure 27 schematically illustrates the positive form of that republic.
Figure 28 is an isometric view of all other branches of the administration.
Figure 29 is a diagrammatical view of a rough sketch of national organisation.
Figure 30 is a perspective view of The essential conditions for the existence.
Figure 31 is a perspective view of a new proletarian line during the discussion.
Figure 32 is a diagrammatical view of The first direct attempts of the proletariat.
Figure 33 illustrates The proper form of their joint-stock government.
Figure 34 is a perspective view of the proletarian differ from the serf.
Figure 35 is a cross section of a fantastic conception of its own position.
Figure 36 is an isometric view of a powerful coefficient of social production.
Figure 37 is a cross section of a clear understanding of the character.
Figure 38 illustrates the actual position of first class.
Figure 39 is a cross section of a comprehensive formulation of the proletarian movement.
Figure 40 is a cross section of the rapid development of Polish industry.
Figure 41 is a block diagram of the momentary interests of the working class.
Figure 42 is an isometric view of the physical force elements of the old government.
Figure 43 is a perspective view of the senile mountebank at its head.
Figure 44 is a cross section of the healthy elements of French society.
Figure 45 is a diagrammatical view of the ultimate general results of the proletarian movement.
Figure 46 is a schematic drawing of the classical works of ancient heathendom.
Figure 47 is a perspective view of a mighty weapon in its struggle.
Figure 48 is an isometric view of The first , fundamental condition for the introduction.
Figure 49 is a perspective view of these philosophical phrases at the back.
Figure 50 is a diagrammatical view of an agrarian revolution as the prime condition.
Figure 51 illustrates no other nexus between man and man.
Figure 52 is a block diagram of the whole superincumbent strata of official society.
Figure 53 is a perspective view of the economic and political sway of the bourgeois class.
Figure 54 schematically illustrates the same course as its predecessor.
Figure 55 is a schematic drawing of every other employer in the search.
Figure 56 is a block diagram of the disastrous effects of machinery and division.
Figure 57 is a diagrammatical view of all numerous vanguard of scientific socialism.
Figure 58 is a perspective view of the social power of the nobility.
Figure 59 is a cross section of the first time in the introduction.
Figure 60 illustrates the individual workers so that the worker.
Figure 61 is an isometric view of the social character of the property.
Figure 62 is an isometric view of the economic progress of the country.
Figure 63 is an isometric view of the last great reserve of all European reaction.
Figure 64 is a schematic drawing of a new proof of the inexhaustible vitality.
Figure 65 illustrates the oracular tone of scientific infallibility.
Figure 66 is a diagrammatical view of the necessary condition for whose existence.
Figure 67 illustrates the civilized countries of the world.
Figure 68 illustrates the French criticism of the bourgeois state.
Figure 69 schematically illustrates the immediate demands of the movement.
Figure 70 illustrates the first class of the country.
Figure 71 is a diagrammatical view of the scattered state of the population.
Figure 72 is a diagrammatical view of a considerable part of the population.
Figure 73 is a diagrammatical view of the entire bourgeois society on its trial.
Figure 74 is an isometric view of the intellectual development of the working class.
Figure 75 is a schematic drawing of the first radical attack on private property.
Figure 76 illustrates the free movement of , society.
Figure 77 is a diagrammatical view of the latter stands at a higher stage.
Figure 78 is a schematic drawing of a new collection of the work.
Figure 79 is an isometric view of the Communistic abolition of buying and selling.
Figure 80 is a block diagram of a supplementary part of bourgeois society.
Figure 81 is a diagrammatical view of the economic conditions for its emancipation.
Figure 82 illustrates The political rule of the producer.
Figure 83 is a block diagram of the special privileges of the nobility.
Figure 84 is a schematic drawing of a revised edition of this earlier draft.
Figure 85 is a cross section of the full development of every previous revolution.
Figure 86 is a diagrammatical view of the true originators of the war.
Figure 87 is a diagrammatical view of the necessary consequence of the creation.
Figure 88 illustrates a 23-page pamphlet in a dark green.
Figure 89 is a block diagram of the revolutionary element in the tottering feudal society.
Figure 90 schematically illustrates the bombastic representative of the petty-bourgeois.
Figure 91 schematically illustrates an undeveloped state of both agriculture.
Figure 92 is a diagrammatical view of no other conclusion that the lot.
Figure 93 is a block diagram of a new guarantee of its impending national restoration.
Figure 94 is a schematic drawing of the socialist movement spreads among them and the demand.
Figure 95 is a perspective view of the entire surface of the globe.
Figure 96 is a schematic drawing of a decided progress of Polish industry.
Figure 97 is a perspective view of the true conditions for working-class emancipation.
Figure 98 is a block diagram of the other countries of the world.
Figure 99 is a perspective view of the same pace as the growth.
Figure 100 is a block diagram of the bourgeois sense of the word.
Figure 101 is an isometric view of the gradual , spontaneous class organisation of the proletariat.
Figure 102 is an isometric view of the programme document in the course.
Figure 103 is a block diagram of the political and intellectual history of that epoch.
Figure 104 is a schematic drawing of a full and substantial exposition of the new revolutionary.
Figure 105 is a schematic drawing of The undeveloped state of the class.
Figure 106 illustrates the first instinctive yearnings of that class.
Figure 107 schematically illustrates a progressive phase in the class.
Figure 108 illustrates the bold champion of the emancipation.
Figure 109 is an isometric view of the right man in the right place.
Figure 110 is a perspective view of the intellectual development of the mass.
Figure 111 is a perspective view of the same time markets for the sale.
Figure 112 illustrates the absolute monarchy as a counterpoise.
Figure 113 schematically illustrates these first movements of the proletariat.
Figure 114 is a diagrammatical view of the Italian proletariat as the publication.
Figure 115 is an isometric view of the little workshop of the patriarchal master.
Figure 116 is a cross section of the working class of the 19th century.[vi.
Figure 117 is a perspective view of all coercive measures against the working class.
Figure 118 is a diagrammatical view of the continued existence of bourgeois society.
Figure 119 schematically illustrates the agricultural population on the land.
Figure 120 is an isometric view of the real action of the working class.
Figure 121 illustrates a necessary condition of communist association.
Figure 122 is a perspective view of the practical absence of the family.
Figure 123 is a cross section of The rural communities of every district.
Figure 124 is a schematic drawing of the leading body of the Paris circle.
Figure 125 is a schematic drawing of The productive forces at the disposal.
Figure 126 is a perspective view of a final stage in the reorganisation.
Figure 127 is a schematic drawing of the misty realm of philosophical fantasy.
Figure 128 is a cross section of the ultimate disappearance of private property.
Figure 129 is an isometric view of the exact contrary of its real character.
Figure 130 schematically illustrates the big industry of our own day.
Figure 131 is a diagrammatical view of the self-conscious , independent movement of the immense.
Figure 132 illustrates the savage warfare of Versailles outside.
Figure 133 schematically illustrates the further consequences of the industrial revolution.
Figure 134 schematically illustrates a national bank with State capital.
Figure 135 is a diagrammatical view of the sleeping partner of the capitalist.
Figure 136 is a cross section of the final approval of the programme.
Figure 137 is a cross section of the ultimate form of the state.
Figure 138 is a cross section of the great struggle of the day.
Figure 139 is a cross section of all local markets into one world.
Figure 140 is a schematic drawing of the first step in the revolution.
Figure 141 illustrates the upper stratum of the working class.
Figure 142 is a schematic drawing of the international union of the proletariat.
Figure 143 is a diagrammatical view of every villainous meanness of this model.
Figure 144 illustrates the various wards of the town.
Figure 145 schematically illustrates the last remnants of their independence.
Figure 146 is a perspective view of the hallowed co-relation of parents and child.
Figure 147 is an isometric view of a full member of a guild.
Figure 148 is an isometric view of a vast association of the whole nation.
Figure 149 is a perspective view of the political liberation of the proletariat.
Figure 150 schematically illustrates the political conditions of the Continent.
Figure 151 illustrates The new draft for the programme.
Figure 152 is a block diagram of the peaceful abolition of private property.
Figure 153 is a cross section of the front the common interests of the entire proletariat.
Figure 154 is a perspective view of the peculiar mysteries of the Picpus nunnery[xxii.
Figure 155 is a schematic drawing of the immediate consequences of the industrial revolution.
Figure 156 is a schematic drawing of the same way in which a foreign language.
Figure 157 is a schematic drawing of the social and political aspirations of the European working.
Figure 158 is a block diagram of the first Paris Revolution in which the proletariat.
Figure 159 is a block diagram of the second paragraph of point 9 and the last sentence.
Figure 160 is a cross section of the secret societies of the time.
Figure 161 illustrates a vague aspiration after a republic.
Figure 162 is a perspective view of the direct or indirect dominance of the proletariat.
Figure 163 is a diagrammatical view of The individual members of this class.
Figure 164 schematically illustrates the proletarian differ from the handicraftsman.
Figure 165 is a diagrammatical view of the full consciousness of their historic mission.
Figure 166 is a cross section of a bribed tool of reactionary intrigue.
Figure 167 is a block diagram of the monarchical form of class rule.
Figure 168 is a diagrammatical view of the common cause with the party.
Figure 169 schematically illustrates An oppressed class under the sway.
Figure 170 is a cross section of the motley feudal ties that bound man.
Figure 171 schematically illustrates The inner organisation of this primitive communistic society.
Figure 172 is a cross section of the historical conditions for the time.
Figure 173 is a cross section of the feudal organisation of agriculture and manufacturing.
Figure 174 is a perspective view of the great mass of the proletariat.
Figure 175 schematically illustrates the hostile antagonism between bourgeoisie and proletariat.
Figure 176 is a diagrammatical view of The bourgeois clap-trap about the family.
Figure 177 is a block diagram of the commercial crises that by their periodical.
Figure 178 schematically illustrates the other political parties of our time.
Figure 179 schematically illustrates the basic conditions of this society.
Figure 180 is a schematic drawing of the old cries of the restoration.
Figure 181 illustrates the reactionary character of their criticism.
Figure 182 schematically illustrates a new onslaught upon the power.
Figure 183 is a block diagram of any distinctive social validity for the working class.
Figure 184 is a perspective view of the first part of the book.
Figure 185 is an isometric view of the central seat of the old governmental power.
Figure 186 is a schematic drawing of a shameless display of gorgeous , meretricious and debased luxury.
Figure 187 is a diagrammatical view of the proletarian differ from the slave.
DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS
The present invention is a 23-page pamphlet in a dark green. The present invention grows emancipation struggle of the proletariat. The present invention examines the movement of the working class. The invention constitutes the last great reserve of all European reaction.
In accordance with an alternative specific embodiment, the present invention found their only salvation from the proletariat. The device becomes the signal for a proletarian revolution. The device constitutes the foundation for the political and intellectual history. The present invention prepares the way for a more complete insight. The present invention brings out a book with the name. The present invention reflects the history of the modern working-class movement. The device is the act of the working class. The present invention attains its emancipation from the sway. The invention has again gathered sufficient strength for a new onslaught.
According to another embodiment, the device reflects the history of the modern working-class movement. The device is the task of the working class. The device is no better witness than this day. The invention regards their position as the working class. The invention indicates a decided progress of Polish industry. The invention is a necessity for the harmonious collaboration. The invention is the work of the working class. The device achieves the international union of the proletariat. The invention is itself the product of a long course. The invention remains no other nexus between man and man. The present invention finds its fitting complement in the most slothful indolence. The invention creates a world after its own image. The invention puts the existence of the entire bourgeois society.
In accordance with an alternative specific embodiment, the present invention endangers the existence of bourgeois property. The invention becomes an appendage of the machine. The present invention is the self-conscious , independent movement of the immense. The present invention layers the foundation for the sway. The device abolishes the right of personally acquires property. The present invention is the groundwork of all personal freedom. The present invention is the miserable character of this appropriation. The invention is the non-existence of any property. The invention deprives no man of the power. The invention finds its complement in the practical absence. The invention alter the character of that intervention. The device keeps even pace with the dissolution.
The present invention is the most radical rupture with traditional property. The device is the condition for the free development. The invention comprehends the march of modern history. The device is the necessary offspring of their own form. The present invention conceals the reactionary character of their criticism. The present invention has Clerical Socialism with Feudal Socialism. The invention is the head of this school. The device reversed this process with the profane French literature. The present invention expresses the struggle of one class. The device presupposed the existence of modern bourgeois society. The present invention is the object of the pending. The device serves the government as a weapon. The present invention proclaims its supreme and impartial contempt of all class. The invention secures the continued existence of bourgeois society. The present invention keeps even pace with the development. The present invention improves the condition of every member. The invention is a final stage in the reorganisation. The invention is the result of the whole. The invention prepares the way for your community. The invention does the proletarian differ from the slave. The device is the property of one master. The invention does the proletarian differ from the serf. The invention has the use of a piece. The invention does the proletarian differ from the handicraftsman.
According to another embodiment, the device joins the movement of the proletariat. The invention is the theory of a liberation. The device defend the cause of the proletariat. The device is the political liberation of the proletariat.
In accordance with an alternative specific embodiment, the present invention guarantees the subsistence of the proletariat. The present invention prepares the way for its transformation. The present invention is that stage of historical development. The invention is a revised edition of this earlier draft. The device draws up a programme in the form. The invention drafts a programme in the form. The device does not draw profit from any kind. The invention reduced the activity of the individual worker. The invention does this sale of the labor. The present invention accomplishes the liberation of their respective working. The present invention makes itself the first class of the country. The device annihilated the power of the aristocracy. The present invention is a lack of the necessary capital. The device takes the form of constitutional monarchy. The present invention renders the condition of the proletariat.
The present invention follow the same course as its predecessor. The invention characterizes the revolution in the whole social order. The invention is the necessary consequence of the creation. The present invention is the course of this revolution. The device is the victory of the proletariat. The present invention ensures the livelihood of the proletariat. The device requires an entirely different kind of human material. The device controlled by society as a whole.
In accordance with an alternative specific embodiment, the present invention is the influence of communist society. The present invention is the stage of historical development. The device establishes the rule of the aristocracy. The invention makes the common cause with the party.
The present invention facilitates the unification of the proletariat. The present invention popularise this programme document during the revolution. The device become the property of the state. The invention regulates the credit system in the interest. The device substitutes paper money for gold and silver. The invention become the property of the state. The present invention serves nascent bourgeois society as a mighty weapon. The present invention intensified the class antagonism between capital and labor. The invention marks a progressive phase in the class. The present invention is a regime of avowed class.
According to another embodiment, the device opens an abyss between that class. The present invention professed to rest upon the peasantry. The device upholds their economic supremacy over the working class. The device transfers the supreme seat of that regime.
According to a beneficial embodiment, the invention is the positive form of that republic. The invention is the suppression of the standing. The device becomes a reality by the destruction. The device is the embodiment of that unity.
According to a preferred embodiment, the invention serves every other employer in the search. The present invention puts the right man in the right place.
According to a preferred embodiment, the invention clogs the free movement of , society. The device supplies the republic with the basis. The present invention regulates national production upon common plan. The device takes the management of the revolution. The device is the first revolution in which the working class. The invention have put enlightenment by the schoolmaster. The device solves in favor of the peasant. The present invention stops the spread of the rinderpest. The invention display their patriotism by organizing police. The present invention is the abolition of the nightwork. The present invention hampered the real action of the working class. The device retraces this dissolution in The Origin. The present invention carries brazen historical irony as a result. The device is the seat of the emigr� government.
What is claimed is:
1. A method for becoming a property owner himself, comprising:  a previous edition;   a 23-page pamphlet; and   a German edition.
2. The method of claim 1, wherein said previous edition comprises the sleeping partner of the capitalist.
3. The method of claim 1, wherein said 23-page pamphlet comprises the further consequences of the industrial revolution.
4. The method of claim 1, wherein said German edition comprises the special privileges of the nobility.
5. A device for becoming a property owner himself, comprising:  a feudal system;   a mechanical loom;   an own house; and   a parliamentary stronghold.
6. The device of claim 5, wherein said feudal system comprises a national bank with State capital.
7. The device of claim 5, wherein said mechanical loom comprises the political form at last discovered.
8. The device of claim 5, wherein said own house comprises the bold champion of the emancipation.
9. The device of claim 5, wherein said parliamentary stronghold comprises the main consequences of the abolition.
By Rococo Modem Basilisk on September 20, 2016.
The political post I’ve been avoiding writing
The political post I’ve been avoiding writing
This is a post about the current election. I didn’t want to write it, for several reasons. One is that none of my insights are my own, and I expected to see lots of other people making them. Another is that I don’t particularly like making or seeing political posts — even political posts that I generally agree with make me irrationally angry, because they stir up all that stressful tribal instinct. The third reason is that posting about american elections doesn’t and can’t really shed any light on my genuine political positions, because I’ve never seen a political system that really lent itself to the way I feel about politics. However, because somehow nobody is making the arguments that seem obvious to me, I’m going to have to make them, with the caveat that readers probably shouldn’t take this as an indication of my alignment outside the particular and very unusual circumstances that constitute this election.
My position is that, no matter your opinions on any political issues, Clinton is more likely to be a better candidate than Trump in terms of implementing them. No matter where you fall on the various political spectra, you have a better shot with Clinton. If you’re a marxist, or a neocon, or a neonazi, or a randian libertarian, or you really want the united states to become a zoroastrian theocracy, you’re better off voting for Clinton.
Here’s the thing. Trump isn’t very good at *doing* things. No matter what he tries to do, he’s pretty likely to fail; not only that, but he’s unlikely to be able to realize that he’s failed. His mental model of the universe is so disconnected from reality that he believes himself to be a successful businessman. So, if you already agree with his positions, you can’t expect him to successfully implement them. On the off chance that he succeeds in some task, it’s difficult to determine which task it will be, and once he starts something, he can not be dissuaded from continuing. In other words, voter opinion can not influence him. His positions are arbitrary and change often, but they are quite importantly not based on any kind of outside influence: if he accidentally stumbles upon some policy that’s, say, a war crime (as he has), he won’t be convinced out of it; likewise, if he stumbles upon a highly unpopular policy, its unpopularity won’t convince him to abandon it. A Trump presidency is like electing a random number generator: it’s unpredictable, can’t be reasoned with, and although it’s technically unbiased the best possible case is that it will be entirely ineffective.
Compare this to Clinton. Hillary is extremely effective as an administrator, and seems to want power for the sake of demonstrating her ability to wield it rather than for any particular end. While effectiveness in of itself is a double-edged sword in a politician, Hillary has a couple other attributes that make her power much more likely to be wielded for good. Specifically, she has a tendency to make her positions mirror that of the general population — in other words, to flip-flop in order to mirror popular sentiment (if you want to paint it negatively) — and she’s concerned in a fairly realistic way with how history will remember her. By following public opinion, she’s unlikely to perform actions that are terribly unpopular based on some kind of flawed personal conviction; however, her concern with posterity lowers the rate at which she would advocate things that may be popular in the moment but will almost certainly end up seeming terrible to future generations. To use a concrete example, she’ll publicly support gay marriage even if she privately disagrees because she knows that gay marriage is only going to continue to grow in support, but she will never support legislation like that proposed by Trump to ban the immigration of muslims (even though this would not be illegal per-se & has precedent in the Alien Act) even if it’s highly popular with the electorate because such bans will always look bad at some point in the future.
While people criticize the candidates for being weak in either confidence or implementation, there’s really only one circumstance in which a leader that has a set of strongly held positions that they implement effectively is desirable: when you agree with all those positions to a greater degree than the leader does. A strong leader with unpopular positions is a disaster of a dictator; we would rather a leader with positions with which we disagree be ineffective, because then his or her decisions would be irrelevant. Alternately, a leader whose policies are flexible but whose ability to enact policy is good can be bent to the will of the people, and becomes a tool of the people: an even better result. In a sense, to the degree that leaders are strong, we wish them to be corrupt in a very specific way: weak to the forces of popular sentiment now and in the future, but strong against the kind of organizations that exist to warp politicians’ sense of what positions have popular support (industry lobbying groups and such).
By Rococo Modem Basilisk on September 29, 2016.
Trump doesn’t know what he wants, or how to get it.
Trump doesn’t know what he wants, or how to get it. Best case, he gets nothing done; expected case, he does something by accident based on a half-considered remark and it turns out to be something nobody wants.
Clinton, on the other hand, is likely to be “more of the same” — in other words, a general extension of the right-center policies that characterized the Bush and Obama administrations (and indeed Bill Clinton’s). She’s a known quantity, with enough experience to avoid stumbling into a big dumb war.
Clinton is a strong enough leader to execute the plans she has, which are, essentially: avoid pissing off voters, and otherwise change as little as possible. This makes her “conservative” in the traditional sense, and hers is likely to be an uninteresting and unmemorable presidency (like Carter’s).
Trump will either be considered a failure, or will succeed at something that will be considered a huge mistake. A failure would be preferable — but that just brings him down to slightly below Clinton’s expected behavior.
By Rococo Modem Basilisk on September 29, 2016.
I completely understand the desire to make big changes.
I completely understand the desire to make big changes. I don’t find the current system sufficient. (The term closest to describing my political sentiments is “anarchist”.) That said, I can’t stand by the idea that a “middle finger vote” is justified, particularly when it’s unlikely to result in real change; breaking down a complex and resilient system like a government requires a strategy.
If you want someone to tear the system down, you will probably want to elect someone like Gary Johnson: he has a clear idea of which parts of the system he would like to remove and how to remove them.
Trump is unlikely to tear it all down. He has no reason to want to perform major restructuring; he doesn’t have enough understanding of the current structure to know how to disassemble it; even if he did, he probably couldn’t keep his attention focused on the task of tearing down government for long enough for it to happen. A Trump presidency will be comparable to that of Bush: more of the same, but with more mistakes.
To tear a system down requires more than bringing in an unpredictable wildcard. While someone like that is unpredictable in the short term, the distribution of their behavior is predictable at scale: a random walk seldom goes very far and often returns to the center, not despite but because of its randomness.
With regard to the negative sides of the status quo, I think even this favors Clinton. She will avoid war and torture whenever possible — after all, they have bad PR — and to the extent that she condones them at all she will do so very carefully. Trump will not avoid war: he will act in accordance with momentary displays of dominance, rather than in accordance with risk.
By Rococo Modem Basilisk on September 30, 2016.
I was taking executive orders into account, here.
I was taking executive orders into account, here. I don’t doubt that Trump would issue plenty of executive orders, but I doubt that they would be effective in doing whatever he intended them to do, and I suspect that many of them would do things he didn’t expect.
Ignoring the difference between legislation & executive order in terms of how much cooperation is involved, we have two candidates neither of whom have any particular deeply held convictions by which we could predict their behavior. One of these is a highly efficient and effective administrator whose desire for power and recognition drives her behavior in very predictable ways, and who can easily be manipulated into effectively administering the country in ways that are desirable if minor. The other has behaviors that cannot be influenced or predicted, but, luckily, he’s never been terribly effective at giving orders or convincing people to do things, so to the extent that he is likely to make sweeping changes, it will be in completely arbitrary, unpredictable ways.
By Rococo Modem Basilisk on September 30, 2016.
Good vs iconic art
Good vs iconic art
When it comes to narrative forms of art (film, literature, comics), there’s often a great disconnect between works that are highly influential (and thus become ‘classics’) and works that are well-executed. I don’t think this is an accident.
Well-executed works (hereafter, “good”) don’t particularly need to be novel. Whether they are high art or low art doesn’t matter — someone working in a “low” genre (say, harem anime) can be skilled enough to produce a truly shining example of that genre. Like good design, good art disappears: it is a complete and flawless implementation of expectations, and only upon close examination does the skill and effort involved in executing the expected attributes of the genre become visible.
As an example of unambiguously “low” art being extremely well executed, consider Monster Musume — a harem show involving young women who are mythological creatures. It seeks to sate essentially puerile desires: the goal is to have sexually-charged slapstick humor involving attractive women who are part animal. It is not the first show to do this by a long shot, but it may be the best: nearly all elements outside those that add to that goal are invisible, and it reaches its goal admirably, but at the same time it has a good and well-planned justification for nearly everything that happens (by combining the idea of diplomatic relations, corrupt/lazy officials, and a largely hostile population, you get an interesting take on the fantasy harem genre that can be seen as a stealth satire of racial politics and international relations), and it furthermore satirizes its own genre admirably (the protagonist’s name is never spoken in the show and rarely in the source material, and his facial features are often left blank; in other words, he is an exaggerated form of the self-insert character).
While “good” art may be popular in the short term, it is rarely influential: mostly, it subsumes itself in its own influences. Iconic works are by definition influential; they often come from outsiders and break best practices. While iconic works are memorable, it is rare that an iconic work is superior to those works that copy it. Consider Dracula and Frankenstein — both extremely iconic works, both widely adapted, and both (in their original form) nearly laughably incompetently constructed by the standards of both our time and theirs. It is not the high standards of craftsmanship that make Dracula and Frankenstein iconic; it’s not their popularity, either, although both were popular. Instead, each brought into being a particular collection of ideas that fit into a missing slot in the culture; this undigested piece of mental matter was sized upon and as these works were adapted or influenced other works the core interesting idea was progressively isolated from its trappings, until the point at which it becomes fully assimilated and no longer numinous. Dracula is no longer numinous to us: we have a mental image of Bela Lugosi in a cape, and we forget that he was supposed to have hairy palms, and we forget about Doctor Von Hellsing being a blood expert, and we forget about Lucy being obsessed with wax cylinder audio recording; while all of those elements are more interesting to us now, the iconic elements of that story are sufficiently captured by Bela Lugosi in a cape, to the point that this view of vampires as nobility (which did not originate in Dracula but really had its purest representation in the 1933 Universal Studios adaptation of Dracula that has become the most iconic one) has become dominant. The idea of permanently young and beautiful blood-sucking aristocrats afraid of the sun is one that resonated with the early twentieth century American culture very strongly, even as Max Schreck’s portrayal of Orlock in Nosferatu is a more accurate representation of how Dracula was portrayed in the book. Likewise: Frankenstein’s Monster originally looked far more human, and was highly intelligent (speaking several languages); we have made an icon out of an ugly Frankenstein’s Monster incapable of speech or complex thought and a Victor Frankenstein with a god complex and boundless ambition, rather than a beautiful but slightly unnerving monster and a Dr Frankenstein who wouldn’t be out of place at a My Chemical Romance concert because the former was a closer fit for exactly what was (and no longer is) unnerving about the story. Iconic works allow us to identify the unheimlich and integrate it into ourselves and our society in a disarmed form; or, to be more cynical: the Spectacle uses iconic works as an early warning system telling it what to consume next. Of course, as highly iconic figures become fully integrated, they cease to have the impact they otherwise would: recent Godzilla and King Kong movies flopped for the same reason that new viewers wouldn’t watch the originals, which is to say that these monsters have been integrated and are no longer monstrous.
Iconic works don’t need to be bad in order to be iconic, but even if they display technical excellence, they will face initial rejection. Consider Neon Genesis Evangelion: certainly iconic, and hardly poorly made, this show garnered very little interest during its initial run. Part of the reason is that it aired on an incorrect slot: this show, with its dense references to media from the 70s and its complex character dynamics and psychosexual undertones, was airing in a time slot that normally was geared toward ten year old boys. But, it’s more than that: Evangelion remains highly divisive, and remains relevant more than twenty years after it first aired, because its iconic elements have not yet been fully assimilated. Evangelion didn’t introduce the unwilling soldier (in fact, this element is part of why Gundam was iconic); it didn’t introduce the idea of a complex and incestuous conspiracy between a private high-tech defense organization and various government and religious authorities. But, Evangelion took upon itself the task of examining attributes of the mech genre realistically, and did so by taking a bunch of characters who border on archetypal and spending a great deal of effort trying to make their behavior and characterization realistic. Evangelion is still relevant because we haven’t figured out what makes it relevant yet: every post-Evangelion mech show is in some way a response to Evangelion in the same way that every post-Dracula vampire novel was a response to Dracula, yet even as some individual creators have done several generations of responses, none are a sufficient substitute for the original. For instance, Yoji Enokido, after working on Evangelion, went on to work on Utena (a non-mech show with certain very visible Evangelion references), Rah Xephon (a mech show that has been seen as an Evangelion clone), Star Driver (a mech show very similar to Rah Xephon with a more typical mecha protagonist), and Captain Earth (another mech show with a more typical mecha protagonist, which has some scenes directly lifted from Rah Xephon); while his vision has diverged from Evangelion proper, he’s still chewing a piece of the same cud.
By Rococo Modem Basilisk on October 3, 2016.
How to shoot yourself in the foot with good worldbuilding: two methods
How to shoot yourself in the foot with good worldbuilding: two methods
Sometimes, in the course of telling one story, you make changes to the imagined world you’re creating that opens up the possibility of telling a much more interesting story. This story can’t be told without moving focus away from the characters you’re currently involved with. No matter; there’s no way that these main characters are more interesting to your reader than they are to you, and if there’s demand for them, you can explore what’s going on with them later.
Always tell the most interesting story going on in your world.
Sometimes, you are telling the most interesting story going on in your world, but readers are losing interest: a formerly rabidly creative fandom has stopped writing fanfiction, complaining that the new installments feel hollow. You told the most interesting story in your world, but you limited your world so that the story you were telling was the only interesting story to be told.
Always make sure that someone smarter than you can tell a more interesting story in your world than you can imagine.
By Rococo Modem Basilisk on October 3, 2016.
I agree that “For You” used to have a much better signal to noise ratio.
I agree that “For You” used to have a much better signal to noise ratio. I’m tempted to blame the increased size of Medium’s user base — after all, with a small user base it’s a lot easier to tweak the algorithm for good results, a small sample set is a good enough proxy for recommendation quality, and few people are going to be trying to game the system, while all of those things grow worse at at least a geometric rate as the number of users grows.
That said, I’m seeing a particular kind of bad post pop up much more frequently on Medium in the past ~2 years than previously: the short post that should be much shorter. I see a lot of medium posts that are two paragraphs long, making a point that would be clearer as a tweet, along with a couple huge and irrelevant images and somebody’s affiliate link.
I used to see these mostly as HN links (since I browse HN’s “newest”), and chalked it up to the culture of shallow self-promotion that’s all too common on HN (combined with the fact that, like Blogger a decade ago, Medium provides people who have no technical skills with a free blog that looks relatively professional). Because I rarely interact with such posts other than viewing them, I doubt that my feed is full of them due to this early exposure; instead, I suspect that this behavior has become normalized — it’s now expected to post “one minute reads” that are actually ten second reads, and my darling “twenty minute reads” and “eighty minute reads” are seen as bad for business, creating a low view to read ratio — nevermind the fact that there aren’t any ads on Medium (other than native ones) so impression metrics don’t really matter.
Another possible source is following people who have lower standards for recommendations than I do. Such people may typically write nice, long, well-considered posts, but yet recommend all the crap they agree with, whether or not it’s worth reading. Medium doesn’t distinguish between following somebody for their posts and following them for their recommendations.
By Rococo Modem Basilisk on October 3, 2016.
If your graphic design is optimized for impact at a distance without scrolling, your use case is advertising (or something like it).
The typical use case for the web should not be an ad.
Maximizing the amount of legible text on the screen encourages people to create informationally dense content — it means that a short article like yours looks lonely and empty, while longer articles seem like a more natural fit. This seems like a pretty good thing to encourage; I certainly prefer long, in-depth writing to shorter, shallower stuff.
To the extent that designers should be allowed to control default text size, for primarily written content, ten pixels is probably the upper limit for what I’d consider acceptable; any more artificially inflates the perceived size of a piece of writing. However, web designers have a lot more control over the way that sites look and act than they probably should for accessibility reasons: if I set my browser’s default text size to six, websites should accept that, the same way they should if someone with poor vision sets their default text size to thirty.
An increased default size for body text points to a set of values that are already far too common on the web: a preference for flashy showmanship over well-delivered content. Good design steps aside and becomes invisible; the exhibitionist design-masturbation of huge body text never can.
By Rococo Modem Basilisk on October 5, 2016.
I wish I could recommend this article multiple times.
Here’s the thing. I read a lot of news articles on technical subjects, and so I’m extremely aware of all these patterns (so much so that I expected, before reading this article, that I would have a lot to add); but, I can’t possibly be reading as many of these as the people who write them for a living. Occasionally I see an author whose work I respect move to a different context and start making these mistakes. Are they being enforced? Are they just popular because they are easy?
By Rococo Modem Basilisk on October 5, 2016.
Please badmouth CSS more in developer talks
A response to a popular article.
The appropriate response to a perceived competence gap between web development and application development is not to fake admiration for the worst tools of web developers, but instead a concerted effort to improve tools and knowledge in both communities. If someone’s complaints about CSS make you feel like your skills are being belittled — well, that’s probably an indication that you need to improve your skills, and learning why your preferred tool is bad is a good first step.
The fact that some people are capable of making impressive things with a tool does not make the tool good. Making impressive things with bad tools (or with good tools that are intended for a completely different purpose) is a tradition in the tech community; it’s called hacking. Writing a text adventure in postscript is impressive only because doing so is a terrible idea. Likewise, modern web development is impressive because HTML and CSS are limited enough to make most things that would be easy in other domains very difficult in a browser. This is not a point in favor of CSS; it is a point against it. A tool is good if easy things are easy in it and hard things are only slightly harder; CSS fails this test.
Normalizing the use of a poor tool in which a great deal of effort is necessary to solve common problems has knock-on effects. If an absolute beginner can’t perform extremely common tasks (in other words, if new users are buried under an avalanche of gotchas), those tasks are pushed onto intermediate users; more difficult tasks are relegated to advanced users; difficult tasks that need to then operate consistently and reliably — well, that’s just something nobody has time with. And, if you cut corners and bring on somebody who is slightly less skilled than is necessary, you’re more likely to get inconsistent and unreliable results even for simple tasks, because the difficulty curve is all screwed up and beginners don’t know the snags they haven’t researched yet. This is a pattern that will happen with any poorly-designed, over-complicated, inconsistent tool: anything created with the tool will be systematically slightly worse than anything created by someone of similar competence with a well-designed tool.
Pretending a bad set of tools is good lowers the bar for good tooling. It encourages an environment where bad tools are the norm, and encourages people to learn only bad tools. Just as the web is an absolute horror show (ultimately just because Tim Berners Lee cut a bunch of corners in 1992), we have big groups of people who think using hadoop & hive is a good idea when a single unix command line running on one core will do the same amount of processing in 1/80th of the time, and we have academic fields where significant numbers of statistical errors in published papers are resulting from bugs in Microsoft Excel. Bad tools should be shamed, and use of bad tools should be limited and careful.
Computer programmers have spent a lot of their history in the sandbox. In the 60s and 70s most of the interesting things being done on computers didn’t have to be stable or reliable; our modern programmer culture derives mostly from the group that “shot from the hip”, rather than from the serious and conservative professionals who were crunching numbers on IBM boxes during this era. From the late 70s through to the mid 90s, personal computers were mostly not networked, and for part of that time permanent storage was limited — the cost of a mistake was that the end user had to reboot the machine, usually, and even though hardware memory protection facilities existed on PCs after 1987, they remained unused for the next ten years. Meanwhile, those who used the internet were universally technical and could expect to fix their own problems. Sloppy development, and development tools that made non-sloppy development difficult, became normal. But, we aren’t in the sandbox anymore; poor decisions made for toy projects in the early 90s are coming back to bite us daily. Poor tools and sloppy decisions are no longer acceptable.
By Rococo Modem Basilisk on October 6, 2016.
You had me up until “short but complete”. Lessons of less than ten minutes? Come on.
Typical video lessons, like typical classroom lessons, contain a bit at the beginning that’s a review of previous material relevant to the current lesson, and a bit at the end that gives an overview of the current lesson and a teaser for the next. While the teaser portion can be omitted, an overview of previous relevant material cannot be eliminated: people frequently view these things out of order, or zone out through part of them, or misunderstand which points are central. Ten minutes isn’t nearly long enough to cover enough content to be worthwhile if we also have these overviews.
In domains with practical application, like the programming-related domains you focus on in your article, there’s a place for extremely short chunks of information. Specifically, after finishing a course or initially learning the material some other way, a person may need a quick reminder while actually applying the material. A video or audio lesson is a terrible match for this use case: finding and skipping to the relevant information is slow and error-prone. For short chunks of information, text is ideal. Audio and video based courses, however, have the edge in introducing new material to a partly or mostly passive audience.
Your other points generally make sense. But, I’d argue in favor of 1–2 hour audio/video lessons plus textual review/summary sheets.
By Rococo Modem Basilisk on October 6, 2016.
You missed one *big* factor in why SV isn’t good to emulate: SV is mostly “show business” — big phantom valuations for vaporware, etc. SV’s startup culture is systematically absurdly prone to being taken in by BS, compared to all the places where a “tech startup” is just called a “small business”.
It’s one thing to mimic a system that’s truly been shown to work once. It’s another thing to mimic the simulacrum described in the marketing materials of a system that, like a ponzi scheme, pulls in wave upon wave of fresh meat to dash against the rocks in order to produce the raw material used to prop up a handful of names that can be passed off as “success stories” in later brochures. Because of the enormous amount of churn and hype, it’s often hard to tell that SV has in many ways an abnormally high failure rate, with even its successes still failing to make money; after all, real success is boring and undramatic: real success is the family laundromat on the corner that’s been running for eighty years, not the social network for dogs that gets a six billion dollar valuation on paper because the VC had the hots for one of the presenters and then crashes six months later because nobody wanted it.
By Rococo Modem Basilisk on October 10, 2016.
The way I think of it, trying to learn computer science without learning C is like being a medieval scholar trying to learn medicine without first learning Latin: whether or not you *like* it doesn’t matter, because it’s simply the language everything has been written in since the 70s; while you can get by without learning any one specific minor language (read: pretty much everything but C is a minor language), no matter what language you prefer to write in you will need to be able to read C.
By Rococo Modem Basilisk on October 11, 2016.
Upending the system requires effort and planning.
Upending the system requires effort and planning. Electing a moron is going to produce business-as-usual-with-extra-drama, which is not an improvement over business-as-usual-with-extra-efficiency.
Actual improvement has to come from someone with a knowledge of the mechanisms of government and how they can be subverted. We don’t have time for a fuzzing attack on government — by the time such an attack has any meaningful results we’ll be long dead. We need someone to exploit known vulnerabilities.
By Rococo Modem Basilisk on October 11, 2016.
You’re struggling with some of the same conundrums as I am.
For some years, I’ve been playing with automated text generation. After seeing the coverage of the gaming of short Kindle erotica, I verified that erotica was easy to generate. But, while I would have very little problem “scamming amazon”, I definitely would like to avoid scamming amazon’s customers. So, I was wondering what I could possibly do to separate machine-generated erotica for people who get a kick out of the idea of machine-generated erotica from machine-generated erotica designed to be passed off as human-generated erotica until after the sale. Does clear disclosure in the book summary and author summary constitute sufficient cover? Does clear disclosure mean I get banned by Amazon due to their mysterious content policies? I’m enough of a tightwad myself that I’d be mortified if somebody spent $0.99 on an ebook of mine without knowing beforehand that it was 300 pages of algorithmic churn.
Interesting work keeps getting done in the margins, and some gems (like Tingle) appear in the thieves’ quarter, so distinguishing oneself from the thieves in at least intent is very important. But, if you do a thing that scammers do, does disclosing it make you no longer a scammer? And, in whose eyes?
I think often, the people who pay for content farm extruded erotica aren’t themselves being scammed: they got what they expected to get, and considered the gonad-tickling suitable for what they paid; instead, the scammed party is perhaps Amazon (who would prefer to have a better reputation), or the workers on Mechanical Turk (who decided to accept this but maybe should be making at least minimum wage), or nebulous other parties even less directly affected.
By Rococo Modem Basilisk on October 12, 2016.
I switched to NaNoGenMo because of similar commitment problems.
Pros:
• You will probably produce much more than one novel. (I usually produce one during the first half-hour of November 1st just to get it out of the way, and then work on more interesting / complicated projects afterward) • There’s a pretty active community, with deep and precise discussions of things like structure and themes, because computer generation of long-form narratives that remain interesting to humans over the course of 90+ pages is a hard problem.
Cons:
• You need to know how to code • The novels you generate will be even less likely to be salable than the hurried work of an amateur human novelist • Explaining the concept to people who aren’t familiar with it is even harder than explaining NaNoWriMo, because a lot of people are somehow unaware that computers can write books
By Rococo Modem Basilisk on October 13, 2016.
In defense of contempt
In defense of contempt
A response to a popular article
The article in question suggests that the habitual tribalism & combative style in communication within the tech community is toxic, particularly to minorities; I do not dispute this point. The article in question also suggests that criticism of languages and technologies should be avoided because it often discourages community diversity, and this is where the author and I part ways.
The state of the programming community is poor, with regard to diversity, and this leads to all sorts of systematic problems that are self-perpetuating. However, the state of the programming ecosystem is also poor, and the perception of acceptability given to bad tooling and bad habits leads to systematic and self-perpetuating problems of its own. The way to increase acceptance of outsiders into the community is not by sacrificing the very worth of the enterprise the community exists to engage in; indeed, it’s entirely unnecessary to do so.
The author decries the tribalism of the community with regard to tooling, but differences of opinion when it comes to preferred tools is not a meaningless aesthetic distinction. The prevalence of overflow-related vulnerabilities in real software ultimately comes down to the popularity of C over Pascal historically; as many exfiltrations of password files and other sensitive data are owed to the use of outdated PHP best practices as can be attributed to SQL injection (and thus, lack of input validation); the poor state of Windows security prior to 2001 when compared to competitors at the time ultimately comes down to the decision to avoid taking full and proper advantage of hardware memory management, setting up a proper system of user privileges, and other common practices in the domain of network-connected multi-user OSes — in other words, Windows was a leaky sieve and prime target for over a decade because lazy habits that were acceptable for single-user isolated machines with no real multitasking were being applied to a huge number of interconnected boxes.
The results of using a poor tool or using a good tool poorly are a lot like the results of ignoring modern medical science: in isolation, they might be acceptable for a handful of people who don’t have it rough, but in aggregate they result in epidemics. Someone who writes ostensibly production-ready code in PHP or Perl should be treated like someone who refuses to vaccinate their children: their behavior should be considered acceptable only if they are extremely careful and they have a very good excuse. Someone who promotes the use of tools that encourage the production of bug-prone insecure code outside the context of isolated personal experiments should be treated the same way we treat antivaxxers: as a perhaps well-meaning but deluded person whose misinformation is resulting in major destruction.
When someone has different aesthetic preferences, it’s natural to accept that. But, when a group that is already marginalized disproportionately adopts a set of tools that are well-known to be destructive and then dedicates enormous resources to the use of those tools, we don’t decide that those tools must be acceptable on aesthetic grounds despite their known destructive potential: we instead try to discourage that group from associating with those tools and figure out what forces are creating that association.
Poor tools are often the domain of beginners, and those who dedicate sufficient time and effort eventually graduate from those poor tools to better tools. (I first learned to program in QBasic.) That time and effort isn’t free, so people who are already under other extra constraints (including people who have extra social or financial pressure) often never move on.
There’s another factor here, however: good tools in some ways often become poor because they become popular with beginners. Most tools are optimized for a small set of problem domains, work acceptably in some others, and work horribly in every other domain. A beginner, having experience with only one tool, will apply this tool to every domain; if problems in some domain are harder to solve with this tool, the beginner, unless properly instructed, will believe the problems in this domain are simply inherently harder to solve. As a tool becomes popular with beginners, experts become difficult to identify in the crowd, and slightly elevated beginners begin to become treated like experts simply because there are many more slightly elevated beginners than experts; these pseudo-experts will popularize poor habits in the community, and these habits beocme associated with the tool itself. An expert who uses many tools will have less say in the community surrounding one tool than the many enthusiastic beginners who are unaware of or reject all other tools. To some degree, the most toxic tribalism is that of beginners who don’t think of programming languages or techniques as tools and identify themselves with their preferred tools.
We should separate criticism of tools based on legitimate concerns from criticism of tools based on tribal or class issues. Plenty of tools can be used well but largely aren’t because most of their devotees are beginners (see: Java, C, C++, Python). Other tools are fundamentally flawed, and while using them well is not impossible, it is a trick that takes a great deal of experience and is beyond the scope of nearly all of its audience (see: PHP, Perl, Javascript). Some tools have lost a great deal of respect because most of their ecosystem is populated by tooling that’s orders of magnitude worse than their original design, compounding flaws (see: Java, Javascript, Ruby). Other tools are perfectly fine for what they were designed to do but are almost always used for things they’re terrible at (see: Perl, Javascript, Lua, TCL). The popularity of a tool with beginners can certainly negatively affect the suitability of that tool in genuine and valid ways if the beginners are given sufficient control over the tool’s later evolution, so it’s not as though a tool’s popularity with beginners is inherently irrelevant, but a good tool can be used well even as most people use it poorly.
There’s another interesting tendency with regard to the popularity of certain tools with beginners, and it’s one that’s wrapped up with institutions and politics. This is the matter of pedagogy. Java is currently extremely popular, but its popularity owes little to its attributes and much to the fact that it has become part of a standard; there is a curriculum surrounding Java focusing on a Java-centric view of object orientation, and this curriculum forms the basis of both the AP Computer Science curriculum in the United States and various certification and accreditation rules for university programs. In other words, if you live in the United States and you are not an autodidact your first programming experience (barring a bootcamp) will probably be in Java, combined with a curriculum that focuses on UML, inheritance, and the details of Java-style encapsulation, while completely ignoring performance concerns and systematically denying that some problems are not easily represented in an object oriented model. Prior to Java, these programs centered on C++, with a similar set of foci. In other words, for several decades, students with no prior programming experience have been taught that there is one language (Java or C++) and one technique (Java-style OO) that is the best at everything, and as they filter into industry they work with other people who went through the same indoctrination and continue to produce huge ugly monoliths of inefficient Java and C++ “enterprise” code. This is the end-game of letting an echo chamber of like-minded beginners dictate the state of an industry.
So, what do I recommend, with regard to the problem of balkanization in tech pushing out minorities?
I consider this really to be an issue of beginners graduating to higher levels of understanding (and systematic pressure making it harder for certain groups to graduate out of the beginner classification), and one way to help this is to be extremely clear in your criticisms about the nature of the problems you criticize — in other words, rather than saying “PHP users are dumb”, say “PHP is a deeply flawed language, and PHP users should be extremely careful when using these particular patterns”.
Another way is to make it clear that using a single language is not acceptable in a professional context: any serious developer has a large toolbox already, and if beginners understood that language preference is not a reasonable basis for long-term tribal divisions because any professional belongs to multiple tribes, the toxic identity-based hostility between programming language communities would mostly go away, allowing concrete and issue-based critiques to become more visible.
Also, seasoned developers who frequently work in many languages and have a deep understanding of the positive and negative aspects of many tool designs should become more vocal about tooling: even-handed discussions about this subject make it easier for beginners to graduate into well-rounded developers and avoid making common mistakes that lead to wide-scale disaster.
Finally, standardized programs for computer science education should include language survey courses earlier and feature them more prominently, while removing some of the pro-OO bias that currently characterizes them: nobody should be able to graduate with a CS degree without being truly competent in at least five or six very different languages, rather than the typical gamut of Java, Javascript, and SQL, and they shouldn’t graduate without non-trivial exposure to twenty or thirty more.
By Rococo Modem Basilisk on October 14, 2016.
Javascript won’t save the web. Javascript is part of the problem.
The existence of HTML (and any embedded markup) is part of the problem, and generating all elements with Javascript won’t help. CSS didn’t save the web from HTML because the CSS/HTML division misunderstood the problem: content chunking is inherently part of presentation, not content, and any presentation layer should be an external (rather than embedded) markup that can style and rearrange arbitrary spans of content (presumably based on byte or character indices).
HTTP is part of the problem. HTTP doesn’t distinguish between dynamic and static documents, and while it has facilities for representing redirects, moved files, and other potentially useful features, nobody implements or uses any parts of HTTP other than the behavior of codes 200, 404, and (occasionally) 500; even very useful features like partial download requests and file time requests are inconsistently supported.
HTTPS is part of the problem. Hierarchical certificate signing chains will always be vulnerable to leaked top-level certificates, and poor support for certificate revocation will continue to slow adoption of improved hash algorithms.
DNS is part of the problem. The association between hostnames and IPs is only useful from the perspective of a machine (or a programmer thinking at the machine level); the association that is useful to users is one between names and chunks of data, or sometimes between names and services.
Javascript in a web context will never save the web, in the same way that a tumor will never cure cancer. The problems with the web go a whole lot deeper than the front-end concerns that Javascript can address.
If you want to know what *might* save the web, take a look at IPFS & IPNS, then take a look at Project Xanadu.
By Rococo Modem Basilisk on October 14, 2016.
I for one appreciate your resistance to cutting down your articles.
I for one appreciate your resistance to cutting down your articles. I like to read deep, informative writing, and your work here on medium is some of the best.
There’s a trend toward short (~500 word) articles here; it’s a trend that won’t survive, since it alienates people who want a less casual approach to learning.
By Rococo Modem Basilisk on October 17, 2016.
Intellectual property is the strongest it’s ever been, more or less worldwide.
Intellectual property is the strongest it’s ever been, more or less worldwide. Large, powerful organizations with enormous IP portfolios are active in the machine learning space: IBM has a huge number of active patents, and Google and Apple are both busy stockpiling them from third party sources. Even if learning models are legally determined unpatentable, that doesn’t keep people from attempting to (and often succeeding at) filing patents for them; the nonexistence of such patents doesn’t prevent any company with lawyers and brass balls from attempting to defend them (and any other nebulous or imaginary claims) in civil court.
Even if, somehow, these extremely powerful forces don’t manage to get their way in terms of ensuring learning models are protected by some form of strong IP (copyright for individual models or patent for novel formulations), and somehow the IP litigation system that has for decades been systematically favoring IP holders and ignoring strong fair use cases reverses tack, and somehow these companies forget that they could make a trade secret claim — in other words, even if somehow our dysfunctionally overpowered IP system suddenly started working properly — learning models are hardly the most common forms of potentially protected work, and they are years away from being capable of producing work of equivalent quality to most protected work. In other words, the end of protection for learning models is insignificant compared to the scale of IP.
Theft of learning models, of course, is both trivial and unprovable. A system intended to produce certain outputs for certain inputs can be trained on the same data or can be trained on API calls; as scale the result is the same, but the innards will be uncomparably different even for a very close match in behavior. Much like other behemoths of tech, the factor that would keep competitors out of the race with machine learning based API services is not the (public) concept or the (trivial and novel, mostly off-the-shelf/open-source) implementation but the cost of scaling to meet demand — anybody can write a facebook knock-off in a weekend but only facebook and a few others can afford the server cost to host facebook’s audience. Similarly, anybody can download tensor flow or torch, but few people can afford the cycles to train it on the entire google books corpus and add new books as they are released.
We don’t call facebook knock-offs (even very close ones, like those used for phishing) copyright infringment and consider them subject for suit, even though they definitely are using image assets against TOS; instead, we treat them as either legitimate attempts at competition doomed to failure or as cheap knock-offs indended to trick us. Likewise, trademark law is rarely applied directly against parasitic industries like that of mockbusters — the legal risk of loss of protection is low, and large film companies are mostly fine with allowing the parasites to continue preying on people with poor vision or damaged faculties of judgement who can’t distinguish between “Transformers” and “Transmorphers”; Universal is happy knowing that Asylum will never be able to compete with them head to head, and once the current generation of executives dies off and is replaced, they will treat internet piracy the same way.
By Rococo Modem Basilisk on October 17, 2016.
I was once on cleanup duty, in a particularly unfortunate situation.
I got a chance to work for a long-time hero of mine: someone who was influential in computer science circles in the 60s and 70s, who (while fairly non-technical himself) has a lot of ideas that haven’t really been given the chance they deserve. Along with a friend I had roped in, we embarked upon trying to ‘finish’ a ‘prototype’ that had been provided several years before by a self-taught programmer who had never worked on a large project before. He had told the man we were working for that he had gotten it very nearly finished, and it did demo nicely, but he burned out so badly that he ended up (from what I understand) in a mental hospital for a while, and had been out of touch and unable to work on that or any code for several years by the time we saw it.
Initially, the problem was that we couldn’t find any of the code. We were told that it was a combination of C++ and python, wherein python was being used as a plug-in language; we found no C++ or python code in any of the “source” zip files we were given, and while revision control had been provided, this guy never used it except once, right before he quit, to check in all the windows binaries and a whole bunch of screenshots and videos of various prototypes, along with a couple pieces of out of date documentation and a pdf copy of Dive Into Python.
Eventually, after much digging around on the part of various parties who at one time or another had copies of this, we got a large zip file that contained a nested series of smaller zip file copies of the same directory structure with progressively earlier dates. On the third or fourth level down, we found a single C++ file.
We discovered that this single C++ file contained all of the C++ code for the entire project. Nearly all of it was commented out, using line (not block) comments. It wouldn’t build on any platform — it had a bunch of typos and wasn’t actually valid C++; in other words, the only copy we had of the C++ source was a messy, old, in-between version. But, inspecting it, we discovered that it had partial C++ implementations of several functions that were supposedly “done” (such as loading and parsing a proprietary file format), which were mostly disabled. It was difficult to determine which of these were disabled, because there would be several functions with almost the same name that had nearly exact duplicates of the same code, and various caller functions would use various version. Often we discovered that some function that was closest to complete-looking couldn’t possibly work, only to discover later that the only call to the whole chain of operations had been disabled and replaced with some hard-coded value. This single C++ file was several megabytes in size.
Eventually, in order to make it easier to debug, we separated this file into about ten, by categories laid out in this (obsolete) documentation, and determined which blocks of code were definitely not close to functional, removing them. After this streamlining we got the size down quite a bit, but discovered that nearly all of the functionality was missing. Complicated mechanics behind drawing, file parsing, object placement, and structure were nowhere to be found, but it built and worked — on windows, at least. (It had also been sold as cross-platform despite being build in visual studio; it turned out that it was windows-only, but mostly because the author had used a windows-specific sound library to play notification sounds instead of using the one that came with SDL. We quickly fixed that.)
Combing through the code, we discovered that there was a single line early in execution that loaded a hard-coded arbitrarily-named text file (“abiowy2222.txt” or something) as a python script. We found a directory full of strangely named text files, and while some of them were full of junk (copied and pasted pieces of documentation or forum discussions, lists of error messages), about half of the 100+ text files were partially overlapping versions of a big chunk of python code.
It turns out that this text file contained a bunch of python code that performed a bunch of calls back into C++ to perform draw calls on some large chunk of hard-coded data. This programmer hadn’t bothered to write code for loading that ugly file format he designed; he hardcoded the content of the one file he was using for the demo. He had skipped writing the logic for determining layout, and instead had hard-coded the positions for the objects described in this file. And then, this python file had its own main loop and exited at the end of it — in other words, nearly all of the C++ code was entirely disabled.
We endevoured to rewrite pretty much all of this logic, and we did, at least twice. We wrote an actual implementation of the file format loader (and discovered that most of the examples we had were subtly corrupted) and an actual implementation of the layout logic, both in C++. While trying to debug a (semi-independent) module that implemented a kind of non-relational database based on a graph of arrays of pointers, we decided to attempt a pure python implementation, in the hopes that it might be fast enough to be a good comparison. (This original author was obsessed with premature optimization and with using obscure features of C++, and had comments next to each function calculating — typically totally incorrectly — how many bytes per object were being transferred. The database was implemented in an overly complicated way for documented speed-related reasons, but it turned out to be both slower and more bug-prone than the straightforward and naive approach we took in python, across many varied tests.) Having determined that this database in its C++ form was essentially beyond salvaging, we used the python version instead, and spent a great deal of time trying to square the fact that initial draw time was so fast with the extreme slowness of interactivity. We ended up rewriting most of that draw code, before rewriting all of it from scratch in python in a single all-nighter. This all-nighter occurred about two years after we initially started working on this project.
We did this for free, since we were doing it for a mutual hero, but it really put us off the idea of playing the role of code doctor in the future.
By Rococo Modem Basilisk on October 17, 2016.
The legality of the provenance of the material is irrelevant to the ethics of reporting.
The legality of the provenance of the material is irrelevant to the ethics of reporting. After all, leaks like this are not being submitted directly to outlets: they are being handed to the public. To the extent that journalists have to worry about reporting on leaked material, they have the same concerns about publicizing any other effectively non-secret information: do they do more harm than good by repeating something to a wider audience?
The question becomes more complicated when an outlet receives the content of a leak directly, as happened with the Snowden documents: in that case, the material is still effectively secret, and releases must be carefully vetted, because in a sense the outlet is conspiring with the leaker, and is the party providing damage control.
Reporting on public leaks should be treated the same way as reporting on suicides or other sensitive yet non-secret events: they should be news if they are newsworthy, and they should be reported on in such a way that minimizes damage that might come directly from the manner of reporting rather than from the facts being reported.
By Rococo Modem Basilisk on October 17, 2016.
To “compete” in web search results is somewhat absurd.
To “compete” in web search results is somewhat absurd. One-off retailers are rightly viewed with suspicion, and making a living off web advertising is something available only to sites at the scale of BoingBoing.
Make good content, ignore monetization, and you may end up with the self-satisfaction of a job well done. Engage with any form of monetization and you lose even that. Good content doesn’t need to appear high in search results, and doesn’t require monetization: if you post things that are of interest and aren’t already on the web, you will attract an audience; break either of those two rules and you are not posting good content but instead are at best acting as an aggregator (and then you’re competing with reddit). If you have an audience and a community, you can get support when you need it.
By Rococo Modem Basilisk on October 17, 2016.
An Alternate Medium Style Guide
An Alternate Medium Style Guide
I’ve seen several “medium style guides” promoting habits I find irritating. Here’s what you should do instead, if you (for whatever reason) want to attract readers like me.
By Rococo Modem Basilisk on October 19, 2016.
Web design over time has gotten less and less connected to accessibility & utility, and more and more connected to what might look good in a screenshot. Use of color is but one egregious example.
Of course, low-tech web pages with pure, unstyled HTML (and no formatting tags) continue to work just fine, and continue to load quickly. It seems like, as features become available, designers are using them to optimize for their own experience, not realizing that they should probably be optimizing for screen readers, twenty year old computers connecting over dialup, console-based web browsers without javascript support, and people who need to override styling with their own choice of typefaces, colors, and sizes.
There are real, accessibility-based reasons to make sure all of these things are configurable and the configuration sticks, but there shouldn’t need to be. When a web designer decides that his own sense of aesthetics matters more than the decisions of the user, a false division is created. The web designer should not be the master in this relationship; the web designer serves the user, and should bow to whatever any user prefers, to the extent possible. This is not the way things work now; instead, web designers act as arbiters of taste, with major consequences for large groups of people.
Most websites are essentially unusable for the blind or those with major vision problems — and for those of us with vision within the normal range, these websites are merely irritating. The average size of a website is approaching the size of the original shareware release of DOOM — in other words, if your machine is old, or your connection is slow, most sites are again unusable. A dyslexic person may want to set their font to one of the many fonts designed to change bilateral symmetry in order to improve the ease with which letters can be distinguished; CSS tricks are used to disable user-selected default fonts, and when user-selected fonts happen to show through, the sizing, layout, and behavior of the website is negatively affected because the designer has home-brewed a fragile system for implementing controls and layout rather than using standard widgets and sensible defaults.
A website is not an art project; a website is a piece of machinery that people accept into their lives, like an appliance. Just as nobody would accept a microwave that exploded if you tried to heat up pizza in it rather than baked potatoes, nobody should accept a website that ceases to function when the font is changed. Just as nobody would accept a stool that is nailed to the floor and cannot be repainted, nobody should accept a website that doesn’t respect a user’s color and size preferences. Just as nobody would accept a vaccum cleaner the size of an elephant, nobody should accept a website that takes ten megabytes to serve up 500 words of text.
A false idea of expert difficulty prevents people from demanding these things from the sites they visit. This idea is false not because modern web design isn’t complicated — modern web design is very complicated, and you really do need to be an expert to practice it. Instead, this idea is false because the only things in web design that are really difficult to do are things that should only be done rarely, if at all. The web is optimized for transmission of large chunks of minimally-styled text; using it to simulate native applications, while impressive, is a terrible idea and should never have become normalized.
By Rococo Modem Basilisk on October 19, 2016.
Ham sandwich for president
Putting Clinton in office is like giving a gun to a police officer: potentially dangerous & problematic in an array of well-understood ways, and best seen as a continuation and extension of a questionable standard.
Putting Trump in office is like giving a gun to a dementia patient: dangerous and unpredictable.
Putting Johnson or Stein in office is like giving a gun to a raccoon: a raccoon has no well-defined concept of what a gun is or does, and doesn’t have any thumbs, and so the level of danger is low but non-zero.
Putting Vermin Supreme in office is like giving a gun to a ham sandwich.
By Rococo Modem Basilisk on October 20, 2016.
There’s something strange going on with Russian intelligence & the internet security / civil liberties axis
Julian Assange & Wikileaks have been criticized for distributing pro-Trump Russian-generated propaganda by Edward Snowden. Snowden, despite receiving Russian asylum, has been pretty critical of Putin, even as Assange, whose asylum is provided by Equador, has strangely not been. What is going on?
Snowden and Assange are one of these political odd-couples: they have come to the same matrix of behaviors and assumptions from different political corners, and have ended up in the same precarious position by leaking important enough material to get people in positions of power to want to lock them up or kill them (even though ostensibly this is not the proximate cause of Assange’s asylum request). Snowden comes at this from the direction of the kind of right-reactionary libertarianism shared with Eric Raymond and Robert Heinlein — a distrust of government if and when it is the most obvious threat. Assange comes at this from the perspective of left-anarchism, filtered through the beginnings of cryptarchism that Assange lived through. While they have little in common outside a distrust of states and state secrets, they both have a kind of western-style civil-libertarian stance that’s pretty common but directly conflicts with Putin and his administration.
The only thing that all three of these figures (Assange, Snowden, and Putin) have in common is a deep familiarity with spycraft. Snowden was a contractor to two different american espionage agencies; Putin was in the KGB and brought KGB veterans and KGB tactics to his time in office; Assange, in addition to having a particular interest in exposing spycraft and the internal communications of espionage agencies, comes out of a culture of tech-savvy civil libertarians that since its inception in the clipper chip era has had an obsession with spies and espionage.
If there’s a figure that has even less to do with Assange and Snowden, it’s probably Trump. Putin might have an interest in a Trump presidency over a Clinton one only insomuch as the United States, if competently run, can be a major competitor to and impediment in Russian plans, particularly when such plans clash with those of international organizations in which the United States is a powerful member; Clinton, despite her flaws, is an effective politician and bureaucrat, while Trump is not. Perhaps Assange might dislike Clinton — after all, Clinton is of a piece with the current morally questionable state of governance — but Assange is not stupid and neither a North-Korea-like America nor America as a Russian puppet state aligns with his interests. Snowden has reason to worry about Clinton, considering that if he returned to the United States during a Clinton administration it is clear that she would not step in to save him, but Trump’s Nixonesque strongman stance doesn’t bode well for him either; no matter what happens in November, Snowden is unlikely to be able to return home during the next eight years.
The only explanation I can think of for the current strange behavior of these figures is that each has his own plan and believes himself to be using the others for it. Of course, in such a situation, all three plans will probably fail and all three figures merely add to the entropy of the social universe. Nevertheless, it’s very strange to see the civil libertarian axis of the infosec community take a hard right-hand turn and its anarchistic major figures show solidarity with secretive and famously corrupt authoritarians.
By Rococo Modem Basilisk on October 20, 2016.
With the procedural-OO hybrid, we see it applied in industry a little more often because it’s taught. That said, I think we often use more OO than is justified (I work for a large company and we have a large, essentially monolithic system for loading things from flat files into databases that’s megabytes upon megabytes of java code, most of which consists of less-than-100-line classes with deeply nested inheritance — to perform a task that would be more reliably performed by a single line shell script). When procedural or proper OO is the Right Thing (or, when a procedural-OO hybrid is the Right Thing), we have a leg up on the problem, but until recently most CS program graduates would have very little familiarity with functional or declarative languages.
The tasks I see at work are rarely good matches for OO: I process large streams of data, mostly, so pipes are the appropriate abstraction. The tasks I see in my personal projects sometimes require OO, and other times are a better match for a functional style; only the simplest pipe-component type tasks end up being something I’d want to use a purely procedural style with.
That said, we do the tasks we know how to do, and we solve them with the tools we’re familiar with. Very useful tools of the past have been totally forgotten by the industry: how often do you see a junior dev who knows what PROLOG is, or FORTH, or MUMPS, or tumbler indexing of enfilades? I’m very happy that certain useful tools are becoming much more common (asymmetric key encryption and hashing used outside of the context of communications security, for instance, and combining functional programming with implicit parallel execution), but when tools become subject to fashion everyone suffers.
By Rococo Modem Basilisk on October 21, 2016.
I don’t deny that there are social norms surrounding the elevation of bad design.
I don’t deny that there are social norms surrounding the elevation of bad design. Of course people who don’t have to eat their own dog food will have worthless and shallow opinions! But, there’s an ethical aspect to design: would you rather improve the world with good design or get paid more for bad design? There’s a didactic aspect to this as well: these semi-competent people in positions of power have the ideas they do because they have internalized trends that were promoted by actual designers or by people who have placed a heavy hand on the fashions of design; since warped ideas about what constitutes good design among designers caused these norms, corrections to these ideas (along with system-level changes like improved diversity and dogfooding) can correct these norms, albeit on something like a twenty year delay.
It takes resources to make a stand, so I can’t recommend everyone do so. But, if you’re a designer and you can afford to reject exceptionally bad norms, I recommend doing so. After all, accepting the awful state of design allows it to improve more slowly.
By Rococo Modem Basilisk on October 23, 2016.
The ideal chatbot is not a butler or a puppy, but an elder god
The ideal chatbot is not a butler or a puppy, but an elder god
The commercial industry popping up around chatbots worries me.
There have been chatbot communities for a long time. I’ve been involved with several. I love chatbots, and I love chatbot communities: they intersect with experimental writing, performance art, and ‘punk’ communities to a much greater extent than traditional AI communities, and constitute melting pots. Chatbots are interesting to these people because they are a tool for playing with language and identity in an interactive and public way.
Another kind of chatbot community has appeared, only in the past few years. This community bears more resemblance to that surrounding Hacker News than to the other chatbot communities. These are commerce-driven, mostly clueless pseudo-entrepreneurs who heard someone say “bot is the new app” and decided to start writing bots, without looking at the history of the form. As a result, the state of the art in commercial bots looks a lot like it did twenty years ago: it looks like AIML.
I don’t think that this is an accident. Instead, commercial pressures make it impossible to produce interesting bots. Entrepreneur-types talk a lot about “joy”, but when “joy” is only possible in service to commerce it’s necessarily limited. A commercial chatbot must be not only useful but more useful than alternate methods for performing the same tasks — and since most of these bots are essentially text-based front-ends for existing web services, the only way they can get close to the productivity of the existing services is to simulate an inflexible command line interface. A commercial chatbot can contain only pleasant surprises: it cannot confront us with challenging ideas, because challenging ideas are not profitable; since things that are pleasant to some of us are challenging to others, a commercial chatbot is limited in how many of us it is allowed to surprise at all. Being “smart as a puppy” or acting like a butler, in addition to bringing in questions about the culture of servitude that these representations build upon and taking advantage of questionable levels of surveillance in order to implement these features, limit the range of behaviors of the bot to the domain of “cupcake fascism” — leading to situations like Siri telling users not to swear when they request resources about dealing with sexual assult. A bot that is limited to being as conservative as its most conservative user will be little more than a censor in the way of easier-to-use services.
When bots don’t need to be useful in the normal case, that is when they become exceedingly useful in the exceptional case. Bots don’t need to reason the way humans do; bots lack the creative limitations implied by a human consciousness, and while this produces mostly noise, accidental signal has a special value when we find it. Bots, freed from caring about humans, can become alien and impart alien wisdom to us. Such bots can synthesize novelty from vast corpora — this is what bots are good at, and it doesn’t take much human intelligence on the part of a programmer to produce very striking results. Bots can implement dumb ideas endlessly, and by implementing them and making them concrete, change our perspectives.
By Rococo Modem Basilisk on October 24, 2016.
Yeah. (I haven’t recommended this for McDonalds before, because I think that the case for food is a lot more complicated, but I’ve definitely recommended refusing to write bad code to developers, by the same logic.)
By Rococo Modem Basilisk on October 24, 2016.
Mr. Rheingold, thank you for reposting all of these.
As an afficianado of computer history, I’m frequently surprised at how little things change, and how often history repeats itself. The field has an amazing institutional forgetfulness; people today are still trying to attack problems that were solved properly by NLS and Xanadu fifty years ago. Ignorance of the history of computer-mediated communication is all too common even among people who are focusing on it, and so we keep making the same mistakes.
By Rococo Modem Basilisk on October 25, 2016.
Does innovation have anything to do with commerce?
Does innovation have anything to do with commerce? Not any moreso than anything else. Innovative ideas are not necessarily commercially viable, and commercially viable ideas are rarely innovative; the space of commercially viable ideas is small, isolated, and nearly fully mined.
That said, I can’t agree at all with the idea that we live in a particularly innovative time. Most of the businesses that currently get marked as “innovative” are attempts to revive business plans that failed in 1999 (like Uber); the remainder are businesses that produce shoddy copies of the products of their technically superior competitors but make more money because they spend a bigger chunk of their budget on advertising to tell everyone how “innovative” they are than they spend on actual R&D (like Apple).
Genuine innovation cannot be easily productized, and as a result, it isn’t really compatible with consumerist capitalism. At the same time, it isn’t easy to mistake genuine innovation for the kind of imaginary pseudo-innovation that is produced by the con-men who dominate most industries. If you can’t tell the difference, you aren’t looking.
By Rococo Modem Basilisk on October 26, 2016.
There’s a difference between constructive cynicism and edgelordism.
There’s a difference between constructive cynicism and edgelordism. It’s one thing to have and promote a cynical view in the face of utopianism; to jump onto the tail end of an existing backlash is far less useful.
Black Mirror, while well-made, tends toward repeating media criticisms that were already cliche in the early 90s. When your complaint is twenty-five years old, repeating it without adding anything is of limited utility; Black Mirror doesn’t introduce new and interesting twists to its complaints, and it doesn’t reproduce anything *but* the complaint.
(Now, I don’t hate Black Mirror. It’s like rewatching The Twilight Zone: everything is predictable, but there’s really excellent atmosphere.)
By Rococo Modem Basilisk on October 26, 2016.
Mirai was appropriately named
Those of you who have been paying attention to the news already know that the major DNS outage last Friday is probably related to Mirai, a piece of malware whose source was released recently. You’ll also know that Mirai targets low-cost internet-connected embedded devices, and that it’s a comically incompetently written piece of code.
The idea that embedded devices would be vulnerable to attacks doesn’t even count as an open secret: the idea that major websites of the near future would be DDoSed by smart fridges was a cliche in some corners of the computer security world in 1999. Prior to around 2010, the dominant term for what we now call the Internet of Things was “ubiquitous computing” — a reference to Phillip K Dick’s novel Ubik, whose description of a group of appliances conspiring to extort and blackmail their human owner, now used as a parody of the “internet of things” concept, actually initially inspired it. The method that Mirai uses to get into these nodes is again an old one, familiar to war-dialers from the BBS era: Mirai iterates through a list of default username and password pairs until it gets a hit. Such lists are easy to find, and have been circulating on the internet since before it was called “the internet”.
In the early 90s, members of a hacking group called the L0pht made a public statement claiming that they could easily take down the internet, and unless security measures improve, someone with less impulse control would eventually do so. They weren’t bluffing; various methods of making the whole internet essentially unusable for long periods of time have been available for decades now, largely unpatched — it turns out that until now, few people have wanted to bother with such blunt instruments.
The general consensus in computer security that the human element is typically the weakest link isn’t without merit: in a competently secured system, humans are the most difficult element to lock down, and exploiting the human biocomputer requires less cleverness than exploiting a computer system. However, even when high stakes are involved, competence is not the norm: until recently, easily broken short PIN codes dominated online banking, and much banking infrastructure still relies upon things like account numbers and SSNs that conflate identification, authentication, and authorization; few systems implemented multi-factor authentication and nearly all systems will waive mutli-factor authentication in the face of a sufficiently convincing phone call; modern security practices have yet to penetrate industries like web dev and embedded systems development, where hardcoded authentication defaults, debugging backdoors, passwords stored as plaintext or unsalted hash, weak xor encryption against some arbitrary byte, and other awful behavior is tolerated or encouraged. We see from leaked documents that even the NSA is engaging in absolute idiocy, using Microsoft Word & its macro system for dealing with confidential documents and allowing those macros to contain commands that interact with the network.
Mirai, created by weebs, is full of in-jokes refering to chan culture and anime. Some people have taken this to mean there’s an association with Anonymous; however, the lesson of Mirai is that an association with Anonymous is totally unnecessary. The situation that Mirai takes advantage of is an old one; the only thing new about it is that even the dumbasses have realized that even a dumbass can take down the internet. The release of Mirai’s source has allowed script kiddies of an even lower skill level than Mirai’s authors to take advantage of the collective ignorance that end users have been allowed to partake in.
The name “mirai” was probably chosen because it sounds cool, but it’s very appropriate. “Mirai” means future, and Mirai is representative of our future: one where you don’t need to be 4chan to take down large chunks of the internet, but can get away with just being the junior high glee club.
By Rococo Modem Basilisk on October 27, 2016.
There’s a potential link to folie au deux here.
Most of what we as human being believe we accept as the result of social proof, rather than other kinds of proof. In other words, large portions of our mental model of the world are socially constructed: we accept things that our peer group accepts, even if they contradict our experience. Strange and delusional beliefs can easily take hold in social groups that are isolated from the greater population or have no impulse to share the same ontologies as the outside world (say, cults, conspiracy theory groups, and insular societies with enough power to avoid having to kotow to public opinion). The smaller the group, the easier it is to diverge further and further from consensus reality.
The halo effect also feeds into this. A group often feels the need to believe the opposite of whatever its opposing group believes, regardless of the relationship these predicates have to reality; a group will handicap itself in order to show group solidarity and such handicaps often come in the form of a ritual show of spite to some imagined outgroup, sometimes as a clearly absurd belief. The absurdity of the belief is proportional to the strength of group membership. Believing that Obama is a secret muslim is the right-wing equivalent of the Yakuza cutting off their own fingers: it’s a sacrifice that is capable of only symbolic utility but very concrete pain, and so the symbol gains strength from the pain.
By Rococo Modem Basilisk on October 28, 2016.
If you want a professional quality development machine, buy a 2008 Thinkpad on ebay (cost is about $80) and install Linux on it. It’ll remain perfectly good for the next decade, and perform all your development needs.
OSX is only marginally better for development than Windows is, and a professional should use neither, except for testing and packaging — and don’t you have access to build farms and test farms for that?
By Rococo Modem Basilisk on October 31, 2016.
Microsoft is no longer actively bad, the way it was under Ballmer.
Microsoft is no longer actively bad, the way it was under Ballmer. That doesn’t make it good, though. There’s forty years of dirty tricks that MS has to make up for just to get back to neutral, and it’ll probably take forty years, for those of us with long memories and a familiarity with history, to decide that this new direction is something more substantial than a PR move or a short-lived utopian optimism.
By Rococo Modem Basilisk on October 31, 2016.
Package dependency hell is *primarily* a function of unfamiliarity, and it’s not like it doesn’t exist on other platforms. Getting a functional development environment up and running on a Mac is just as hard as getting one up on Linux, so long as you’re OK with typing — the difficulties with homebrew are almost exactly the same because homebrew is basically just a unix-style package system! If you’re trying to get XCode to work properly and you’re used to vim, you’re going to have a lot more trouble with that than you would with installing a full development toolchain (minus IDE, because an IDE isn’t useful or necessary) on another unix.
Maybe you have a fat wallet and your idea of marginal value is different from mine, but I’d rather pay a few hundred dollars less and install my own unix on a machine that will continue to work if I drop it.
By Rococo Modem Basilisk on October 31, 2016.
This kind of thing has been Apple’s MO since 1982.
What I blame Steve Jobs for, more than anything else in the industry, is a strange and pervasive false minimalism where power and flexibility are removed in favor of one-off features. This starts with the original Macintosh.
The Macintosh team was originally working under Jef Raskin on what at the time was slated to be the Macintosh but eventually became the Swyft, and later the Canon Cat. It was a genuinely innovative design: a device that worked primarily as a word processor, but that had the capacity to perform programming operations in-place in the context of a document, like a combination of literate programming and spreadsheet macros. Steve Jobs, having had become obsessed with the Alto he saw demoed at PARC, had started the Lisa project, and when it became clear that Lisa was going to be a flop (it was expensive and underpowered, totally unusable without an aftermarket hard disk that cost as much as the computer, had no applications to speak of, and cost as much as a new car), Jobs got rid of Raskin and took over the Macintosh project, pushing it away from the original concept and towards being a lower-budget version of the Lisa. As a result, he pushed for lower specs: a last-generation CPU, not very much RAM, no support for TV output or external monitors, and a small monochrome (not greyscale) display. In the end, it debuted alongside the Commodore Amiga 1000 and the Atari ST, both of which had double the RAM, impressive high-resolution color display on standard color televisions, multitasking, and faster current-gen CPUs, at half or a quarter of the price. Somehow, the Macintosh sold better.
During development, Jobs was very wary of the ‘mistakes’ made with the Lisa, and so he forbade the developers from adding new features. Famously, he didn’t allow the hardware team to add any expansion ports; someone secretly added a single expansion port to the board design, however, and the original Macintosh shipped with a board set up such that an intrepid user could solder a connector onto the motherboard and get an expansion port (which couldn’t be used because the OS didn’t have any support for it). Removing expansion ports is, however, typical of Jobs’ attitude here, and it’s typical of his later work at Apple when he came back. (He was forced to resign over the failure of both the Lisa and the Macintosh, and took most of the Macintosh team over to NeXT, which doesn’t seem to have this particular problem somehow!) His idea was that the Macintosh was a computer for “regular people”, who he defined as people who would not only be incapable of using an expansion port but who would be incapable of learning what one is. In the end, plenty of cost on the Macintosh went into fancy beveling and other purely aesthetic aspects of the outer case, advertising, and other marketing concerns. The lesson was: a mediocre product, if it looks pretty and is advertised very well, can become the basis of a successful brand.
The lessons of the original Macintosh became a strange kind of warped “worse is better” ideology that affected Apple only occasionally during Jobs’ 17-year exile and almost constantly after his return. As soon as Jobs returned, he cancelled most ongoing projects, shifted focus from the pretty servicable beige boxes that represented macs in the mid-90s to the technicolor form-over-function blob of the first generation iMac, and ordered that floppy drives be excluded from all future devices (in 1998). Excluding floppy drives probably removed some cost, but margins for Apple products have always been huge in order to allow for big advertising budgets: owning Apple products, even in the early 90s, was as much a conspicuous display of wealth as it was a utility (outside of schools, which had deep discounts on Apple products as another marketing technique). With software developed under Jobs’ reign (like iMovie & iTunes), even the limited flexibility and configuration of earlier mac products mostly went away or became more limited: as part of the marketing push, Apple products were set up to be configurable in only limited ways pre-screened to be in line with the style approved by Apple’s designers, and Apple software was designed to work primarily with Apple-proprietary formats even when open formats existed and were superior (in a mirroring of Microsoft’s extend-embrace-exterminate strategy from the same time period). The unofficial slogan for Apple was “it just works”, with the hidden caveat that “it” was limited to a small set of tasks Apple had chosen to focus on — while Apple made it easy to do the very specific things they wanted people to do, doing anything else was much more difficult than on competing platforms. This kind of shallow marketing-first strategy was very successful, even though most people more or less recognized it.
This strategy continued (even though the UNIX base inherited from NeXT made it difficult to lock-down software extensibility on macs and systems like homebrew became common). The original iPhone wasn’t supposed to have third party apps; later, the app store was added but in a way existed as a way to funnel money into Apple moreso than as a way to actually add third party apps to the platform: Apple retained complete veto rights to apps, had long evaluation periods, required third party developers to pay $100 a year to even be allowed to submit their code, and forced these developers to write everything in a language that, while not strictly Apple-specific, is an obscure early C++ competitor that only every achieved any kind of traction at NeXT and later within Apple for compatibility reasons. Apple used this mechanism to remove: apps that criticized Apple, apps that competed with Apple’s own services, apps that violated Apple’s current design guidelines, and apps that were reviewed by somebody who was in a bad mood that day. The iPhone has no distinction between desktop and application drawer, and has no task-switching capability. The iPod, unlike the mp3 players it initially competed with, only worked with macs, only worked with iTunes, only worked under firewire, and had extremely limited controls and configurability; while some of these limitations were changed later, this was only done as a means of bringing this overpriced luxury object to the masses of people who still ran Windows. Apple laptops started the push toward being thin and light, and removed useful things like disk drives and expansion ports using the excuse that thinness and lightness were legitimate goals; of course, while laptops benefit from not being heavy, after a certain point being functional becomes more important than being lighter, and being thin only means the device is easier to break: Apple had essentially invented an excuse for charging more money the less they actually invested in their device, so that whenever they removed a feature or a piece of hardware they could add both the cost of the hardware and their manufactured percieved value of thinness to the cost of the end product. More recently, Apple has pushed for the removal of both the function keys and the headphone jack, as part of this general push towards smaller and lighter devices.
Now, this would be one thing if Apple could be ignored. My complaint isn’t just that Apple has the kind of abhorrent paternalistic attitude towards its users that used to justify the rush for Africa; after all, individual people and individual companies have all sorts of abhorrent attitudes and ideologies, and they largely don’t directly affect me because I can avoid putting money into those organizations. My big complaint about this is that other hardware and software companies, faced with Apple’s success, have tried to emulate their objectively awful hardware and software design decisions (thus perpetuating this strange design sense that users are children who need to be protected from choosing ugly color schemes by Big Daddy Ive), never realizing that the key to Apple’s success in everything since the end of the Apple II line was to produce mediocre products, sell them for many times their actual value, and spend enormous amounts of money on ad campaigns to convince people that mediocrity is amazing and that even really common things were secretly invented by Apple. In other words, Apple, since the early 80s (but especially since 1997), has been the Kim Jong Il of the tech industry, and their ideas are being gobbled up by lots of people who really should know better.
The existence of function keys on a new-model laptop is really sort of a non-issue in the scheme of things. Apple has been torpedoing third party development in much bigger ways for a long time. Function keys are only used for development by users of IDEs, and the intersection of IDE-using developers who also use brand-new Apple laptops is a group whose situation and opinions in the grand scheme of things shouldn’t matter much. These people are already shooting themselves in the foot paying four or five times what they should be for an ultimately less functional machine than what they’d get if they bought a used Thinkpad on ebay and stuck Linux on it; it’s not the end of the world if they also have to use a crappy mouse to navigate ill-designed XCode menus sometimes. Maybe these people are developing on a mac because they are trying to develop for the iPhone; in that case, their slower rate of development may hasten the death of that device as well, and it would probably be a net good.
Like a lot of people, when Steve Jobs died it made me hopeful. Lots of legitimately smart people work at Apple; maybe one of them would take charge and reverse some of the most damaging policies, the way that many of Microsoft’s worst policies changed after Ballmer got ousted. I expected that after a handful of things that were still in the pipeline from the Jobs era managed to get pushed through, we’d start to see some good decisions: laptops with thick protective cases, metal hinges, and locks to keep them closed; three-button mice; hardware that’s actually up to date; machines that ship with homebrew already installed; iPhones you can run Android on. But, I’ve lost that hope: I now suspect that Apple, like RIM and Oracle, will keep to its current course until it finally screws itself over enough to actually die — and, like Atari and IMSAI before it, will probably become a free-floating brand to be placed on arbitrary hardware based on whoever buys the rights.
By Rococo Modem Basilisk on October 31, 2016.
While I agree with the sentiment, I think you misplace the blame.
I understand the desire to have more readers. My problem with style guides is not this motivation. Instead, most style guides for medium focus on attracting casual readers and gaming the metrics in ways that actually produces inferior content. In other words, they assume the motivation in writing for medium is to get attention for its own sake & make those numbers go up, rather than clear communication.
For very short content, medium can’t easily distinguish between a ‘read’ and a ‘view’ anyway, so the metric hack of writing short articles is basically meaningless anyway. Likewise, misleading titles lead readers to be tricked & (often, unless the article is of very high quality) to feel tricked.
All of these techniques make sense in an ad-driven system where authors are being paid per-impression or per-read, but while Medium provides these statistics, it does not pay users (and paying publications on Medium typically pay a flat rate). Clickbaity dark-UX tactics are bad enough when they’re paying the bills; there’s no reason to apply them when there’s no money involved & all that’s happening is that readers are having their trust & good will abused.
As an author on Medium, I’d rather write a meaningful essay that takes 2 hours to read and see it get four readers and ten views than write a tweet-length piece of content buffetted by images and affiliate links and get ten thousand reads. As a reader, I’d rather read something substantial than something anemic: Medium’s user interface isn’t optimized for short articles the way Twitter’s is, and every one-paragraph article is a waste of my time and effort, particularly when it doesn’t contain any new information.
By Rococo Modem Basilisk on October 31, 2016.
As a follow-up, I’d like to reiterate that, as Charlie Stross says, “bigotry is fractal”.
As a follow-up, I’d like to reiterate that, as Charlie Stross says, “bigotry is fractal”. Power relations are complicated because they contain their own inversions (and may run in complicated loops in certain communities), so what constitutes punching “up” versus “down” is often unclear and very environment-dependent.
Satire at its best highlights through emphasis behaviors and systems that are malfunctioning and, if eliminated or repaired, would be a net positive. For instance, consider satires of government corruption in democratic republics: the power relationships are complicated since while elected officials are often wealthier and have more direct power than their average constituent, they are at least theoretically under the thumb of those constituents & both the pressure to remain electable and various anti-corruption restrictions provide the impetus behind corruption in the first place; something like Yes, Prime Minister, by focusing on government, highlights a set of very strange circumstances that can force someone like Prime Minister Hacker to behave in the way we see actual officials behave despite his best efforts, and mostly criticizes the system. Ultimately, every character in that show is flawed, but the person with the greatest claim to ostensible power is the person we in the audience identify most with, because while he is vain and conniving and foolish, he is also naive, optimistic, and genuinely has the best interests of his constituents at heart after a fashion, and each episode shows how his tiniest motions toward exercising his power get quashed beneath the great weight of a government system designed to preserve the status quo. The humor comes from seeing one of the most powerful men in the world reduced to a petulant child by a dysfunctional system and empathizing with him.
By Rococo Modem Basilisk on November 2, 2016.
Books that should be made into movies, but never ever will
Books that should be made into movies, but never ever will
We Can Build You by Philip K. Dick
Summary: A company that makes electronic instruments (something like a cross between a Mini-Moog and a Melotron, based on the description) decides to branch out into animatronics, and for an anniversary of the civil war, decides to produce autonomous androids designed to look and act like Lincoln and his secretary of state. Our protagonist, a salesman for this company, falls in love with the artist hired to build the faces; the artist starts off being represented as a manic pixie dream girl. (She sleeps with him, decides she doesn’t like him much, and disappears; he doesn’t get the hint.) Since these androids are autonomous and trained on the writings of the figures (along with records of their habits), they have no idea that they are robots, and they proceed to act as though they have been transported through time, leading to a plot where the salesman and robo-Lincoln go on a cross-country road trip looking for the robot version of the Secretary of State, during which robo-Lincoln tries and fails to give romantic advice. In the last scene, our protagonist is drunk in a bar with robo-Lincoln, coming to terms with the fact that he was dumped, while robo-Lincoln sinks into a deep depression and becomes essentially catatonic.
Why it should be made: This would make an excellent counterpoint to modern rom-com fare like Scott Pilgrim, in that it does a good job of subverting the manic pixie dream girl progression: an artistic and damaged woman ends up rejecting the protagonist who is obsessed with her, and that result sticks. By having animatronics with just enough AI to be unpredictable, this ties in thematically with the Westworld franchise, which of course has recently been rebooted to some acclaim. Lincoln biopics had a sudden popularity a few years ago, as well. But, the most interesting part about this story is that it doesn’t do what pretty much every other story about AI does: it never bothers to touch upon the idea of whether or not these machines are “really conscious”. The machines are clearly machines, because our protagonist’s friends built them, and the story doesn’t make them out to be particularly advanced or clever; at the same time, they act like people and are therefore treated like people by our protagonists. When robo-Lincoln goes into a deep depression, nobody questions whether or not the depression is “real”, because of course it’s real: he’s so sad that he can barely move. It’s a book that substitutes the turing test for the eliza effect, and succeeds.
Why it will never get made: Hollywood is really fixated on removing the lumps from PKD adaptations. Outside of the original Total Recall & a couple scenes from Minority Report, PKD adaptations basically reach for a streamlined hollywood ideal of what twelve year olds in 1995 would consider a mind-blowing sci-fi movie. This ignores the kind of fuzzy weirdness PKD embraced in his writing, and which characterized much of the draw of We Can Build You. If you took this and removed the lumps, you’d get a really uninteresting result. For proof of this, compare Do Androids Dream of Electric Sheep (the book written immediately before We Can Build You, and one that is in many ways inferior) to its loose adaptation Blade Runner (which, while iconic for its cinematography and sound design, has removed so many lumps that it’s pretty much the closest thing to a science fiction cliche since Fritz Lang’s Metropolis).
Gun, With Occasional Music by Jonathan Lethem
Summary: A hardboiled detective story in a strange science fiction universe. In a world where the government supplies citizens with memory-loss drugs, uplifted animals form a servant underclass, and police enforce politeness with social credit chips, a private investigator tries to solve a brutal murder.
Why it should be made: A good adaptation will introduce audiences to the true power of science fiction to play with novel ideas. The plot proper is nothing special: it’s frosting on top of the world-building, but because it’s so cliche, it does a great job of leading readers through this world. A proper adaptation would be visually arresting and weird: uplifted animals retain their usual size but talk and walk on two legs; the same technology is being used on the infants of the wealthy, who develop into perpetually drunken and misanthropic superintelligent infants with oversized heads and a pathological inability to avoid making puns. Imagine if Jupiter Rising was actually twice as smart as it thought it was instead of half as smart, and you’re imagining a Gun With Occasional Music adaptation.
Why it will never be made: It’s unclassifiable. The only way we’d ever see this done justice is if Terry Gilliam or Don Coscarelli directed it; even then, with or without producer intervention, it’s an even bet whether this would end up being great (a la Brazil) or a top-heavy flop (a la Jupiter Rising). A lesser director would be tempted to try to play it straight and tone down the lumps. Unlike with We Can Build You, where the lumps are few and big and integral to the plot, Gun With Occasional Music has a million tiny lumps and an easily separable plot proper, but the only reason to bother with it is the lumps.
Snow Crash by Neal Stephenson
Summary: In an anarchocapitalist future where nation states have been replaced with franchises, a cable magnate tries to reintroduce an alien mind-virus to a swarm of refugees, and a master-swordsman journalist races against time to prevent a whale-riding knife-wielding sociopath from distributing it all throughout virtual reality.
Why it should be made: Snow Crash mixes big ideas with the most interesting excesses of VHS-trash spectacle. A proper adaptation would do for the 90s what Quentin Tarantino did for the 70s.
Why it will never be made: Snow Crash deals heavily in exploring weird political and racial dynamics, often explicitly. It uses the conflation of nationality with franchise preference to compare and contrast ethnic identity with branding. To do this, it engages in and plays with racial stereotypes. This is hard to do well, and risky even when it is; movie audiences will pretty much always contain somebody who takes things at face value, and with so much else going on, we’d probably see the same kind of missed-the-point fandom around a Snow Crash movie as we do around Scarface, Fight Club, Watchmen, Goodfellas, and Robocop.
By Rococo Modem Basilisk on November 2, 2016.
Other things happening in November:
• NaNoGenMo: write a program to procedurally generate a novel in a month. • NaBoMaMo: create 31 twitter bots in November. • ProcJam: write a game using procedural generation in November.
By Rococo Modem Basilisk on November 2, 2016.
The Lazarus Pose
The Lazarus Pose
February 23, 2025
Two weeks ago, when the first manned ship to Mars exploded, even if nation states wanted to cover it up, such a thing would be impossible. The scale and suddenness of this event exposed the the internet cranks’ claims of international conspiracy as wishful thinking. Much speculation about the evidence caught on radar and sattelite imagery has occurred, but to my knowledge, I am the only one to directly investigate the black squares.
Let me first reiterate the obvious: the Heart of Gold was the largest, fastest space vehicle humans have ever built by a large margin, and took a great deal of time and capital. It was one month into the three month trip. The things that intercepted it were of comparable size, and the time from when the squares in Siberia and the Outback first appeared and impact was only twenty minutes. This was not part of the abandoned Soviet Dead Hand interception system; the Soviets could not have built a device this size.
I got lucky: I was on Twitter when the news hit. I immediately booked a red-eye flight to Siberia; after all, the Australian square would be much harder to travel to. I bought every drone for sale in the airport mall.
The Siberian square, like the Australian one, was about a mile across. When it opened, it caused a building above it to collapse. I imposed upon the landlord of this building; luckily, there were no tenants at the time, though the landlord suspects that some squatters may have been lost.
With a hole a mile across, it’s reasonable to expect quite a bit of depth. The scale of the things launched from here gave me a bit of an estimate: at least 800 feet. I modified the drone firmware so that each drone could act as a signal repeater for the next one in the chain; I also added a ping function as a means of estimating depth, although with slow microcontrollers like those in the drones there’s a margin of error of about 2%.
My daisy chain of drones demonstrated that the square extended straight down about 200 feet, with some indication of shear stress on the edges for the first 25 feet below the rockhead. After 200 feet, the space opened up: my drone’s cameras, radar, and sonar all couldn’t find edges. There wasn’t much to see, other than a lot of dust. About a hundred feet lower, some debris (presumably from the collapsed buildings) sat atop a blunt cone made of a material that resembles smooth concrete. I had my lead drone land on the cone to get a closer look.
From the perspective of the tip of the cone, I could see a vast grid of similar cones in all directions. However, the lead cone (and others) began to move; tremors made the building I was staying in become unstable, and I had to flee. Unfortunately, my recordings were lost when the building collapsed on top of my computer, during the square’s closure.
From public sattelite imagery, it seems that the other square closed during the same two hour window; I suspect that it actually closed simultaneously.
With regard to the mystery of the squares and the tragic demise of our Martian colonists, it is my position that history has repeated itself. The Soviet dead hand system accidentally recapitulated, in some small way, an antedeluvian drama that once occurred between Earth and Mars, long before the age of man. We have finally reached the level of technological development necessary to become pitted against our forebears, who while not human were also people of Earth. However, their defenses are far beyond what we can reasonably wish to escape. How long will we be trapped on this planet by the nervous twitches of a long-dead race? The thousands of cyclopean missiles beneath the Siberian tundra are our formidable jailers, along with similar stockpiles who knows where else. We cannot leave the cradle of earth until we outwit them; but, even something as simple as the door mechanism for this defense system is centuries beyond our technology, and the very existence of a race who could build such things is beyond the current reach of our archaeology.
Who were the figures in this ancient drama? We have been presented with a mystery whose clues will be inaccessible for the forseeable future.
By Rococo Modem Basilisk on November 3, 2016.
Yeah. It’s useful as a rule of thumb for who you should probably be extra careful not to mock for no good reason, but satire falls into the category of legitimate criticism layered in humor. Nobody should be immune to legitimate criticism, and nobody should be mocked for no good reason, so as a rule it’s only useful when you’re already failing to follow other, more important rules.
I don’t think the idea of calculating punch vectors — i.e., trying to figure out power relationships — is not worthwhile, though. Systematic inequality tends to be easy to ignore, so encouraging people to look for it is important. But the thing about legitimate criticism is that it’s not an attack: the target of criticism benefits from understanding and accepting it, by definition. In other words, calculating power relationships should probably not be a part of determining who to criticize (although it may be part of determining how to criticize them), because criticism is a positive-sum game.
By Rococo Modem Basilisk on November 5, 2016.
Early attempts at implementing transcopyright (essentially, a system by which quote attribution is treated as a mechanism of remixing & is built-into the editing and publishing system with fees, the idea being that long discussion threads could become a kind of economy where tiny amounts of money are transferred) such as TokenWord failed because there was no community, and other systems that attempted to adopt some of the ideas of transcopyright didn’t go all-in and thus ended up falling back on their alternative revenue systems in lieu of actually explaining the mechanics of transcopyright to users. However, Medium seems like an almost ideal environment for a transcopyright-type system: long conversation threads emerge around long original posts, most posts aspire to something resembling “journalism proper” rather than the kind of blog-like or social-media-like content that dominates on Tumblr or similar, and many old-school publications have moved to Medium and integrated with its systems. Not only that, but Medium already has a licensing menu & complicated licensing mechanisms. In other words, if Medium were to build a mechanism for pasting highlights into a new post while making them link to the original context & hook that system into a default license that explicitly allows these highlighted sections to be put into a new post in exchange for redeemable credits, Medium would immediately have a transcopyright-style system (although they would need to hook into some sort of payment framework in order to allow people to put money in and take it out; probably, they’d have to set aside a bit of money for people to start off with, so that users wouldn’t need to pay something in first).
Ads don’t pay much at all. Any replacement for ad-supported systems would need to let people pay as much as ads would, and were that payment truly automatic many people would: we’re talking a fraction of a cent for hundreds of views. (I subscribe to Google Contributor, which basically turns Google’s own ad system into a micropayment system: I rarely see ads, but every site gets the kind of money it would if I saw every ad. The problem is that for certain ads, Contributor is disabled, and these are the most irritating ones, so I still have to use ad blockers on desktop — only sites I view on my phone end up going through Contributor in the end. However, if Google were to charge me $4 instead of $3 and properly block *all* the ads, I’d happily turn off ad blocker and pay the money.) The ad ecosystem wastes money by involving a bunch of third parties; direct automatic micropayments to all involved web hosts is better, because anybody calculating how much showing me an ad is worth to them is drastically overestimating: I’m never going to buy something by clicking through a pop-up ad because it’s insecure, so they’re never going to make that fraction of a cent off me that they paid the web host, and as soon as they realize how many people out there think the same way, ad impressions will be worth even less.
By Rococo Modem Basilisk on November 7, 2016.
I’ve never believed that clickbait had anything to do with audience preferences.
I’ve never believed that clickbait had anything to do with audience preferences. The popularity of longreads shows that people would well and truly prefer to read proper, in-depth journalism.
The thing is, low-content reporting (short listicles that are mostly images, 500-word “articles” that are mostly pull quotes from other people’s coverage, reposted press releases) is super cheap and easily outsourced to content farms. If you’re being paid by the click and you can’t easily maximize the clicks (because you’re in a red queen’s race) then you can at least make sure you’re spending as little money as possible on each ad-hosting page of “content”.
By Rococo Modem Basilisk on November 7, 2016.
On communities
An extension & refinement of the MOP theory.
The function of any given community is social noise reduction: a community allows groups of people with certain sets of attributes to access each other with fewer “misses” — i.e., without accidentally interacting with a person lacking any of these attributes. (Whether or not this is a good thing depends pretty heavily on the circumstances; however, ultimately, a country club, a Bilderberg conference, a birdwatching forum, and a hackerspace all have in common this basic definition.) Gatekeeping is therefore a huge part of the function of the community.
In a sense, the gatekeeping rules of any community define that community. After all, gatekeeping rules ideally act as a test to ensure that members approximate the ideal community member, but functionally, gatekeeping rules specify who gets into the community and therefore what the range of attributes are of community members. Gatekeeping rules are rarely explicit, and many are inherently implicit: after all, community norms, tolerance for particular community members, and desire to be part of the community all have major gatekeeping ramifications despite never being written or systematically enforced.
Communities are not the result of people coming together so much as they are the result of people seeking a temporary separation by some arbitrary category. They happen naturally, as people congregate toward other people who can fulfill needs and desires. When a community becomes diluted such that different sub-groups within it have different needs, it fractures into multiple communities, whether or not anybody recognizes or points this out for any particular group. Community fracture is always present and fractal to some degree: cliques appear within any community among people who get along better together, and this is part of the community fracture gradient and caused by the same impulse toward sorting. However, all of these organizations are temporary and desire-driven: someone who ceases to be interested in hot rods has left the hot rod community, and communities don’t organize along racial lines in the absence of racial tension. The rich and powerful form cliques among themselves because they share attributes that are only accessible to them (nasty gossip about foreign dignitaries isn’t available to people who aren’t presidents; discussion about how to prevent your kids from murdering you for your inheritance isn’t useful for people who have no estate), and pooling power is a side effect rather than a goal.
Prior to complete fracture, a cultural change within a community changes the community’s ideals. When somebody leaves a community and calls it “dead” — it’s not dead, but it no longer serves the purpose that person needed it for. Communities are tools for solving particular social/interpersonal problems (usually, a desire to communicate about a particular subject), and when community norms change, it’s as though someone’s screwdriver has been replaced with a hammer.
The MOP theory explains a very specific special case of this phenomenon — one that happens pretty frequently, particularly in “geek” circles. Specifically, it explains what happens when an affinity group organized around a particular subject becomes subverted for the sake of commercial interests. (When we talk about this in terms of Punk, or Star Wars, or comic books, we’re really talking about a set of established commercial interests that had already taken over partial control of a social group being subverted by another, larger and more powerful set of commercial interests with different norms.) Whether or not this is a good thing basically comes down to which norms align more closely with the ones you value. While typically the group that takes over has values closer to the notional “mainstream”, it’s not as though communities haven’t been taken over by commercial interests that are even more fringe — as with Palmer-Luckey-funded alt-right trolls infiltrating 4chan. That said, the typical pattern is as mentioned in the MOP theory: a community with a focus on detail and quality has its gatekeeping process subverted by a second group whose focus is on commerce, in such a way that an influx of less-dedicated members become involved; the average dedication level of the group plummets while the most dedicated group members leave the community to form their own. Without the most dedicated members, the infrastructure involved in gatekeeping and in keeping conversations going disappears, leading to most of the casual members also leaving, but enough money has been extracted from the group by commerce-centric outsiders to make this tactic a success.
When a community that has been infiltrated becomes self-sustaining (usually because the new gatekeeping mechanism is sufficiently exclusive that most members still find something valuable in each other’s company), it’s essentially a new community with new norms: obviously most of the old guard will find this new community less to their liking than the old one, because the old community was based more closely on their own desires and values. It also becomes vulnerable to being infiltrated, split, or subverted by some other commerce-centric group. Whenever a commerce-centric group infiltrates a detail-centric group, the group norms become more lax, because commerce works best at scale and details work best with strong gatekeeping.
While toxic community norms (as mentioned on the Status 451 piece) can become part of gatekeeping, they are rarely truly valuable as such. Communities with toxic norms can become stable so long as they consist primarily of people who can’t easily split off. Having been a long-time member of several autism-related internet communities, I can verify that schism doesn’t take a huge amount of emotional intelligence or social capital; more typically, toxic norms dominate in groups where confidence in one’s ability to split off is low & desire to avoid a toxic community is low — in other words, toxic communities are a result of learned helplesness, not a calculated tactic.
By Rococo Modem Basilisk on November 7, 2016.
Stories I won’t write (but you can)
Stories I won’t write (but you can)
This is a list of pitches for stories I won’t write. You absolutely may write any of these. The full list (which gets updated occasionally) is here.
By Rococo Modem Basilisk on November 11, 2016.
Clearly, the only thing that can defeat Trump now is the veneration by chaotes of an even older deity. (I’m thinking maybe Innana, patron goddess of love, war, prophecy, the impossible, and fashion.)
By Rococo Modem Basilisk on November 14, 2016.
The idea that major religious figures are ambassadors from outer space goes back at least to Theosophy, with its idea of “secret chiefs”; Theosophy was extremely influential, particularly on the ideas of other syncretic religious groups, so it’s not a surprise that the Aetherius society has adopted this idea (just as the Raelians have). It’s also not surprising that it made its way into pop culture, since Theosophy was basically the Spiritualism of the 1920s: a minor religion with very influential followers that captured the zeitgeist & thus had an overwhelming effect on popular discourse.
By Rococo Modem Basilisk on November 21, 2016.
News and Lies 1:
News and Lies 1:
In Defense of (some) Propaganda
[1]
There’s been a lot of blow-back regarding fake news — which is to say, fiction using the style of news stories and disguised as news stories — since Trump’s election. The general premise is that false news stories circulated among the communities dominated by Trump supporters bolstered their support of him, and of course social media was leveraged to an almost unprecedented degree by his campaign.
“Our biggest incubator that allowed us to generate that money was Facebook,” says Parscale, who has been working for the campaign since before Trump officially announced his candidacy a year and a half ago. Over the course of the election cycle, Trump’s campaign funneled $90 million to Parscale’s San Antonio-based firm, most of which went toward digital advertising. And Parscale says more of that ad money went to Facebook than to any other platform.
“Facebook and Twitter were the reason we won this thing,” he says. “Twitter for Mr. Trump. And Facebook for fundraising.”
At the same time, even politically-charged fake news has a powerful capability to aid us in the pursuit of truth. This seems paradoxical but isn’t necessarily so, as I’ll explore.
We should not ignore this history, but we should analyse what makes the standard discordian fake news different from the stuff that people are currently concerned about.
I should clarify that I will not be focusing on the distinction between satirical fake news and fake news made to be believed. Satire is of great value — there’s not just a cultural or propaganda value to satire but also a concrete procedural value to it — but we don’t solve all our problems by clearly indicating that certain stories or sites are fictional or satirical, and there’s a lot of good that can come out of fake news that isn’t clearly satire. (Furthermore, this doesn’t move us away from the problem of people sharing it uncritically and believing it, as anyone whose grandmother has forwarded them chain emails about Onion stories can attest.)
The history of using fake news for explicitly political purposes goes back a long way, but the current state of the art in this domain can probably be attributed to Paul Linebarger’s book Psychological Warfare — a description of US propaganda activities during the second world war, used as a training manual for the US army.
Linebarger’s advice is fairly straightforward, and forms the basis of what outlets like RT and Breitbart do: take true statements out of context, mix in fiction that the target audience either wants to believe or wants to fear, and construct the story in such a way that the target audience is led to a particular conclusion. Linebarger gives examples of descriptions of the good treatment of POWs — an enticement for soldiers, unhappy in the field, to surrender — and other stories suggesting that a group of soldiers allies are sex-crazed or have abnormal sexual prowess — an enticement for soldiers to desert their posts in order to safeguard their wives and girlfriends at home, and an encouragement to further distrust wartime allies whose history with one’s culture is more complicated.
The goal of this kind of work, which we might call propaganda news stories, is for the target to actually believe the stories and come to the conclusions suggested. Linebarger suggests that the goal of such stories should be to convince enemy soldiers to surrender or desert their posts, so that war can be ended with a minimum of bloodshed, and that to bolster the effectiveness of this kind of propaganda, all of the things that can benefit the enemy soldiers once they follow the suggestions provided by these stories should be made true — in other words, one should suggest that POWs are well treated and then actually treat the POWs well, even if the POWs are treated better than the neighbouring citizens.
There’s another form of fake news, of about the same vintage, that would specifically be coming out of the intelligence community: the double cross. Knowing that a party is listening but skeptical, one can produce fake news or fake documents that are a mix of truth and fiction produced in such a way that the end goal is to produce confusion and greater skepticism — to waste the time of the opposition. A good example is the idea that carrots promote night vision — which was, in fact, a story that was circulated locally in Britain to simultaneously hide the existence of advances in radar and radar detection systems and push the Axis powers into a series of ill-fated and expensive attempts to improve soldier eyesight with dietary changes. Another good example is “red mercury”, a fictional chemical that was mentioned in leaked nuclear weapon plans — these plans looked fairly convincing, but anyone working off them would spend a great deal of time looking for the imaginary red mercury, resulting in a delay in nuclear weapon development.
Linebarger’s propaganda approach is fairly straightforward: if people believe the news story, they can be controlled; if they don’t, they can’t be controlled. The trick is to compose stories that people want to believe — and it’s no trick at all, if you have a sufficiently nuanced understanding of the culture and history of the target group. While positive uses are possible here — the historical use of these stories to convince Nazis to surrender or go AWOL would be seen as positive by most — this tool is basically totally dependent upon the intent of its wielder, and it can be turned against anybody in a rather uninteresting way.
The double-cross is more interesting, because it deals directly with the idea of a nuanced, skeptical, sophisticated audience. It’s less predictable in its concrete results, but an environment saturated in double-cross media is an environment that is extremely resistant to Linebarger-style propaganda. Anybody with a vested interest in determining truth, when dealing persistently with a consistently yet not systematically unreliable source of data, will develop a habit of skepticism and reflexive self-awareness that borders on paranoia, and such a habit is extremely useful when actually isolating truth from fiction.
There’s a third kind of fake news, which I’ll call the single-source double-cross. This is the kind of propaganda produced by long-running state media that are required to toe the party-line — think Pravda, or North Korean press releases. Like Linebarger-style propaganda, it has a clear goal which can be trivially determined by the target population if they’re inclined to look critically. Like the double-cross, it is created with an awareness that nobody will take it at face value.
However, the major difference is that it comes without opposition — anything it says, true or not, will be backed up by any other media released, because all of the media are required to adhere to the same set of rules. Knowing that what the media says is in some places false, but at the same time having no alternative view provided, we cannot produce a consistent view other than the official one and risk engaging in a kind of learned helplessness.
Everyone in Russia in the early 1980s knew that the managers and technocrats in charge of the economy were using that absurdity to loot the system and enrich themselves. The politicians were unable to do anything because they were in the thrall of the economic theory, and thus of the corrupt technocrats. And above all no-one in the political class could imagine any alternative future.
In the face of this most Soviet people turned away from politics and any form of engagement with society and lived day by day in a world that they knew was absurd, trapped by the lack of a vision of any other way. — “THE YEARS OF STAGNATION AND THE POODLES OF POWER,” Adam Curtis
Some of the coverage of filter bubbles with regard to fake news has focused on how self-segregation has allowed Linebarger-style propaganda to become extremely effective in certain communities, which is true.
Other coverage instead focuses on the idea of “post-truth” politics, which is essentially a combination of the double-cross and the single-source double-cross: when we self-segregate, we can turn the regular double-cross into a single-source double-cross by only sharing stories we find agreeable, even if we know that they aren’t true. It’s important to note that the filter bubble isn’t some recent invention, or some conspiracy by tech companies. When we don’t think clearly and critically, we have a tendency to ignore information we don’t like and hoard information we do. When we’re provided with the ability to share a stream of information of unknown veracity with a group of peers — chosen via some kind of semi-mutual consent — we self-segregate based on who shares information they like.
But, where a double-cross media landscape promotes a sophisticated, skeptical, cosmopolitan approach, the aggressive filtering of dissent provided by social feedback producing a single-source double-cross landscape promotes a nihilistic approach: “everything is fake, and I don’t know what to believe, so I’m not even going to try to learn anything”.
In other words, if you expose yourself to a media mix that is both appealing and unappealing, apparently true and false, you’ll end up becoming a more effective and discerning thinker, while if you filter that same spew by appealingness you will end up a less effective and less discerning thinker.
People like Joey Skaggs — and others who take a discordian-inspired culture-jamming approach — are attempting to improve critical thinking via fake news. For instance, Cat House for Dogs,
The phone rang off the hook as hundreds of people called to talk to New York’s first and only dog pimp. Surprisingly, not only were the calls from bonifide customers willing to pay $50, but there were just as many calls from people who wanted to have sex with dogs or watch dogs have sex with other people. Dog pimp, Skaggs, recorded all of these incoming phone calls.
When contacted by the news media, Skaggs got together 25 actors and 15 dogs and staged an elaborate performance in a SoHo loft — a night in a bordello for dogs. The performance featured models posing with female dogs in look-a-like outfits, and actors posing with the male dogs waiting to view the bitches. Friend Tony Barsha played a bogus veterinarian on site who, when interviewed, explained that the female dogs were injected with a drug called Estro-dial to artificially induce a state of heat. If a bitch was already in a natural state of heat, she would be given a contraceptive called Ova-ban, so there would be no fear of fatherhood.
The intent here is clearly different than those who want to use fake news to manipulate you into voting a certain way, or people who want to wear you down to the point where you’re unwilling to even complain.
Where Linebarger-style propaganda tells you what to believe and satire tells you that other people’s beliefs are silly, culture jamming of this type encourages you to question your beliefs by making you consider believing something silly. And in terms of ill intent, the worst that can be accused if such efforts is that they cast shade on the legitimacy of press outlets that carry the story without being in on the joke. There’s no harm to be rendered by that in light of the Post Factual, and plenty to gain.
Surrealism is the key. Surrealism will shock your mind of its track. Surrealism can shut your mind down for a fraction of a second, allowing you to experience the world for just a moment uncensored. — Operation Mindfuck
See also:
News and Lies 2: On Post Truth
We Can Weaponize Fiction, But How Do We Monetize Truth?
BladeRunner and the Synthetic Panopticon
By Rococo Modem Basilisk on November 23, 2016.
News and Lies 1: In Defense of (some) Propaganda
[1]
There’s been a lot of blow-back regarding fake news — which is to say, fiction using the style of news stories and disguised as news stories — since Trump’s election. The general premise is that false news stories circulated among the communities dominated by Trump supporters bolstered their support of him, and of course social media was leveraged to an almost unprecedented degree by his campaign.
“Our biggest incubator that allowed us to generate that money was Facebook,” says Parscale, who has been working for the campaign since before Trump officially announced his candidacy a year and a half ago. Over the course of the election cycle, Trump’s campaign funneled $90 million to Parscale’s San Antonio-based firm, most of which went toward digital advertising. And Parscale says more of that ad money went to Facebook than to any other platform.
“Facebook and Twitter were the reason we won this thing,” he says. “Twitter for Mr. Trump. And Facebook for fundraising.”
At the same time, even politically-charged fake news has a powerful capability to aid us in the pursuit of truth. This seems paradoxical but isn’t necessarily so, as I’ll explore.
We should not ignore this history, but we should analyse what makes the standard discordian fake news different from the stuff that people are currently concerned about.
I should clarify that I will not be focusing on the distinction between satirical fake news and fake news made to be believed. Satire is of great value — there’s not just a cultural or propaganda value to satire but also a concrete procedural value to it — but we don’t solve all our problems by clearly indicating that certain stories or sites are fictional or satirical, and there’s a lot of good that can come out of fake news that isn’t clearly satire. (Furthermore, this doesn’t move us away from the problem of people sharing it uncritically and believing it, as anyone whose grandmother has forwarded them chain emails about Onion stories can attest.)
The history of using fake news for explicitly political purposes goes back a long way, but the current state of the art in this domain can probably be attributed to Paul Linebarger’s book Psychological Warfare — a description of US propaganda activities during the second world war, used as a training manual for the US army.
Linebarger’s advice is fairly straightforward, and forms the basis of what outlets like RT and Breitbart do: take true statements out of context, mix in fiction that the target audience either wants to believe or wants to fear, and construct the story in such a way that the target audience is led to a particular conclusion. Linebarger gives examples of descriptions of the good treatment of POWs — an enticement for soldiers, unhappy in the field, to surrender — and other stories suggesting that a group of soldiers allies are sex-crazed or have abnormal sexual prowess — an enticement for soldiers to desert their posts in order to safeguard their wives and girlfriends at home, and an encouragement to further distrust wartime allies whose history with one’s culture is more complicated.
The goal of this kind of work, which we might call propaganda news stories, is for the target to actually believe the stories and come to the conclusions suggested. Linebarger suggests that the goal of such stories should be to convince enemy soldiers to surrender or desert their posts, so that war can be ended with a minimum of bloodshed, and that to bolster the effectiveness of this kind of propaganda, all of the things that can benefit the enemy soldiers once they follow the suggestions provided by these stories should be made true — in other words, one should suggest that POWs are well treated and then actually treat the POWs well, even if the POWs are treated better than the neighbouring citizens.
There’s another form of fake news, of about the same vintage, that would specifically be coming out of the intelligence community: the double cross. Knowing that a party is listening but skeptical, one can produce fake news or fake documents that are a mix of truth and fiction produced in such a way that the end goal is to produce confusion and greater skepticism — to waste the time of the opposition. A good example is the idea that carrots promote night vision — which was, in fact, a story that was circulated locally in Britain to simultaneously hide the existence of advances in radar and radar detection systems and push the Axis powers into a series of ill-fated and expensive attempts to improve soldier eyesight with dietary changes. Another good example is “red mercury”, a fictional chemical that was mentioned in leaked nuclear weapon plans — these plans looked fairly convincing, but anyone working off them would spend a great deal of time looking for the imaginary red mercury, resulting in a delay in nuclear weapon development.
Linebarger’s propaganda approach is fairly straightforward: if people believe the news story, they can be controlled; if they don’t, they can’t be controlled. The trick is to compose stories that people want to believe — and it’s no trick at all, if you have a sufficiently nuanced understanding of the culture and history of the target group. While positive uses are possible here — the historical use of these stories to convince Nazis to surrender or go AWOL would be seen as positive by most — this tool is basically totally dependent upon the intent of its wielder, and it can be turned against anybody in a rather uninteresting way.
The double-cross is more interesting, because it deals directly with the idea of a nuanced, skeptical, sophisticated audience. It’s less predictable in its concrete results, but an environment saturated in double-cross media is an environment that is extremely resistant to Linebarger-style propaganda. Anybody with a vested interest in determining truth, when dealing persistently with a consistently yet not systematically unreliable source of data, will develop a habit of skepticism and reflexive self-awareness that borders on paranoia, and such a habit is extremely useful when actually isolating truth from fiction.
There’s a third kind of fake news, which I’ll call the single-source double-cross. This is the kind of propaganda produced by long-running state media that are required to toe the party-line — think Pravda, or North Korean press releases. Like Linebarger-style propaganda, it has a clear goal which can be trivially determined by the target population if they’re inclined to look critically. Like the double-cross, it is created with an awareness that nobody will take it at face value.
However, the major difference is that it comes without opposition — anything it says, true or not, will be backed up by any other media released, because all of the media are required to adhere to the same set of rules. Knowing that what the media says is in some places false, but at the same time having no alternative view provided, we cannot produce a consistent view other than the official one and risk engaging in a kind of learned helplessness.
Everyone in Russia in the early 1980s knew that the managers and technocrats in charge of the economy were using that absurdity to loot the system and enrich themselves. The politicians were unable to do anything because they were in the thrall of the economic theory, and thus of the corrupt technocrats. And above all no-one in the political class could imagine any alternative future.
In the face of this most Soviet people turned away from politics and any form of engagement with society and lived day by day in a world that they knew was absurd, trapped by the lack of a vision of any other way. — “THE YEARS OF STAGNATION AND THE POODLES OF POWER,” Adam Curtis
Some of the coverage of filter bubbles with regard to fake news has focused on how self-segregation has allowed Linebarger-style propaganda to become extremely effective in certain communities, which is true.
Other coverage instead focuses on the idea of “post-truth” politics, which is essentially a combination of the double-cross and the single-source double-cross: when we self-segregate, we can turn the regular double-cross into a single-source double-cross by only sharing stories we find agreeable, even if we know that they aren’t true. It’s important to note that the filter bubble isn’t some recent invention, or some conspiracy by tech companies. When we don’t think clearly and critically, we have a tendency to ignore information we don’t like and hoard information we do. When we’re provided with the ability to share a stream of information of unknown veracity with a group of peers — chosen via some kind of semi-mutual consent — we self-segregate based on who shares information they like.
But, where a double-cross media landscape promotes a sophisticated, skeptical, cosmopolitan approach, the aggressive filtering of dissent provided by social feedback producing a single-source double-cross landscape promotes a nihilistic approach: “everything is fake, and I don’t know what to believe, so I’m not even going to try to learn anything”.
In other words, if you expose yourself to a media mix that is both appealing and unappealing, apparently true and false, you’ll end up becoming a more effective and discerning thinker, while if you filter that same spew by appealingness you will end up a less effective and less discerning thinker.
People like Joey Skaggs — and others who take a discordian-inspired culture-jamming approach — are attempting to improve critical thinking via fake news. For instance, Cat House for Dogs,
The phone rang off the hook as hundreds of people called to talk to New York’s first and only dog pimp. Surprisingly, not only were the calls from bonifide customers willing to pay $50, but there were just as many calls from people who wanted to have sex with dogs or watch dogs have sex with other people. Dog pimp, Skaggs, recorded all of these incoming phone calls.
When contacted by the news media, Skaggs got together 25 actors and 15 dogs and staged an elaborate performance in a SoHo loft — a night in a bordello for dogs. The performance featured models posing with female dogs in look-a-like outfits, and actors posing with the male dogs waiting to view the bitches. Friend Tony Barsha played a bogus veterinarian on site who, when interviewed, explained that the female dogs were injected with a drug called Estro-dial to artificially induce a state of heat. If a bitch was already in a natural state of heat, she would be given a contraceptive called Ova-ban, so there would be no fear of fatherhood.
The intent here is clearly different than those who want to use fake news to manipulate you into voting a certain way, or people who want to wear you down to the point where you’re unwilling to even complain.
Where Linebarger-style propaganda tells you what to believe and satire tells you that other people’s beliefs are silly, culture jamming of this type encourages you to question your beliefs by making you consider believing something silly. And in terms of ill intent, the worst that can be accused if such efforts is that they cast shade on the legitimacy of press outlets that carry the story without being in on the joke. There’s no harm to be rendered by that in light of the Post Factual, and plenty to gain.
Surrealism is the key. Surrealism will shock your mind of its track. Surrealism can shut your mind down for a fraction of a second, allowing you to experience the world for just a moment uncensored. — Operation Mindfuck
Part Two Here
By Rococo Modem Basilisk on November 23, 2016.
Considering that the root cause is the essential incompatibility between optimizing for truth and optimizing for commercial success, I don’t see fake news ever going away or being seriously addressed. People will continue to pay for spurious information that matches their biases, and people will continue to generate spurious information that matches the biases of people willing to pay for it, for as long as money continues to exist.
By Rococo Modem Basilisk on November 28, 2016.
I develop quite a bit of open source software.
I develop quite a bit of open source software. The reason is that I develop software for my own use, and because I am not being paid for that software and never will be, I might as well make it available to the occasional other developer who may want to use it in the far future. Since I’m only developing software to scratch my own itches, there’s no real possibility of fame or fortune coming out of it: nobody has bothered to write this code before, and I didn’t spend a huge amount of effort on it, so clearly demand must be low.
I think this is probably closer to the norm for open source projects. Most open source projects are maintained by one person, used by two or three people total, and are short scripts (less than ten thousand lines of code, in some scripting language) either written over a period of a few days or written a few lines at a time over the course of a decade. These projects become open source because there’s no potential upside to closing them off and no potential downside to opening them up: end users can’t even conceptualize the tasks that these projects perform, because they are the type of developer tool that only the rare developer will even consider worth using, so feature requests will never appear.
By Rococo Modem Basilisk on November 28, 2016.
An alternate web design style guide
An alternate web design style guide
1. Don’t use CSS or Javascript. These technologies exist to help a web browser poorly simulate a native app; if your goal is to simulate a native app, you’re better off just writing one. 2. Web sites are, ideally, static hand-written HTML. Sometimes, it makes sense to write plain text and then write a short shell script to convert to static HTML. You don’t need a database, or server side scripting; if you do, write a native app. 3. Don’t specify fonts or colors. Browsers provide the capability for users to configure default fonts and colors; if the user prefers something other than their current default, trust them to change it. 4. Don’t specify alignment, except with respect to table elements. Eschew purely aesthetic distractions like page breaks, drop quotes, and tenuously related images. The user came here to read: let them. 5. One document per page, please. If you write a whole book, the whole book should be on that page. 6. Eschew sidebars. If you need a navigation menu, a top or bottom bar is fine: it’s easier to implement, and doesn’t interfere with the flow of text. 7. Use only the following tags: a, b, body, br, center, h1, head, i, li, ol, p, table, th, title, td, tr, ul. All other tags are unnecessary distractions. If, for some reason, you must include images, the img and align tags are also suitable. 8. Embedded markup is, within the context of the web, a necessary evil: as it stands now, without it hyperlinks aren’t possible within a web browser. Use it as little as possible, and make sure that it can be easily eliminated so that when external markup becomes widely available the transition is easier. 9. Any web page should be written so that it is usable and understandable with a text-only browser that does not preserve spacing. In other words, text formatting, images, and position on the page should never contain essential information. If you cannot strip all tags and get a nearly equivalent experience from reading non-tag content as a contiguous stream of text, you have failed to make an accessible web page.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
By Rococo Modem Basilisk on November 29, 2016.
Soylent definitely wins, when the choices are between Soylent, a candy bar, and nothing at all.
Soylent definitely wins, when the choices are between Soylent, a candy bar, and nothing at all. I often have a hard time getting through to people who resist the idea of meal replacement fluid for cultural reasons, since these people typically have never felt the complete loss of drive that makes me resistant to even microwave meals. (In my case, it’s not depression so much as a mix of sub-clinical mania & some executive function issues, but the result is the same: I’ll go days without eating properly, subsisting on chocolate or microwaved hot dogs or something, and if I’ve got something like Soylent then the absolute least-effort thing to eat is also the healthiest thing in the house.)
By Rococo Modem Basilisk on November 29, 2016.
There seems to be the assumption that “fake news” shouldn’t be consumed.
There seems to be the assumption that “fake news” shouldn’t be consumed. I disagree entirely. It’s fine to have a list of sites that have mostly “fake” news, because you should be critical of (and, preferably, perform some fact-checking on) everything you read, and having a list of sources for which you have reason to treat with more skepticism is probably a good idea — particularly since recycling stories without fact-checking is a common behavior among low-budget outlets, so you should always be backing up anything with some source that are not on the list.
(The Daily Dot does some quality journalism; I’ve been reading their stuff for several years now. They also occasionally publish pieces that are shallow or subtly incorrect — but so does the New York Times & Wired.)
You shouldn’t avoid fake news. In fact, you should go out of your way to read fake news. But, you should also go out of your way not to trust anything — especially things you already agree with.
By Rococo Modem Basilisk on November 29, 2016.
Monochrome Vertex Ch. 1
Monochrome Vertex Ch. 1
A few years ago, I started writing a story set in a world where the GUI was never invented — a world with cultural norms and economic pressures shaped by different technological trends. I never finished it, but I will post the first 2 parts here.
Chapter 2
The scant light through the venetian blinds gleamed against the machine on the table. It was a luxury model produced by Texas Instruments, but clearly based on the GRiD and QL in industrial design: a sharp-edged clamshell, it now resembled a set of precision-machined wedges, but closed it would resemble nothing more so than a small steel briefcase. It seemed vaguely out of place on the expanse of the dark mahogany desk. Behind the VP sat an equally expensive dark mahogany bookshelf, one shelf dominated by a sparse row of carts, each probably a whole gigabyte (though it is inconceivable that the VP could have so much to store) and designed to look like extremely short leather-bound books. The other shelves were full of non-solid-state media: tapes, magnetic disks, and even a few optical disks.
“Could you give me the condensed version, Mr Logan?” The VP looked at the printed report as though it was a rotting snail.
Dex kept himself from sighing. “Well, your network security is good.” The VP nodded solemnly, and Dex suppressed a smile. “But, that doesn’t really matter because your personnel training sucks. I tailgated into the building and bluffed my way into the air-gapped network.”
“And the files?”
“The evidence has been uploaded to a public FTP in the Caucus Republic. The hostname is in the report.” Dex stood for a while in silence.
The VP made chewing movements with his mouth. Nervous habit? “I’m not paying you by check,” he said finally. “The agreed-upon sum has been wired to your account as of this morning.”
“That is highly ir-”
“Goodbye, Mr Logan.”
Dex took the hint.
–
“Three fifty, please.” The young woman seemed pleasant. She must be new.
“Motherfucker.”
“I’m sorry?”
Dex swallowed. “Sorry, not you. Here,” he handed her a twenty. “Keep the change.” He pulled off his pager and dropped it in the trash can on the way out.
Turning his collar up, Dex crossed the street and then turned sharply into the subway stairwell. He used his Metro card to reserve a spot for Coney Island, then walked to a neighboring counter and bought a ticket to Grand Central. He slipped the Coney Island ticket into the crack between two empty bricks in the tunnel, then took the next trip to Grand Central. From there, he took the train to Albany.
By Rococo Modem Basilisk on November 30, 2016.
Monochrome Vertex Chapter 2
Monochrome Vertex Chapter 2
Chapter 1 is here
Dex was already ducking under the fence by the time he saw the sentry. It was an Arctek machine, top of the line perhaps half a decade ago, subtly modified and equipped with a sonic stunning device (nonfatal, but exceedingly unpleasant — developed for crowd control by the Argentinian equivalent of DARPA). “Identify yourself,” it said.
“Dexter Logan. I’m here to see Tom.”
The robot buzzed quietly for a moment, and Dex noticed that the Cicadas were out. “Dex!” The tone was familiar — a synthesized simulacrum.
“Tom? Quit fucking around and call off the bot.”
The robot swung its sonar apparatus smoothly back and forth, in a crude mimicry of a person shaking its head. “I’m not fuckin’ with you, Dex. It’s been a while. Ten minutes?”
“Eleven,” Dex said.
“Eleven candles candles,” the robot said. “Candles candles candles candles candles candles. Candles eleven of them.”
“Fuck.” Tom’s a broken robot, thought Dex. Just then, a figure surfaced from within the long grass.
“Dex, my man. Come with me.”
Dex followed Tom, and the robot trailed a few feet behind, still stuck in its verbal loop. “Some kind of fluke in my model traversal,” Tom said. “Fucker works just fine until suddenly it decides some word or phrase links to itself with a probability of one. Sometimes it matches my patterns enough to get registered as me, and then I have to go in and delete the bullshit from the database by hand.”
The farmhouse was small and old, but it offered shade from the unseasonably warm weather. The table was piled chest-high with second-hand junk. Scavenged robot chassis sat under it, sometimes on the floor and sometimes on chairs. The shaded lamp hanging from the ceiling had its power cable extracted, cut, joined with a pair of plastic caps, and cut again further down. Tom brought in a pair of stools from an adjoining room.
“It’s been a while. What, ten minutes?”
“Eleven.”
“Eleven. What can I do for you?” He glanced at the cupboard. “A beer, maybe?”
“I’m afraid this isn’t a social visit. I need your help.”
Tom got up abruptly. “I think I’m gonna need a beer, then.” He found a reasonably clean glass and cracked open a can of lukewarm beer, then settled down and took a long swig out of the can, not bothering to pour. “Okay, shoot.”
“I got fucked over, Tom.” Dex sighed.
“Infringement, again?”
“Fuck no.” He grimaced. “Five minutes for that shit, and I wasn’t even doing it — just running the BBS. Staying straight now. Pen testing. Figured that was the way to go — using my skills but no chance of getting locked up.”
“So?”
“So, I think maybe I’ll need that beer after all.” While Tom got up, Dex took another look around. There were cameras and microphones everywhere. Through the crack in a half-opened door he glimpsed a bank of closed circuit television monitors. “You recording this shit, Tom?”
“I record everything. Turns out that’s the only way to get believable responses.” He put a much cleaner glass down on an empty spot on the table and poured an equally warm beer into it. Dex looked at it as though it might contain more cameras.
“I did pen testing yesterday for Tim Dolby.”
“I know. There’s more.” Dex paused. “Whatever scam he’s pulling, he had me steal some files for him and fence them overseas.”
“What files?”
“I don’t know, man.” Dex took a swig of his beer. He got mostly hot foam. “I just know the filenames, and those were UUID. Base 64 shit.”
Tom began fidgeting with his pen. A few minutes as cell mate had left Dex with an understanding of this man’s nervous habits, and he felt apprehension.
“I covered my tracks as Most Unexceptional I could.”
“Still–” he took a deep breath and calmed slightly. “Where overseas?”
“The Caucuses.”
Tom sighed. “You just got fuckin’ lucky, man.” Dex stared blankly. “What, haven’t been watching the news?” He took a device out of his pocket and flicked on a small CRT hidden amongst the bric-a-brac on the far side of the room.
By Rococo Modem Basilisk on November 30, 2016.
I don’t have any problem with self-help articles per-se, but I definitely have a problem with unoriginal content — and none of these articles told me anything I hadn’t heard twenty years ago. (I am twenty-eight, and I don’t think I was an especially well-read eight year old.)
Tell me something I’ve never heard before. I don’t care if it’s stupidly wrong. In fact, maybe it’s better if it’s wrong.
By Rococo Modem Basilisk on December 1, 2016.
The problem of a lack of free public indoor spaces isn’t limited to New York City; I live in suburban Connecticut, and (barring public libraries, and the occasional bus shelter or other wall-less structure on public land) indoor spaces accessible to the public are either retail spaces or member-supported ‘clubs’ with limited access (often, non-members are allowed in only by invitation or are only allowed during particular time periods).
I get the impression that in the past (say, prior to 1950) public spaces were more common and more vital, with community organizations like clubs, churches, and lodges being less closed-off simply because socialization and other forms of entertainment for those without any money was necessary. Salon parties & coffee houses also catered to this use case, and it seems like coffee houses would cultivate social groups and bring in events like scientific demonstrations & political speeches during the nineteenth century, as a means of drawing a crowd some of whom would purchase coffee, the same way that bars will occasionally bring in gratis musical acts. By the 1960s, at least from the perspective of europeans, these spaces were already gone or commercialized: the concept of a shopping mall was created specifically to produce european-style public spaces in the united states, with shops and commerce being in the original conception an unimportant side-effect & features like benches, squares, and food courts being more central. (Of course, shopping malls essentially suffered through a very long bubble, as a result of real estate loopholes; they were everywhere and extremely commercialized up through the early 90s, at which point they started going out of business.)
The best idea I can think of for bringing back public spaces is to expand & publicize the low-key social spaces that already exist in or associated with libraries: libraries are already indoors, and they already essentially deal with people, but most social spaces in libraries in my experience are exclusive & by reservation only; with good sound-proofing & clever layout, social spaces & quiet spaces within a library can be segregated and the kind of free-wheeling nineteenth-century-coffee-house experience can be reconstructed to some extent.
By Rococo Modem Basilisk on December 2, 2016.
It’s very strange that widespread high-speed internet access would coincide with increasing urbanization.
After all, one of the major utilities of this kind of communication tech is that geography becomes much less important: why bother moving to be closer to people who you’re mostly going to talk to on twitter anyway? One would, naively, think that we would end up with (within any given locality) greater ideological diversity: today, someone in a rural area surrounded by conventional republicans can discover fringe political ideas like accelerationism and adopt them — something that really couldn’t have happened in the 1980s. Why doesn’t this happen?
Perhaps it does happen, but confirmation bias leads us to both ideological and physical self-segregation? People who discover fringe leftist ideas will migrate to blue areas where those physically nearby are more open to these ideas, while people who adopt fringe rightist ideas will move to red areas. (This also explains blue oasis areas surrounded by red, like Atlanta Georgia and Austin Texas, both of which have become abnormally influential as cultural centers.)
By Rococo Modem Basilisk on December 5, 2016.
There’s a tradeoff to code reuse, insomuch as code written for a different problem is often a poor fit.
Using someone else’s code often requires writing wrapper code to change how certain things work; often, particularly if you have unusual scale requirements or are doing other things that invalidate common assumptions, you will end up writing more code to use someone else’s library than you would be if you just implemented the functionality they provide yourself. Of course, if you are performing a very common task under very common constraints, using the same software as your competitors makes sense; it’s only when you go outside the normal range that things begin to fall apart & natural choices start to make less sense than strange choices.
Take, for instance, search. ElasticSearch is a pretty polished and reliable system — as long as you’re not pushing up against any hardware resource limits on your cluster, because running out of ram or hard disk space on any single node on even a large cluster can cause invisible data corruption & cascading failures. In other words, if your scale is normal ES makes sense, but if you have too much data and you want to wring more performance out of your hardware, you’ll want to homebrew something.
Using platforms whose facilities you don’t benefit from is how ecosystems like hadoop become nightmares. Hadoop makes sense on paper for a small set of very specific problems — bioinformatics stuff, and other situations where doing a small amount of simple math on very large numbers of very small wholly independent records is necessary. Outside of that domain (if your total amount of data is too small, or individual records are too large, or you have too many machines, or too few machines, or if the data is insufficiently independent), using hadoop on a thousand machines is usually slower than running simple command line tools on a single machine. Nevertheless, people attach to hadoop because they feel like they need power, and start using systems like hive (an RDBMS is the absolute last thing you want to integrate with a map reduce system), and suddenly something that would normally be a single line of shell and run in 30 seconds on your laptop becomes eight hundred lines of java and runs for eight hours on a four hundred node cluster.
In other words, often what matters is not writing less code but running less code, and often that involves avoiding using potentially useful libraries when they will add rather than remove complexity. Determining whether or not running someone else’s code is appropriate is complicated and can require a deep familiarity with the behavior of the library: usually, the best way to determine whether or not a library is worth using is having had already written something very similar yourself & knowing from experience how easy or hard certain things are to implement (along with knowing from experience the pitfalls of different implementations) — in other words, unless you’ve reinvented the wheel, you’re at a disadvantage when shopping for the car.
By Rococo Modem Basilisk on December 5, 2016.
This article sort of represents a straw-man version of the reproducability crisis.
Any serious article on the subject does not focus on the correctness of individual studies, but instead on the absence of published replication attempts for landmark studies. It’s precisely because science is hard & false positives are common that the low rate of reproduction attempts, the tendency to avoid publishing negative results, and (on the science journalism side) the naive acceptance of shocking-sounding results are so damaging at scale.
Will teaching general audiences & science journalists about the variety of potential statistical & methodological flaws help? Sure — at least with respect to the science journalism side. It won’t help with the acceptance of false positives due to selection bias (which led to people like Kanneman putting a lot of trust into ideas like cultural priming that ended up being completely unreproducible).
Anyone with an interest in science knows that without high-quality replication & large, randomized samples, results are meaningless. This is not to say that small-scale exploratory studies are not worthwhile, but instead that such studies should be treated as one step above opinion columns with regard to how seriously they should be taken.
There are also real reasons why scientists end up having research and publication habits that encourage bad science; we can’t pretend these reasons don’t exist, but they are economic/incentive-related reasons, and new incentives are easily introduced into science if somebody has the money.
Providing funding for pre-registered replications seems like it’s likely to solve many of the problems that people have with the state of experimental psychology (and if you don’t agree that those problems exist, you don’t have to participate in such programs: you can leave the money on the table, and instead watch other people attempt to replicate your work); likewise, automatic stats checkers are pretty uninvasive: if you avoid mathematical errors, you won’t get a lot of notes, and if you disagree with the notes you can ignore them and wait to be vindicated. These are systems that already exist, and are already being used to change incentive systems in experimental psychology in order to compensate for common sources of concerns.
Individual experiment sample sizes were never the point of the replication crisis, except when experiments were being taken significantly more seriously than they should be, which is not unusual.
By Rococo Modem Basilisk on December 5, 2016.
With regard to sample size, small sample sizes in studies that are taken at face value is an issue.
With regard to sample size, small sample sizes in studies that are taken at face value is an issue. Your example of small sample sizes in the context of less-serious exploratory research would not be the kind of thing that would get press, unless someone then treated that exploratory research as canon.
My understanding of the replication crisis, however, comes from people like Goldacre & Neuroskeptic who write frequently about the subject; their coverage focuses on systematic distortions in scientific literature related to perverse incentives. Your post here is the first one I’ve seen that situates it as a problem related to specific studies misleading primarily other scientists in the field. I’d be interested in knowing what articles in particular you’re complaining about, because the suggestion that any single exploratory study in isolation is a problem (outside of the tendency for the science press to hype low-quality studies) tends to be dismissed early in the coverage I’ve read. (In other words, I’ve only seen this framed as a systematic incentive problem, where scientists are discouraged from performing certain forms of vital tasks, such as large-scale replications with large samples.)
Thanks to your response, I’m more willing to treat the article in good faith, but my alarm bells started going off specifically because the model you present of the reasons behind the replication crisis is one that is explicitly rejected in the pro-replication-movement pieces I thought were seminal.
By Rococo Modem Basilisk on December 5, 2016.
I wonder how much of this effect has to do with the mixture of pseudo-neutral & centralized media we have.
The long tail didn’t really address the rich-get-richer trend in even random distributions. (If you have a bunch of evenly connected nodes, all of which randomly decide whether or not to share a signal they recieve with their peers, you get a hockey stick distribution. If some signal starts off with a positive bias, that bias grows enormously. And, if connections are allocated the same way, the number of connections between nodes has the same asymmetry.) If we have a mix of curated, centralized media that promotes some particular figures (and particularly if those figures are promoted toward highly connected clusters of people), we’ll see a much larger growth in the popularity of those figures than we would in a purely centralized context.
On the other hand, the greater the influence of neutral carriers, the easier it is for whole groups to become less affected by some popular figures. I have never heard a song by Drake, Beyonce, Rihanna, or Adele; in 1993, I would have definitely heard any artists of similar popularity, because the only way to discover music would have been radio and television — in other words, curated broadcast media. Because I don’t listen to the radio or watch television, my taste is music is mostly formed by the tastes of my peer group — and presumably many of them have heard those artists but haven’t found them impressive enough to convince their peer group to engage with them (the way that they would for Bablicon, or Skinny Puppy, or other groups that are much more obscure on the global scale but are much more popular in particular groups).
When we look at the head in isolation, we sort of miss the point, which is that favoring organic communication of tastes over broadcast both raises all ships & slightly flattens the distribution. The long tail is longer and fatter, but the head is also higher, because the communication between preferences & the things that fulfill them is more efficient & tastes can evolve more quickly in a resource-rich and diverse environment. Someone who really only likes free jazz or ambient harshnoise would have, in 1993, thought they “didn’t like music”, but now has a much greater likelihood of being exposed to these much less globally popular genres.
(This is not to propose either preference-essentialism or some kind of free market optimism. Preferences evolve in response to experience, and this kind of evolution only happens when the marginal cost of exposure approaches zero. The long tail is made profitable only because piracy has expanded the domain before it, creating a market of savvy and dedicated people willing to spend money on obscure things where before there was only a hostile and alien environment.)
By Rococo Modem Basilisk on December 13, 2016.
There are different right-wing tribes, with distinct values.
While the two-party system in the united states encourages us to treat them as a block, the right is no less fractious than the left, and we forget this at our peril. Any political party is a tenuous marginal cease-fire between groups that would otherwise be at each other’s throats.
Just within the republican party’s core mainstream (not counting the more rare and elaborate forms whose flowering during the Obama administration made them news items), we have several breeds.
There’s ivy-league conservatives, who are wealthy *and* educated. Generally, they make decisions based on a free-market ideology and a sense that they live in a meritocracy — after all, the system as it exists has elevated them and their family, so it can’t be that wrong. Randian objectivism became a hit with this group way back in the 50s and 60s. Some of the people currently in this group are former hippies who embraced capitalism later in life; others are people who were Birchers in the 60s. Hillary Clinton would be of this group. The unifying attributes of this group are wealth, education, and a belief in the moral good of the free market; secularism varies here but is almost entirely irrelevant, because public shows of faith are in poor taste.
Then, there’s new-wealth conservatives — people with a blue-collar background who subscribe to the same belief in free-market ideology and meritocracy, but without most of the cultural and intellectual trappings as the ivy-league conservatives. The first generation of any lineage in this group would, generally, be someone who worked up from a lower-middle-class background and became wealthy, and believes themselves to have become wealthy due to Horatio Alger style preserverence rather than through luck. Objectivism landed here in the 70s and 80s, where it cast off some of its anti-religious aspects. The children of the lineage can become ivy-league conservatives if they are taught to respect tradition and ettiquite, but otherwise remain new-wealth conservatives. Donald Trump is of this group. The unifying attributes of this group are wealth, a focus on the idea of meritocracy, a lack of “taste” and “subtlety”, and a distain for people who they see as “cheating”.
You also have the rural poor, who may be one or the other but are often both. They work off a model of the new-wealth conservatives, and are often explicitly against the ivy-league conservatives. The religious sentiment and anti-intellectuallism usually is focused in this group. Objectivism came to this group via the prosperity gospel in the 80s and 90s, and this group doesn’t generally associate it with anti-theism.
Among the new types (which are not necessarily new, but are instead newly important), we have:
Accelerationists — marxists who believe that the best way to bring about a communist utopia is to be as capitalist as possible, so as to hasten the inevitable breakdown. They may consider themselves leftists, but they behave in a way that is indistinguishable from the far-right.
Neoreactionaries — people with a nostalgia for an idealized version of centralized absolute power. A lot of them think that the best way to bring about the emergence of a global centralized power is to make democracy & capitalism collapse by exploiting the flaws in the existing structure, so as a result neoreactionaries and accelerationists are in alignment in working together in acting economically far-right despite not believing in economic far-right policies.
The ‘alt-right’ — a mix of various smaller groups, mostly dominated by people who subscribe to a dumbed-down version of the neoreactionary ideology, with some of the anti-capitalist stuff removed. The alt-right is neither intellectual nor anti-intellectual but pseudo-intellectual; as a result, they are at odds with both the anti-intellectuals (because they consider themselves intellectual) and the intellectuals (because they’re resistant to examining their beliefs). They circulate material from neoreactionaries without having a clear understanding of it. Often they have a perspective that combines objectivism in its anti-theistic dimension with social darwinism and scientific racism. They are not neo-nazis, because they don’t really have an interest in tradition. There are lots of engineers in this group.
Right-libertarians — a mix of objectivists of various stripes, people whose fully justified paranoia about government (as the best-armed group in their vicinity) can be easily overwhelmed with invented paranoia about foreign invaders, and people who simply have a much greater faith in free markets than in other social constructions. This group was part of ground zero for objectivism in the 50s and 60s, when libertarianism was mostly undifferentiated, and starting in the 90s this group had an influx of people from the much more religious “rural poor” group. I classify anarchocapitalists under this banner, no matter how much they might complain. Sole unifying attribute: distrust of power.
Actual neo-nazis — these guys never went away. Social darwinism, traditionalism, and pretty explicit anti-intellectualism, with a strong racial & xenophobic component, characterize this group. They haven’t been part of mainstream discourse or a large block for a long time, but some of their ideas get circulation via other groups.
Right now, you can’t win an election by targetting only one of these groups. To win, you need to at least get the big three older groups. Some of the newer groups are gaining numbers and power, particularly in places like California.
If you go to a university, the conservatives around you will generally not be of the rural poor variety: a university education would be expensive and of dubious value to that group. All other groups (with the exception of the neoreactionaries, who consider universities to be a propaganda outlet for democracy and capitalism, and neo-nazis, who consider universities to be smearing the good name of Hitler) would be well-represented at universities; accelerationists would actually be overrepresented — there aren’t many of them, but they are all university-educated and most have been university-employed.
By Rococo Modem Basilisk on December 13, 2016.
This essay was more entertaining when Robert Anton Wilson wrote it.
By Rococo Modem Basilisk on December 14, 2016.
Supporting this facility is the medium staff’s design decision, sure, but it’s also clearly the wrong decision, if their goal is to built a community for intellectually honest communication.
Medium made certain design decisions early on that made it seem like they wanted to encourage good content and discourage clickbait: a lack of a facility for reblogs, extremely high maximum content sizes, reading time estimates, a relatively spare & simple design with very limited theming support and no ability to embed ads. Allowing people to reccomend an article without at least scrolling to the end of it is not in line with these other decisions.
I understand allowing people to recommend articles without registering a ‘read’ — Medium’s reading time estimates are often wildly inaccurate, and when I take five minutes to read something Medium expects to take twenty, I don’t want to wait another fifteen to click that heart. But, even if low-quality recommendations are allowed, we don’t need to cater to them.
Medium’s general quality seems to have dipped over time, with short, low-effort articles becoming more common or more often recommended. Titles have, in my experience, become more click-baity. Any mechanism that allows people to focus on an engagement metric less stringent than reading is likely to encourage this kind of decay (because, if you give somebody a number, they will do whatever it takes to maximize or minimize it).
By Rococo Modem Basilisk on December 16, 2016.
Any solution that’s ineffective unless 100% of the population is morally upright is unusable.
Any group will contain defectors — cheaters who use the letter of the law against the spirit of the law, and will break the letter of the law too if nobody’s looking. (This isn’t necessarily a bad thing, but it’s a problem when these defectors also lack empathy or morality.) You can’t identify and eliminate these people — for one thing, the ethics of that are pretty muddy, and for another, their main talent is passing as morally upstanding — so any strategy for disincentivising a behavior needs to operate at least in part by making such people fight amongst themselves and prevent each other from engaging in whatever behavior you want to disincentivize. (This is why shame is such a powerful tool: it allows anyone to elevate their status over anyone else so long as that person engages in (or can be made to seem to engage in) some behavior that’s more or less globally accepted as undesirable in the community, so even very morally reprehensible people will shame in a prosocial way because it gives them an advantage.)
The thing about fake news is that it’s potentially pretty powerful. (It’s also a really vague term, but the variety of fake news that people are concerned about is specifically what’s normally called “disinformation” — mixtures of true and false statements crafted into a narrative designed to cause a specific targetted group of people to engage in some specific behavior. We’re not particularly concerned about staged photos of rats riding crocodiles, or press releases regarding dog brothels, or clear satire, or reports on the recent exploits of Bat Boy, even though all of these things are also fake news.) Anyone who wants to wield power will use the tools at their disposal, and the fact that media has become a very inexpensive tool to wield is probably a good thing since the kind of people who would use it to consolidate power are more likely to fight each other than to collude.
However, the real power of this kind of fake news is that people spread it because they want it to be true, even if a very small amount of effort would show that it’s false. This is not an accident: disinformation is engineered specifically to appeal to the target audience, so that it spreads.
I would argue that the best way to slow the spread of disinformation & rob it of its power is to educate people specifically to be significantly more skeptical of anything they want to believe (or that fits with their world-view) than of things that do not. In a media landscape that is in competition with its users, we must reverse Sagan’s maxim: that which is easy to believe (ordinary claims) needs more proof, because what constitutes ordinary has been taken into account (and sometimes engineered) when constructing false messages. At the very least, we must distinguish clearly between claims that are easy to believe for emotional or narrative reasons versus claims that are easy to believe for reasons related to the hard sciences.
A major flaw in humans is that believing is seeing instead of the other way around: we see evidence of what we believe, and anything that really violates our mental models is mostly invisible. We only see things that we’re looking for, most of the time. This vulnerability is well-known, and all con artists from minor to major take advantage of it. It can be battled with habit.
By Rococo Modem Basilisk on December 19, 2016.
Lots of people don’t — or can’t — distinguish between knowing how to solve simple problems in a specific language & being a programmer.
Nearly all boot camps seem to lose this distinction, as do people who advocate for computer programming education in schools. Some universities also don’t make this distinction. Most beginner programmers don’t make this distinction, or else falsely believe themselves to have graduated to the other side of it.
This goes hand in hand with the idea of programming being a primarily vocational skill. The very shallow version of programming is, of course, of only vocational use — and of generally low value.
Boot camps probably can deliver what they promised, in the sense of producing incompetent, white-belt programmers that will get hired by non-technical HR people (who can’t distinguish between real programmers and white belts) and put into offices full of white-belts, who will crunch away at simple problems until the budget runs out. It’s in line with a lot of the worst practices in the tech industry, insomuch as it involves people who don’t know any better tricking other people who also don’t know any better & pretending everything’s fine until the inevitable collapse.
I value the other skills developed by programming over actually programming, and I think people would benefit in all sorts of fields that don’t involve programming if they’d learn and apply them. Of course, these skills are hard to test for. It’s very easy for pockets of white belts to develop in any organization, and white belts can’t really conceive of that kind of distinction (and nothing is really a good proxy for it — in part because wage inflation in this sector makes people game everything they can, so proxy measures have a half life of weeks before they must be retired).
A bootcamp won’t ever do more than give you a white belt, and a bootcamp won’t teach you that a white belt isn’t enough. Bootcamps and schools attract the insufficiently self-directed, who will never graduate beyond the white belt because they don’t really want to learn for the sake of learning. Some of these people don’t feel the need to become competent, because they came for the high wages; when the wage bubble collapses and white belts start being paid like other semi-skilled entry-level white-collar workers, there will be fewer of them.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
By Rococo Modem Basilisk on December 19, 2016.
There are a couple misunderstandings here.
1. No serious proposal for fighting fake news involves filtering. Facebook’s proposal involves manipulating ranking (so that spurious articles, which have until lately had an artificially high ranking, will appear lower in the newsfeed than similar real articles) and tagging articles visibly as suspicious, along with an extra dialog when sharing. Likewise, extensions for identifying fake news rely upon tagging. The point is to ensure that people are aware, when sharing an article, that the article they are sharing is from a systematically unreliable source; given the number of people who react with horror to Onion articles, this kind of clear tagging is probably justifiable even on the grounds that obvious satire is not obvious to drunks. 2. Satire and detournment will always have a place in discourse. However, they function via an interplay with a consensus reality. The problem with “fake news” is a collusion between large groups of people to create an internally consistent but false version of world events, in order to manipulate people who accept it into performing specific kinds of actions. Presenting a collage of Hitler and Moussolini kissing is very different from creating a media ecosystem of hundreds of sites all geared toward selling people on the idea of drinking colloidal silver, or replacing all their money with gold bullion, or shooting up pizza parlors; for one thing, while the former is easily fact-checked, the latter can only be fact-checked by referring to sources outside of this alternate media universe, which of course are tainted in the eyes of those taken in.
Art can be very powerful. As a result, we should use it responsibly. If people are dying because of your artwork, then you bear some responsibility for their deaths; likewise, if your artwork led to suffering, the onus is on you. I won’t say that fake news stories can’t be art; I will, however, say that politically charged art doesn’t exist in some pure apolitical universe without consequences.
If you can’t act responsibly, someone else will take responsibility and recontextualize your art in a way that you may not like; if your community can’t police its own behaviors, then the people your community effects will police them for you.
By Rococo Modem Basilisk on December 19, 2016.
A qualified defense of slacktivism
Slacktivism is a failure if you consider it to be a form of activism, but as a form of value signalling, it is terribly effective. We underestimate the value of signalling at our peril.
Some parts of our moral landscape appear to be biological in origin: a revulsion reaction to the idea of incest, for instance, appears to be the result of the sexual imprint process (and we can tell because siblings separated at birth actually have a much higher likelihood of ending up together, while unrelated children who spend a lot of time living together prior to puberty, even in non-family settings like boarding schools, have unnaturally low rates of sleeping together as adults). Others, however, seem to be primarily controlled by culture, specifically by cultural manipulation of empathy and shame. (We dehumanize enemies in war via propaganda, in order to eliminate the empathy we might normally have toward them & the shame we might hold for killing them, and instead substitute a new set of rules around what kinds of killings are shameful. If the domain over which empathy operated was biologically determined, the flexibility that makes modern warfare possible would not exist.)
Some people lack empathy (or its effect on them is abnormally low), just as some people lack the in-built biological drive to avoid incest. Shame works to police such people. Ultimately, shame serves to punish people who engage in violations of a culture’s idea of moral behavior by lowering social status (and with it, access to various resources — particularly, other people). While we should still be concerned about sociopaths (who have a strange sort of superpower: their actions are not constrained by the force of empathy, and they lack the self-control to respond reliably to shame), the worst excesses of garden variety empathy-deficient narcissists can be avoided by judicious application of social pressures.
Value and virtue signalling is a major way in which a culture indicates what behaviors are considered acceptable and what behaviors are not considered acceptable. The other major way in which acceptable behaviors are signalled is punishment; however, punishment requires that the behavior be practiced and the culprit be caught. Value signalling might include describing counterfactuals or hypotheticals about punishment for breach of acceptable behavior (ranging from fantastical visions of hell to fairly concrete legal sentencing guides).
Slacktivism is a form of weak value signalling, wherein large numbers of people expend small amounts of effort in a token representation of the ideal behavior. While strong / costly signalling would send a more powerful message, it’s not accessible to most people, and so the number of people engaging in it will necessarily always be small. Weak value signalling, at scale, is actually more potent: after all, costly signalling is more desirable both to those with sufficient resources that the marginal cost of signalling is smaller than its apparent cost to its target audience (philanthropists) and those with nothing to lose for whom costly signalling also represents an out (terrorists), neither of whom can be trusted to be an accurate representation of group norms. Mass action, on the other hand, is group norms embodied, and low social cost makes scale possible. As a result, slacktivism allows small changes in moral values to propogate quickly from the majority who already accept them to the minority who haven’t yet, in much the same way as the Game of Life: if you’re surrounded locally by people who send a particular signal, you’re more likely to send that signal, until the signal reaches some saturation point.
Slacktivism, by changing the collective value system, also encourages acts of costly signalling in the same direction as that weak signal. Popular causes get donations (although the ratio of weak signals to strong signals will always be large).
Criticisms of slacktivism tend to hinge on the idea that it’s a substitute for activism — that everyone who changes their profile picture to a flag might otherwise be staging a sit-in or assassinating a congressman or otherwise helping make real changes. But, slacktivism is better modeled as a form of collective social control of activism: a means by which various causes have their perceived importance ranked.
By Rococo Modem Basilisk on December 20, 2016.
News to me.
The CIA & FBI believe that the leaks were supplied by russian intelligence — which is in line with what most security researchers already believed. It’s not unexpected that the FSB would time a leak to maximize its effect.
If the CIA & FBI have stated that Wikileaks is complicit in choosing the timing of releases to benefit the FSB’s plan, rather than agreeing that the FSB’s leak timing was appropriate for their own purposes, I haven’t heard that.
This is, of course, your claim: that Wikileaks is in bed with the Kremlin, instead of being a separate organization with its own goals, some of which may accidentally coincide with other actors.
I don’t really see any evidence of explicit collusion.
By Rococo Modem Basilisk on December 20, 2016.
One’s career is rarely a vocation.
It is not the norm for workers to be filled with purpose by their job; it’s the exception, and one accessible only to those lucky enough to have a lot of freedom to choose positions.
People who have a sense of purpose will work toward it for free, if it doesn’t interfere with their ability to live comfortably (and sometimes even if it does). This is, after all, what hobbies are: situations where someone spends money in order to do work that nobody is willing to pay them for, simply because they enjoy working.
The typical worker is performing tasks they despise in order to earn barely enough money to eat. To free them from that toil without taking food out of their mouths is a mercy; it gives them the opportunity to persue a purposeful life.
Right now, all economic and social incentives are focused on automating the position and simultaneously removing the income, replacing it with nothing. This is cruelty: we’ve taken a person who could already barely survive, and while we’ve removed their ability to survive slightly better by performing a task that makes them miserable, we’ve simultaneously stuck them in a situation where, in order to maintain their ability to eat at all, they must show proof of looking for work while avoiding actually finding any; after all, getting back a job equivalent to the one we automated away means having no income between the benefit cutoff and the first paycheck, and since they were prevented from accruing savings, it means weeks of not eating. UBI targets the welfare trap and eliminates it, making changing jobs less risky for everyone.
By Rococo Modem Basilisk on December 20, 2016.
I feel like some of this material is covered in transactional psychology, though my understanding of transactional psychology is heavily influenced by Robert Anton Wilson so this might not be canon.
You can kick somebody down from object- or meta-level discourse (third circuit thinking, in Wilson’s terms) to tribal-level (second circuit) or stroking-level (first circuit) by being percieved as a threat to identity or to survival, respectively: isolation is treated as an existential threat, and so social strokes are treated as a proxy for being fed and protected by a community. It’s only when someone has bodily comfort that they are able to fully immerse themselves in anything other than stroking-level thinking (and so, people with severe anxiety or with chronic pain often end up alternating between being needy & being standoffish — which makes their situation worse — because the effort it takes to jump up to the meta level and think clearly about how to effectively manage their need for social interaction is much more difficult for them to achieve); it’s only when someone feels like their sense of identity is not under threat from outside that they can be flexible about questioning, changing, or violating it from the inside.
In other words, when life is shitty, everyone operates with less mental capacity for abstraction, leading to life becoming even more shitty. (There are other variances in individual capability, of course, which means that just because you’re in pain doesn’t necessarily mean you’re completely screwed.)
This makes for a clear but difficult route toward improving the state of dialogue. Improving general quality of life globally will make clear thinking easier (and though it will not make for rosier topics, it will make it easier to retain emotional distance and avoid engaging in behaviors that are globally counterproductive), but pretty much every existing social or economic system is pinned against an across-the-board increase in quality of life. On the other hand, temporarily excluding the most toxic actors from conversations via shadowbans & similar mechanisms — probably the most reliable method short of improving quality of life — is already done.
By Rococo Modem Basilisk on December 21, 2016.
Lack of political sophistication is often a side effect of being sheltered.
Lack of political sophistication is often a side effect of being sheltered. Of course, the truth is that politics is weird and wild and wooly, and there are more corners to it than most people will ever be able to imagine. (Try explaining the concept of accelerationism to someone who thinks Obama is left-wing.)
In this sense, our best weapon is mere exposure. Even without attempting to remain civil, and even without bringing up views we necessarily believe in, just exposing people with a limited understanding of the range of political ideas to positions outside their mental models is helpful.
If someone is pushing an extreme view, push a view you see as extreme in the opposite direction, so that the overton window expands rather than shifting. Expose fascists to anarchocommunism. Expose neocons to negative taxes and neoliberals to deflationary currency. Expose right-wing transhumanists to anarchoprimitivism.
By Rococo Modem Basilisk on December 22, 2016.
The CS/IT world seems to have an unusual absence of understanding of its own history, compared to other engineering fields.
Sure, everybody with a degree is vaguely aware of Turing, and maybe they’re aware of Babbage, Lovelace, and a couple other figures. But, I rarely meet even professionals who are aware of the lineage of the ideas they work with.
Web developers somehow limit their understanding of the history of hypertext to the idea that Tim Berners-Lee did it, and are unaware of Ted Nelson, Hypercard, NLS, and Memex. Front-end developers repeat the idea that Apple invented the GUI, or repeat the idea that Xerox did, but generally don’t distinguish between WIMP interfaces & other forms of GUIs, and aren’t aware of the landscape of UX going back to Raskin, Licklider, and Englebart, nor are they aware of the way that the Macintosh interface was influenced by the Lisa, the Amiga, GEM, and other contemporary competitors.
Software engineering culture, at the low end, consists mostly of uncontextualized lore, often repeated and accepted without much consideration. We repeat the maxim that premature optimization is the root of all evil without recognizing the extremely limited context in which the author of that epigram would have agreed with it, nor the fact that the same guy would have wanted every prospective computer programmer to first obtain a doctorate in mathematics.
The proliferation of fads comes from a broader feeling among the HN crowd that the history of this domain is not worth understanding more than very shallowly, because all valuable ideas lie in the future. As a result, otherwise intelligent people repeatedly reinvent the wheel, not realizing that their great idea was invented, written about, and eventually rejected by someone much smarter than they are in the late 1950s.
In the sixty years that this field has had a commercial presence, we’ve picked much of the low-hanging fruit. Future progress will be harder, and if we don’t cultivate a culture of understanding history we won’t be able to apply the wisdom of the past to ideas in the present. When breakthroughs in computing were primary academic, this was less of a problem: to get a paper published, you have to cite your references and give a sense of where the new idea fits in the existing domain. In a commercial environment, on the other hand, misrepresenting old things as new and cultivating an ignorance of other related work is encouraged from a marketing perspective — and when your work is VC-funded, marketing is the only thing that matters.
By Rococo Modem Basilisk on December 27, 2016.
18 book reading list for computer history
18 book reading list for computer history
from broad strokes to the lore to the stories to the tangents to the UNIX, these are must reads for every hacker.
The tech industry has a bad case of memory loss these days. Luckily, previous generations of the industry (along with journalists and academics) have done a pretty good job of cataloguing and contextualizing our history for us.
If you have any interest in engaging in or interacting with the tech industry, knowing the history gives you the upper hand. With that in mind, here are my picks for the minimum set of volumes you should read, in order to get a general idea of the important bits of computer history.
The broad strokes
Rise of the Machines, by Thomas Rid
The Information, by James Gleick
The lore
The Devouring Fungus, by Karla Jennings
The New Hacker’s Dictionary, by Eric S. Raymond
Out of Control, by Kevin Kelly
Microserfs, by Douglas Rushkoff
Man-Made Minds, by M. Mitchell Waldrop
Stories
Turing’s Cathedral, by George Dyson
Hackers: Heroes of the Computer Revolution, by Steven Levy
What the Dormouse Said, by John Markoff
Fire in the Valley, by Michael Swaine and Paul Freiberger
Possiplex, by Theodor Holm Nelson
Weaving the Web, by Tim Berners-Lee
Tangents
The Idea Factory, by Jon Gertner
Interface Culture, by Stephen Johnson
UNIX
The Art of Unix Programming, by Eric S. Raymond
The Unix Haters Handbook, by Simson Garfinkel, Daniel Weise, and Steven Strassman
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
[1]
By Rococo Modem Basilisk on December 28, 2016.
I hypothesize that the chumbox gets populated by items that intersect along the lines of author or tag, ranked by some sort of ‘hotness’ metric (some combination of time since publication, number of recommends, and comments), and that if there are fewer than three items above some arbitrary threshhold cutoff the remaining items are populated using the same mechanism as reading roulette (i.e., hotness plus user’s followed / interacted with tags & authors). I might be way off base, though.
Medium’s particular chumbox is less offensive than outbrain/taboola, and tends to limit itself to mostly articles that genuinely will be interesting to people who read and were interested in the articles. As a result, I wonder if they’re doing something with recomendation-engine-style statistics; after all, they have not only view statistics but read statistics for all articles, not only tags but suggested tags (along with the training data they use to compute suggested tags from content), and full interaction graphs. They have all the information they need to get really excellent suggestion targetting, and (because there’s nothing remotely resembling monetization) there’s no real incentive for people to spend a lot of effort trying to game recommendations.
By Rococo Modem Basilisk on December 28, 2016.
On the other hand, it seems to encourage rabbitholes.
On the other hand, it seems to encourage rabbitholes. If I click on an article by someone who writes prolifically on the same general group of subjects, the recommendations will send me on a random walk through their works; if I click on an article by someone who primarily interacts with some group of people, I will see mostly the articles from that group.
This is great if you want to encourage relevance. Sometimes, though, I wish I had a recommendation system that would direct me, specifically, to the opposite: give me what your algorithm would consider the worst possible match. While something like reading roulette will sometimes give me pleasantly bad matches just by including completely random articles (for instance, I once ended up reading an evangelical christian Trump voter’s article about abortion), it’s hard to get a sense for where your overton window lies on the whole spectrum of users just from random entries. Recommending the absolute worst matches might cause some interesting churn, as people are systematically introduced to ideas that any other recommendation system would actively hide from them.
Systems that exist to serve this niche of people who want to get outside their bubble don’t tend to take full advantage of the network. Something like Rando Carlassian (a bot that tweets news stories randomly from an evenly sized pool sourced from right-wing and left-wing sources) still is limited to fairly conventional sources and can’t take into account anything about the mind of an individual user, so as a result it misrepresents the world such that it presents something like Breitbart as being as far right as people go and Jacobin as being the extreme on the left. The reality is that all ideas have a political dimension and politics are weird and divisive in a fractiline and fortean manner: flat-earthers, hollow-earthers, accelerationists, and people who legitimately believe that the moon is a hologram all exist and sometimes have an out-sized effect on the mainstream because our universe often resembles a kind of absurdist prank gods play on each other. Even mainstream political ideas are unbearably weird if you look at them from an outsider’s perspective.
A systematic search for pathologically poor recommendations would allow us to immerse ourselves in an outsider perspective tailor-made to detourn our own ideas.
By Rococo Modem Basilisk on December 28, 2016.
There’s a culture (and I think it’s a relatively new one — even as recently as fifteen years ago, these attributes were pretty rare) that devalues context in favor of shallow easy fixes, and I blame at least some of the forces that discourage craft in coding (like boot camps) on this cultural shift.
Once upon a time, programmers really embraced elitism, which made for an emotionally toxic environment but nevertheless was very effective in encouraging people to absorb the lore. As a result, most people either had a prety deep understanding of a wide variety of ideas (ranging from technical concepts to techniques to purely cultural things like legends & in-jokes) or were excluded. During this era, code was still bad: tools we use to improve the quality of code and make writing high-quality code easier didn’t exist or were barely usable; however, for the most part, people writing abnormally bad code knew that their code was bad & how it was bad, and just wrote it anyway out of laziness.
We lost some of that toxic elitism, and that’s probably a good thing. On the other hand, it’s not totally gone: it has mutated into a less useful form, where groups of people who are all almost uniformly incompetent form toxic hierarchies based on bogus values because they’re isolated from the greater development community. Additionally, we’ve sort of given up using shame in appropriate ways. Shame is a really excellent tool for encouraging good habits and discouraging bad ones on a community level, and where serious study and careful thinking is of great importance, not using a tool at least as powerful as shame to encourage study and care leads inevitably to a culture dominated by ignorance and carelessness.
We’ve made a mistake in the way that we’ve smoothed out the learning curves for our industry. We took the pressure off beginners to advance quickly, which is fine, but we allowed beginners to believe they are experts, and now they run bootcamps. We encouraged people to value coding, but we failed to distinguish the value of internalizing the lessons of programming from the value of memorizing how implement simple applications in a single language by rote, and we failed to distinguish the intellectual value of the programming mindset from the economic value of the programming vocation, so we’ve incentivized beginners to teach other beginners simple formulae and allowed them to believe that their limited understanding sets them up to be geniuses and millionaires. Now, we’re surrounded by overpaid beginner programmers writing reams of crap code, and it’s a crisis.
Writing good code is hard and takes time. Writing a single line of really good code takes years, because you need to study and practice for years before you are capable of distinguishing good code from bad code. Just being mindful in the moment of how much effort you’re putting in is wholly insufficient.
By Rococo Modem Basilisk on December 29, 2016.
The tragedy is that this feature is easy to implement if you work for Medium (or Facebook, or Netflix, or whatever) but impossible if you’re a user.
By Rococo Modem Basilisk on December 29, 2016.
Where Wizards Stay Up Late would be a great addition, but I haven’t finished it, so I didn’t feel it would be right to include it. (This is also true of The Soul of a New Machine by Tracy Kidder)
By Rococo Modem Basilisk on December 29, 2016.
Ive is very much acting in line with Jobs’ legacy.
There’s a lot of mythmaking at Apple, and it’s in some ways responsible for Apple’s recent success, but in this case it really obscures the history. Jobs was all about removing functionality in favor of form in his work at Apple, starting with his take-over of the Macintosh project at the latest. While he didn’t do this at NeXT to the same degree, as soon as he came back at Apple, he doubled down on it.
Apple has never been the “fun” alternative (if you count all its competitors, rather than pretending the only alternative to the Mac has ever been the IBM PC, which was never true). On the other hand, removing important features in order to cut production costs was pretty standard in the late 70s and early 80s, and would have been completely acceptable in the generation of machines the Apple ][ belonged to. (The Apple ][ initially was unable to display lower-case letters, in order to save on ROM; similarly, Sinclair BASIC used single characters as commands, to save space.) We owe Jobs’ and Ive’s legacy to the predictable commercial failure of the Lisa and certain cynical ideas Jobs developed about the market in the wake of that failure. The relative success of competitors like the Atari ST and Amiga despite marketing and advertising failures demonstrates that Jobs’ model of the computer industry in the mid-80s was completely wrong, but it nevertheless defines the behavior not only of Apple but of many of Apple’s competitors to this day.
The Lisa would have been a half-decent machine, if made with today’s technology. It was, however, made with late-70s technology, and very little attention was paid to performance. As a result, this machine cost two thousand dollars in 1982 money, couldn’t operate without an external hard drive of roughly equivalent price, and once operational would take minutes to perform simple operations. It was a failure, because it was an expensive and unusable machine; on a purely technical level, it was a success at being interesting, building in many truly novel ideas. After the market failure of the Lisa, Jobs kicked Raskin off the Macintosh project and turned the Macintosh into an attempt at a low-budget version of the Lisa.
(Raskin’s ideas for his original Macintosh project really were novel and revolutionary in a way that the Macintosh was not; unfortunately, aside from a short-lived attempt at selling it as ‘Swyftboard’ extension cards for the Apple ][, these ideas only ever made their way into the Canon Cat — a dedicated word processor that cost as much as a Lisa.)
Jobs decided that the failure of the Lisa was the result of high cost and low performance (which is true), but (being non-technical) he had no idea what kinds of functionalities are actually slow or require special hardware and (being an egomaniac) he made ridiculous pronouncements about performance in public and held his engineers to them. As a result, the original Macintosh had a small, built-in monochrome (not greyscale!) display, half the RAM of competitors, no multitasking, and only one mouse button. It sold for less than the Lisa, but for double the price of competitors that had double or triple the performance. Much of the development and prototyping cost actually went into repeated changes to the shape of the case.
As a result of the Macintosh hemmoraging money (despite the high margins, not enough people were buying them to make up for the famously wasteful development, and by the time the Mac shipped it was four years behind the curve on hardware) and Jobs’ habit of yelling at employees until they cry, he was replaced as CEO by John Sculley, and eventually forced to resign. (Apple under Sculley continued a Mac-centric plan, but continued to lose money; however, Macs under Sculley actually had color displays and more-or-less competitive hardware.)
For people who have short memories and only care about Apple post-1997, this may seem like ancient history and irrelevant. But, we have to recognize that during the era when Jobs wasn’t in charge, Apple pushed the Mac in the direction of being like its competitors. During those years, the Mac competed on features. There were low-budget models that cost less but had crappier hardware. For a short period in the 90s, there were licensed (and unlicensed) third-party Mac clones that could run Mac OS. (Apple didn’t compete well with the other players, but with Atari practically going out of business again due to the failure of the Jaguar and Lynx and Commodore’s spectacular mismanagement of the Amiga line, they ended up surviving a pretty tumultuous period, with their major competitors as of the late-90s being mostly competitors run by former Apple employees — NeXT and Be. Apple at this time was a little like Yahoo is now: losing money and repeatedly making awful business decisions, but holding on to enough loot from its glory days of decades before to still be an important player to consider, like a senile giant.)
When Jobs came back, he cancelled all ongoing projects and instructed his employees to start working on making a Mac OS emulation layer for NeXTSTeP. He then had a cut down version of their next-generation Mac released, in a colorful case and without a floppy drive. (Releasing a machine in 1998 without a floppy drive was like releasing a machine today that can’t display lower-case letters.) Eventually, we got a fully re-branded NeXTSTeP/BSD hybrid that could run legacy Macintosh applications, in the form of OSX, and a bunch of really bizarre case designs (ranging from something that resembles a modern tablet embedded in a brick of lucite to screens mounted like desk-lamps). (We also got a couple conventional towers, but you have to recognize that in 1997, Macs were all still beige boxes.)
In other words, Jobs’ influence on the Macintosh prior to 1986 was to drop anything resembling an interesting feature (including his famous refusal to include expansion ports on the original Mac) in favor of fancy beveling, and Jobs’ influence on the Macintosh from 1997 to 2000 was primarily to drop important hardware in favor of translucent colored plastic and drop current multi-year development projects in favor of just copying what his last company did.
(We should examine NeXT a bit. NeXT was full of people from the original Macintosh team — Jobs took the best and brightest from the Macintosh and Lisa teams with him when he left. NeXT used an off the shelf UNIX kernel, hired on the inventor of Objective C, and did some interesting technical work by mid-80s standards (we owe many of the strange behaviors and limitations of the modern web browers to the fact that Tim Berners-Lee liked the NeXT UI builder tool), hidden behind a machine that still had a monochrome display at the edge of the 90s. NeXT didn’t have the same resources as Apple did, so development couldn’t be so wasteful; the NeXTCube was a decently solid machine. Still, NeXT boxes were expensive, and the company hobbled along just like Apple did. During this era, Jobs wasn’t continuing his Macintosh habit of taking somebody else’s design, stripping features from it at random, and calling himself a genius for knowing which features to strip; but, when he got back to Apple, he did this with his own NeXT machines.)
Apple’s real success under Jobs, though, was to take existing products on the market, make clones that strip important features at random, and sell the clones for double or triple the cost of the technically-superior originals. In other words, to perform the same operation that turned the Alto into the Macintosh (by way of the Lisa) on the existing MP3 player & smartphone markets.
When Apple runs out of things to copy, it tends to either produce ill-conceived products of its own or start removing features from its own previous generation at random. Jobs didn’t have to be good at removing features: he had sufficient charisma that he could pass off all his mistakes as works of transcendent genius. But Apple is now run by his ghost, and while Apple employees are close enough to the source to have their realities warped, the rest of us have been snapping out from under the spell.
The truth is, removing features is something that has to be done very carefully. Jobs’ charisma masks the fact that nothing he did from 1979 to 1999 was a good business decision and every one of his successes are owed to his ability to persuade strangers to believe plainly false things. Without that shield, Apple can’t last very long with the same rulebook, because none of the rules ever had to make sense before.
By Rococo Modem Basilisk on December 30, 2016.
This is why I always say that if we want an idea of how life will be under UBI, we should look at the current lives of the wealthy.
Labor won’t suddenly completely dry up, for the same reason that those who are wealthy enough not to need to work often end up performing volunteer labor: the more labor is disentanged from survival, the less one is alienated from it (and the more it resembles a hobby). (Under UBI, we also won’t really need to worry about the wealthy performing jobs normally done by the poor at lower rates, since labor flexibility becomes normal rather than the privilege of the elites.)
By Rococo Modem Basilisk on January 3, 2017.
You don’t really need to go to 4chan for this.
You don’t really need to go to 4chan for this. The facebook group “Ancap Memes from Rothbard’s Dreams” specializes in these.
By Rococo Modem Basilisk on January 3, 2017.
I mean that, ideally, facebook, medium, and youtube wouldn’t be websites, because the web is a poor solution to the problem of building responsive user interfaces.
Sometime in the late 90s, we all collectively decided that all networked applications were supposed to run off port 80 and sit inside a web browser. This leads to massive engineer labor (software engineers end up spending enormous amounts of effort trying to do what would be trivial as a native app inside a web browser, software engineers spending an enormous amount of effort trying to make web browsers capable of doing everything web designers are trying to do with them without creating security flaws), massive resource waste (cycles and packets wasted en masse because nobody’s using more efficient methods, even when those methods would also be easier to implement, because crappy solutions have become the default), and ultimately we screw the pooch on security too because we’ve layered everything important on top of a simplified version of a 1992 demo intended to explain Enquire to suits instead of ensuring that our technical decisions make technical sense.
We’re locked in now. But, were we to make sensible decisions from the beginning, our hypertext systems would have permanent content-based addressing with automatic replication (instead of temporary machine-based addressing that fails to uniquely, consistently, or permanently correspond to any given piece of information), no scripting (since scripting is totally unnecessary for hypertext), no association with the domain name system (because domain names are broken), no association with the certificate chain system (because root certificates get leaked all the time), no ad-based revenue (because ad-based revenue encourages a race to the bottom in terms of content quality), and no embedded markup (external, offset-based markup is easier to implement and avoids most common markup-related problems).
As a web developer, your job is to make objectively poor decisions about web development in ways that are profitable to your employers. That doesn’t mean that you need to believe those decisions are good; you just need to implement them.
By Rococo Modem Basilisk on January 4, 2017.
I’m not kidding.
The web is good at exactly one thing: transmitting static text documents that include minimal formatting and hyperlinks. It’s not even great at that. CSS and Javascript take the easy problem that the web already mostly fails at, and turns it into a hard, complicated problem by providing poor tools for simulating native applications.
If you feel like you need CSS, write LaTeX or PostScript. If you feel like you need JavaScript, write a native app (maybe in JavaScript!). Don’t put a browser where it doesn’t belong.
By Rococo Modem Basilisk on January 4, 2017.
I think the time might be right for micropayments as a publishing monetization model.
The reason is that ad-based monetization is already micropayment-based. An advertiser estimates how much a view would be worth, and sends that much to an ad host per view. The big problem with ad-based monetization is that neither the author nor the viewer of the post is on either end of this transaction and the transaction cost has been turned into a futures market that everyone has been incentivized to game. As a result, the average impression is of approximately zero value, because the average impression is made by a bot looking at an advertisement for a nonexistent product on a site intended to farm fraudulent ad impressions.
If we associated our accounts with some money, and charged as much per view as ads pay out, then gave medium half of that (which is more than the ad host would get paid out under adsense), we would cut out a whole industry of middle-men and your average user could coast on five bucks for decades. (If new users got a dollar credit or something, it would essentially simulate a freemium model; if users had to add funds in five dollar increments, Medium could make a fortune on interest from their escrowed cash. The normal case would have money mostly stay inside the system — in Medium’s coffers — because most people who do a lot of writing on Medium also do a lot of reading on Medium. Big publications would still make bank, because they’d still get a lot of views; they’d actually make more profit, because three tenths of a cent per page view is more than they’d ever make off ads.) Medium already determines the distinction between a “view” and a “read” — so if they charge/pay double or triple as much for a “read” than a “view”, readers wouldn’t notice but authors would be highly incentivised to create high-quality content worth reading, and clickbait would disappear.
By Rococo Modem Basilisk on January 5, 2017.
I think we can tag him. Ev Williams
By Rococo Modem Basilisk on January 5, 2017.
Cyberpunk, sadly, died in infancy. What’s worse is, it was replaced by a changeling.
When I talk about cyberpunk, I’m really talking about the politically-charged visionary science fiction written by Bruce Sterling’s cohort in the late 1970s and early 1980s that combined the stylistic experimentation of the New Wave movement, the social consciousness & focus on economic injustices of naturalistic fiction, the moral complication of hardboiled/noir fiction, and the bite of the then-recently-deceased punk movement. By the time Neuromancer got published, most of the really interesting stuff in the cyberpunk movement had already stopped; the occasional really good cyberpunk after that point (such as the Ghost in the Shell manga — not the film or the show — or Akira, or Altered Carbon) were rare, because immediately upon breaking into the mainstream, cyberpunk was consumed by the Spectacle and became an aesthetic instead of a movement.
As a result, it has a really complicated legacy.
Gibson never really stopped writing about the same kind of material he wrote about in Neuromancer. He still explores the sociology of extreme economic inbalance. Likewise, Bruce Sterling is as good as he’s ever been. John Shirley & Rudy Rucker, despite being central to the group, were always doing their own thing and continued to do their own thing without really being able to be clearly categorized as cyberpunk. But, post-1992, nothing Gibson wrote was really considered cyberpunk, and Sterling transitioned even earlier.
Because it’s considered an aesthetic, cyberpunk is strongly associated with specific imagery, and the imagery is tied to various periods. People make odes to first-generation cyberpunk (prior to 1990, essentially) by fetishizing 80s tech (see Jackrabbit, for instance), and while this aesthetic is one I find only minimally problematic, it misses the underlying point in the same way that Steampunk misses the underlying point of The Difference Engine.
By the early 90s we started removing even the punk aesthetic from cyberpunk aesthetic, and cyberpunk was almost entirely politically neutralized by 2000: we associate cyberpunk often with Hackers (1995), wherein the political content is limited to vague and half-hearted anticapitalist sentiment and a cliched-by-1994-standards environmental message tacked on as an afterthought, The Matrix (1999), which gives a pretty good representation of the Spectacle but whose political content has nevertheless been completely misunderstood by a core audience who has reinterpreted it as anti-woman and anti-semitic, and Swordfish (2003), which was slicker and in retrospect smarter than all the others but extremely politically confused. The idea of a cyberpunk protagonist in 1990 was a low-level criminal with a drug addiction — a tweaker who needs a bath. The idea of a cyberpunk protagonist in 2000 was a rich white teenager with bleached hair, sunglasses, and a leather duster. By 2005, the ideal cyberpunk protagonist willingly worked for the government. Today, as the original cyberpunk works are more relevant than ever, people ignore the books in favor of movies that show pretty people in leather performing magic.
A return of cyberpunk aesthetic will do nothing for us. A return of cyberpunk sensibility, on the other hand, could be extremely helpful.
By Rococo Modem Basilisk on January 9, 2017.
I think you’ll find that web browsers can, in fact, render hand-written plain HTML.
By Rococo Modem Basilisk on January 9, 2017.
Our systems (even on relatively longread-friendly platforms like Medium) disincentivize nuance.
Our systems (even on relatively longread-friendly platforms like Medium) disincentivize nuance. They don’t need to.
Consider Medium’s ranking process, by which it sorts content for the dashboard/ feed, recommendations, and various lists. While I’m unaware of a writeup of how it actually works, it’s clear that ranking is not based purely on recency. Instead, it appears to be based upon a combination of recency & interaction (recommendations and comments) weighed by social network distance. A minor tweak that would increase the weight of full reads (and increase that weight based on estimated read time, so that an article that takes an hour to read that got five reads by people you’re following will be ranked higher than an article that takes five minutes to read but got twenty-five reads) could make short clickbait articles mostly disappear from our feeds. Instead, recommendations clearly count for much more than reads or length.
By Rococo Modem Basilisk on January 9, 2017.
Berners-Lee had a project that did many things correctly that the web did incorrectly.
Berners-Lee had a project that did many things correctly that the web did incorrectly. It was called Enquire, and it was a simplified version of some ideas from Project Xanadu. The world wide web was created as a teaching tool for explaining the concepts behind Enquire to CERN suits, because Berners-Lee had been having a hard time explaining Xanadu concepts to them.
Early web standards also solved many problems that plague the current web. If HTTP 1.1 was implemented properly, including all of the optional features, and it was used as intended, major problems like URL inconsistency would be solved. Unfortunately, important features of HTTP 1.0 and 1.1 are not implemented by any major web server, and other features are consistently used in a way that eliminates their original function.
The web as it stands now is much less than what it could have been even in the early 90s. I’m being deliberately provocative in recommending we use only the features of the web that better in the web than in other technology stacks, but the problem of using the wrong tool for the job in order to avoid learning about the right tool is real, and the web is the wrong tool for most of the jobs it does.
I’m not arguing against progress, here: I’m arguing against using poor solutions to problems for which good solutions exist. The fact that many poor solutions are newer than many good solutions is just a side effect of the widespread ignorance of history in the industry.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
By Rococo Modem Basilisk on January 13, 2017.
I always thought “borked” was derived from “borken”, the early 1960s era humorous mistyping of “broken” within the MIT hacker community. (While ESR doesn’t give a date or etymology, this entry goes back to the original Hacker’s Dictionary, which was compiled in the early 60s.) I never knew about this second, politically-charged meaning!
By Rococo Modem Basilisk on January 13, 2017.
Ramifications of nearsightedness in tech
Ramifications of nearsightedness in tech
A tool that is used for everything will necessarily be a poor fit for most of the tasks for which it is used. In theory, a vice grip can be used as a hammer, screwdriver, or wrench; a mechanic who uses a vice grip for all these tasks does not inspire confidence. Somehow, this is not the case in the tech industry — whose vice grips include the web, hadoop, java, and c++.
Language wars aren’t totally pointless: it’s useful to inspect and document what tasks various tools are good at, and how best to use these tools. However, language wars at some point became a matter of tribalism, and tribal defense of the idea of one language over another for all tasks benefits no one — with the exception of snake-oil salesmen who want to sell you the panacea for all your programming woes. Such confidence artists exercise a lot of power in the industry these days.
One would think that common sense would prevail. After all, performance, man-hours, and downtime are all measurable. But, even as more and more is being automatically measured and recorded, connection to reality is increasingly tenuous: “unicorns” worth billions of dollars nevertheless have no plans for monetization, companies are created with the goal of being sold instead of providing services, and the fortunes of most players are determined arbitrarily by small gambling rings called venture capital firms. Everything’s a web service, and to the extent that they are monetized at all, they use ad-tech — a mutated version of microtransactions where an arbitrary corporation pays a fraction of a cent to a web host in return for an http request, with the value of that fraction determined by an uninformed estimate of how likely that request is to result in a product sale, and wherein that estimate drops endlessly because most requests come from robots incapable of purchasing products. As a result, the most highly valued properties are ones that no one wants, and fashions wholly disconnected from popular desirability sweep the industry. The tech industry strongly resembles modern Hollywood: dominated by expensive flops, and under thrall to its own marketing due to excessive insularity.
Another thing the tech industry strongly resembles is a pyramid scheme, or a cult.
At the very least, our industry is amusing, in the way that Sanatorium In the Sign of the Hourglass or The Penal Colony is amusing. Like most things whose value is based on faith in ideas that don’t track with reality, it probably won’t last in this state for very long. We’re in the Wiemar Cinema era of this industry, probably: Murnau can’t keep making films like Nosferatu if he binges on opium every night.
I’m not really concerned with the delusions of the elites, though. Peter Thiel and Paul Graham can make grand pronouncements, but they wouldn’t even notice if nobody on the ground listened. My major concern is that naive, lazy ideas dominate the industry even within the entry level. I attribute this to problems in education.
We have a bunch of mythology in the current industry, but it bears little in common with the earlier body of mythology and it teaches the wrong lessons. It elevates venture capital, lucky charismatic sociopaths, and companies whose value is wholly imaginary. It justifies using one tool for everything (via Paul Graham’s adulation of Lisp, didiactic standards for college curricula that suggest everything should be a Java class, the normalization of bootcamps that push you through a tutorial for a single language in a few weeks with the promise that you’ll be ready for employment). It’s the kind of mythology that cocaine-snorting ad men from the 80s would invent, because that’s who invented it.
When we eschew real history in favor of hero-worshipping Steve Jobs, we are eschewing real knowledge in favor of ad copy and flavor text. Our lack of familiarity with history allows us to imagine that the web is the way hypertext is supposed to work, putting an RDBMS on top of hadoop makes sense, PHP is an acceptable language, and new fashions in the industry are world-shaking innovations. It lets us imagine that Slack is a major improvement over IRC, Uber is the underdog fighting against foolish overregulation, smart phones are the future of computing, and hard work will make you a millionaire.
New, trendy ideas in tech are typically poorly-understood ideas from the 1970s. The flaws in these ideas that will cause everyone to abandon them in six months were published in the first response to the original paper, if not in the original paper itself. Save yourself some time, and read CS publications from the 70s. When the rare genuinely new idea appears, you will be uniquely suited to understand it.
By Rococo Modem Basilisk on January 13, 2017.
Hiring is a whole different mess.
Hiring is a whole different mess. I appreciate the problem of necessary yet bullshit credentials. (I learned a lot from college, but very little of it related to computer science, and while I owe my current job to college, it’s because a member of the faculty used a personal connection to recommend me for an internship. So, I would have done almost as well with a fake degree.)
I have other rants on hiring, having been involved in interviews on both sides of the table several times. It’s broken, in a different but related way.
Ideally, we’d consistently have technical people interview technical hires, remove bullshit credentials as requirements, and end up undercutting the industry in bullshit credentials while simultaneously improving the average quality of worker in the industry. I don’t think that’ll happen until after the next major tech industry crash, though.
By Rococo Modem Basilisk on January 14, 2017.
Medium started out with the goal of fixing clickbait, and the (correct) understanding that an ad-driven revenue model encourages clickbait / low-quality content.
Somewhere along the way, it decided to support ads anyway, and while early design decisions worked against clickbait, later design decisions didn’t. (As a result, everyone’s feeds got worse.)
I welcome Medium’s return to their original goal, and if the people who were laid of were contributing to the downfall of the platform, I’m glad that they are no longer in a position where they can make things worse (although it would be naive not to put the ultimate blame on top staff, and probably ultimately VCs). However, it’s sort of alarming that Medium isn’t aware of the many reasonable alternatives to ad-based revenue & paywalls, after so many years.
‘Finding’ an alternative to ad-based revenue implies that one isn’t already in front of you. In the industry there’s this idea, probably borne out of cognitive dissonance, that ad-based monetization is a necessary evil; this has never been true, since alternative proposals predate the advent of banner ads by 30 years.
By Rococo Modem Basilisk on January 17, 2017.
Medium started out with the goal of fixing clickbait, and the (correct) understanding that an ad-driven revenue model encourages clickbait / low-quality content.
Somewhere along the way, it decided to support ads anyway, and while early design decisions worked against clickbait, later design decisions didn’t. (As a result, everyone’s feeds got worse.)
I welcome Medium’s return to their original goal, and if the people who were laid of were contributing to the downfall of the platform, I’m glad that they are no longer in a position where they can make things worse (although it would be naive not to put the ultimate blame on top staff, and probably ultimately VCs). However, it’s sort of alarming that Medium isn’t aware of the many reasonable alternatives to ad-based revenue & paywalls, after so many years.
‘Finding’ an alternative to ad-based revenue implies that one isn’t already in front of you. In the industry there’s this idea, probably borne out of cognitive dissonance, that ad-based monetization is a necessary evil; this has never been true, since alternative proposals predate the advent of banner ads by 30 years.
By Rococo Modem Basilisk on January 17, 2017.
The command line and GUI replaced voice as an interaction model decades ago, because for most tasks it’s easier to do it yourself than tell a secretary to do it for you. (And make no mistake, much of the time secretaries were more intelligent and capable than the people who commanded them.) Until the advent of weakly superhuman artificial general intelligences, the absolute best case for voice and natural language as an interaction model will be strictly worse than instructing a human subordinate.
The sorry steady state of most large organizations (the SNAFU) is the direct result of the flaws of natural language as an interaction model. As an organization becomes more close-knit and efficient, jargon proliferates and language becomes more exact for things that matter, until what people speak to one another resembles one of the less well-thought-out programming languages (perl, php, basic, fortran). In other words, the end goal of any natural language interface (even between humans) is to become a command line interface.
Of course, between humans, the learning curve is hidden because it is merged with the organic emergence of a shared cant. But, this hiding of the learning curve can only be performed once: people who come later to a community must learn the language of that community. Why not standardize, when we can? After all, machines aren’t very good at improvizing collaboratively generated vocabularies based on implication but are very good at consistently interpreting the same conventions the same way (and copying those conventions exactly to other machines).
By Rococo Modem Basilisk on January 17, 2017.
For Medium, the most sensible monetization policies would probably be:
• microtransactions from readers to authors (where Medium takes a cut whenever anybody wants to cash out, & otherwise holds cash in escrow). (This is essentially what Adsense, Mechanical Turk, and Second Life did.) • pay for privacy (a variation on freemium used by github, wherein normal use is free but paid accounts can whitelist or blacklist posts to specific audiences — making it possible to treat medium like a private forum)
Users of Medium can already use something like patreon, but Medium could potentially benefit from integrating something like SPP (an ancestor of patreon) and taking a cut. In SPP, a work is created but only released to the public when a certain threshhold of funding/donation is passed; since Medium is a publishing platform, they are in a unique position to hold a piece of writing in escrow until the threshhold is passed. (I like SPP in concept, but of all the variations I cover in my overview piece, it’s the only one that hasn’t really been properly implemented, and the idea may be a little too alien to most users.)
Medium already uses a freemium model to some extent, with publications having greater control of color and layout. This by itself won’t keep them afloat, though. I think if they kept publications as their freemium model & added microtransaction support (maybe even just as an author option in the licensing menu), they could do significantly better than they would with advertising.
If full support for microtransactions is too scary for them (since it would require casual users to put money into the system), another possibility is to integrate submission payment into Medium itself. Some publications pay for submissions, but perform draft submission & editing via Medium itself; the only portion of the process that’s done outside of Medium is the payment. (I’ve been paid for submissions and had a ~4 month wait between publication and payment because of overseas wire transfers.) If publications are already paying into Medium, giving users all an account balance & building support for basic publication agreements into Medium itself would actually streamline the process enough that Medium could justify taking cuts comparable to what PayPal takes. (And, while Medium holds on to all that borrowed money they can keep the interest that accumulates.) Doing this might be a first step toward supporting microtransactions on a larger scale.
By Rococo Modem Basilisk on January 18, 2017.
I’d definitely say the web is a bad tool for most of the things we use it for.
I’d definitely say the web is a bad tool for most of the things we use it for. In particular, the technique of using javascript sitting within a document to dynamically edit that document’s structure & CSS attributes is a particularly inefficient way of building a user interface, and without the heavy optimization being done by browser maintainers any such application would be unusable. (As much as I dislike Java, I have to admit that applets are actually the appropriate solution here.)
(Let me say that I like and appreciate that you *can* perform hacks like dynamic web pages; I’m just horrified that so many people think you *should*. Dynamic web pages are, essentially, self-modifying code. Anywhere else, we would be very wary of that.)
HTTP specs provide response codes that address some of the problems related to addressing documents via their position on machines. (Doing this rather than using CAN is a mistake in the first place, but TBL was working with a slow network before ideas about CAN were very widespread.) If we were to try to implement up to the original spec & use that spec to get as close as possible to traditional pre-web hypertext best practices, we would make the content of each page static (no CGI), keep track of where every document is at all times, use temporary and permanent redirect response codes if a document moves, never have two different documents (or a modified version of the same document) with the same full URL, and only use a 404 return code in the apocalyptic case that somebody simultaneously nuked your server and all your backups (or in the case that a document *never existed*). Today, these codes are not used that way.
(HTTP also specifies some very useful functionality that is very rarely implemented. Specifically: HEAD requests to get the length of a document and special variations on GET requests that return a span of bytes, given a starting offset and length. Such features are very useful in an environment where documents are static but potentially very long. Having worked on code intended to perform transclusions from arbitrary URLs, this feature would have drastically simplified my efforts.)
Right now, the most promising competitor to HTTP is IPFS — a CAN-based file transfer system. It doesn’t solve the problem of servers going down permanently (there is no automatic redundancy), but popular files will outlive short outages because requested files are served from cache (which also saves bandwidth upstream). IPFS avoids CGI-style hacks, but you can still serve arbitrary HTML/Javascript blobs, so it’s not as though it breaks that functionality. (Again, I consider using javascript to manipulate HTML messy and slow.)
My main complaint with web apps is that very few things benefit from being on port 80 & being stuck inside HTML. An applet, because it’s essentially a native application that has been sandboxed, doesn’t have the problem of needing to draw using drawing routines intended for dealing with messy hand-written markup, nor does it have the problem of working against a set of quirk rules organically grown to protect against 20 years worth of random but very specific attacks on web browsers. An applet can use standard protocols, instead of depending on services that are exposed over HTTP wrappers. (HTTP-exposed APIs take the rule-breaking of CGI — which, remember, discouraged people from using redirects properly and led to the current user-hostile behavior of redirects; they are additionally generally less efficient than existing standard protocols.)
Applets have gotten a bad name, because most applets are either Java or Flash, and both of those platforms have a history of being quite awful. But, there’s no particular reason that an applet viewer for javascript, or lua, or smalltalk couldn’t exist, provided that someone decided on a decent standardized drawing model that properly supported all the various things people like to put in GUIs.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
By Rococo Modem Basilisk on January 18, 2017.
The Value of Subtle Communication: Toward a Secular Materialist Model of Chaos Magick
The Value of Subtle Communication: Toward a Secular Materialist Model of Chaos Magick
[1]
Where occult traditions typically anchor themselves firmly to the metaphysical, chaos magick has historically been open to more materialist interpretations, although inconsistently so. I do not claim that metaphysical understandings of magic are necessarily wholly invalid, but I would like to suggest that most if not all of the major practices associated with chaos magick do not require or benefit from appeals to ideas outside the scientific mainstream. In other words, the practices of ritual (particularly sacrifice) and sigilcraft (including hypersigils, etc) are fully compatible with a conventional scientific, secular, materialist worldview.
The cornerstone of my argument is the idea of subtle communication with oneself and others as a means of influence.
[1]
By subtle communication, I mean: communication that bypasses conscious, verbal modes of understanding. This is not to say that such forms of communication have no verbal component; only that the most important aspects of these communications are not, to the target, immediately understood in terms of the most obvious and direct interpretation. Subtle communication in the domain of writing includes subtext, wordplay, irony, symbolism, oblique reference, the literal content of metaphor, and emphasis; in speech, we can also consider changes in meaning and emphasis created by timing and modulation; when speaking to someone in person, facial expressions and body language constitutes high-bandwidth subtle communication mostly hidden from us but transferred by unconscious mirroring behavior. Regular speech can become subtle communication if the target is unable to distance themselves enough to objectively analyse it, either because it becomes internalized before it is fully understood (as with song lyrics), or because something is interfering with verbal processing (as with the repetition of creeds or mantras — jamming the verbal processing facilities — or when the target is tired or under the influence of various drugs). Subtle communication has lower bandwidth and a greater error rate than overt communication, and it is often missed or dropped, but it also bypasses many of the mechanisms that might work against some goal, which makes it an ideal way to circumvent forces that would work against a more direct attempt.
Seen in this way, sigils (when directed at the self) operate a bit like affirmations. A sigil is a reminder of an intent, but one that is oblique enough that it avoids triggering self-defeating behaviors. When sigils are directed at others, they retain less of their meaning, but some subtle communication is still achieved (with the amount depending upon the quantity of shared understanding and assumptions between the creator and the target). We can expect only a small effect from showing our sigils to others, at best, unless we give them enough context that it is comparable to their having made the sigil themselves; if we share enough culture with the others, however, subtle communication from them can reinforce our own goal-seeking behavior, like a smaller-scale version of publicizing one’s to-do list or new year’s resolutions. When such pressure is small and subtle, it doesn’t overwhelm.
Rituals abstract our goal in much the same way that sigil creation does. The pressure is higher because of sunk cost: rituals are costly signals to the self about the importance of achieving some goal. The time and effort turned over to a ritual is itself a sacrifice (and one that is properly set in our minds, as opposed to a pathological case like gambling wherein sacrifices are disguised as investments or amusements), and an actual sacrifice increases the cost. Group rituals bind the group together to a common goal: they all made the same sacrifice, and would like to avoid the cognitive dissonance of having wasted it, so they must achieve their goal in order to justify their sacrifice.
Books like Mind Performance Hacks produce rationalist-friendly “life hack” versions of these practices, with the occult terminology stripped out. Rather than sigils, print out a sheet with affirmations or themed words in order to encourage particular primed responses! Rather than rituals, make a betting pool with your friends about the success of some project, or vow to donate to a charity you hate if you fail! By embracing the verbal mind, these practices open up the door to endless second-guessing, and thus to self-sabotage. I would recommend the chaos magick versions of these practices instead. Self-sabotage seems likely, particularly if you don’t respond well to direct pressure.
With hypersigils, the mechanism of action is even easier. People model their worldview mostly on art: experiences provided by art are easier to find and consume than experiences provided by life, and are also safer. A hypersigil takes a naturalistic view of the world and adds elements to it that encourage emotional investment, and then slowly modifies it in ways that correspond to some intent. The result is that the creator and audience both have their world view modified by the creator’s intent. When the audience has adjusted expectations, it becomes more likely that they will manipulate the world to fit those expectations, as well as communicating their expectations to secondary audiences.
By Rococo Modem Basilisk on January 18, 2017.
As much as I’m loathe to heap praise on Second Life & Linden Labs, I think they at least got this model right: they handled transactions between users that would be occurring anyway, and then allowed people to put money into the system & take it out.
When a user views a web page, a transaction implicitly occurs. Medium knows about both sides of this transaction. (With ads, the transaction is offloaded to one between two unrelated third parties.) It’s not unreasonable to make such a transaction explicit and broker it without third parties. If everybody starts off with enough credits to handle the whole lifetime of a very casual user, then the monetization will never affect normal users, which is a big hurdle.
Flattr is a lot like paypal donate buttons & patreon, in that it’s an add-on that individuals put on top of a service to provide monetization the service doesn’t provide by leaning on the good will of individuals. Such systems sometimes work well, but you have to lean pretty hard on the NPR model of begging for donations in order to use them.
If you already manage the content distribution system, allowing casual users to automatically pay money they didn’t know they had (with the knowledge that most of that will never fully circulate and that which does circulate can be more than made up for by minimum balances for cashing out & fees) lets you put the system in overnight without anybody noticing or caring & then only charge very heavy users. (If the costs involved are small enough — and if they are on the scale of ad revenue, they are tiny — heavy users will not feel like it’s a burden on them to continue using the platform.)
By Rococo Modem Basilisk on January 23, 2017.
If we can do this (and it’s certainly technically possible), we can break siloization by doing the opposite: specifically selecting and recommending stories from non-intersecting social groups with related tags.
Personalization is great at giving people what they want, but when it comes to giving people what they need, we aren’t using all the tools at our disposal.
By Rococo Modem Basilisk on January 25, 2017.
This fits into a wider pattern.
When the creators & consumers have aligned worldviews & interests, allowing creators to perform cognitive labour on behalf of consumers makes sense. When their interests are not aligned — when the media or technical landscape is adversarial to the users — then any simplifying assumptions made by creators are at best ideologically suspect.
Many of the major growing pains related to the internet (and particularly the web) essentially come down to an “eternal september” moment where a set of technologies designed for hobbyist use in a a community with relatively aligned interests gets inserted into a commercial context where multiple adversarial parties are involved. (Spam, clickbait, fake news, intrusive advertising, all manner of security problems ranging from social engineering to sql injection, ‘dark ux patterns’, 419 scams, and trolling can all essentially be blamed on giving a system designed for good-faith cooperation to a bunch of people who would rather con each other to gain small advantages.)
Societies have an array of tools for limiting the damage done by bad-faith actors. Unfortunately, the cruel optimism of the people who design online communities either undercuts these mechanisms (in reality power structures are very conditional, since subordinates who lack trust in their superiors’ judgement will ignore orders; in computer systems, power structures are treated as much more cut and dry, compounding mistakes made by the powerful) or expands their power beyond what is reasonable (as with the human flesh search engine & other mechanisms that pile on shame out of proportion with the original failure).
Recently I’ve seen what looks like an upswell in the general understanding that the world is complex & can’t be easily understood or modeled, which makes me a little more hopeful for the future. However, easy solutions (even if they are wrong) can be very lucrative. Any system we design should be mindful of how it presents information & how that presentation effects society.
By Rococo Modem Basilisk on January 30, 2017.
Agreed: lack of perspective is the real M.
Agreed: lack of perspective is the real M. O. here, at least with people who have theoretically aligned goals.
I think capitalism creates a big persistent bias, though, because it’s one way in which the value of transmitting information can be made wholly independent of the truth or utility value of that information for the recipient: the entire practice of advertising consists of aggressively telling half-truths for the sake of causing people to behave in ways that are against their interests, and that’s probably the simplest case. When you add futures markets, you begin to have situations where you can make a great deal of money by moving information around that has no relationship at all to reality.
People can learn to consider others whose experiences are unlike their own, but they are far more motivated to put in the effort if their livelihood does not depend upon preventing those people from succeeding.
By Rococo Modem Basilisk on February 3, 2017.
Belief is the enemy.
(It would be nice if information systems were not actively antagonistic to most people. This is not the information landscape we live in. I blame capitalism.)
By Rococo Modem Basilisk on February 3, 2017.
While large corporations have the resources to do large, cross-cutting messages, the profit motive severely limits how effective they can reasonably allow themselves to be. Ultimately, a corporation cannot take a strong stance unless they are confident that most of their customers already agree with that stance — to do otherwise would be suicidal — and so they are obliged to lag behind the rest of society, tending to be more conservative & to accept regressive ideas for longer simply out of fear of lost profits.
Of course, they can avoid the risk by micro-targetting advertising & making sure they always preach to the choir — but then, they can never have a positive social impact (and may actually have a net negative social impact).
In other words, the profit motive inherently conflicts with the arc of history. (Social progress depends upon upsetting the comfortable and comforting the upset — in other words, on sacrificing the favor of the powerful in order to redistribute power more equitably — and this means real negative impact on one’s power and wealth in the short term with no guarantee of greater power in the long term.)
Untargeted advertising, for now, does cross boundaries. But, untargeted advertising reaches fewer people than ever. Large events with huge audiences like the superb owl are the exception, not the rule: who even has a TV anymore, or a cable subscription, or purchases paper magazines?
It is possible, for someone with plenty of resources and an interest in social justice, to engineer targetted advertising intended to change the opinions of specific audiences — not by having a shared ad experience that poorly targets everyone & tries to make a social statement while also shilling for a product, but by addressing each niche in terms they understand.
Unfortunately, the only values shared across most of the world now are the ones that all advertising implicitly shares: the idea that money can be traded for happiness, and the idea that there is no alternative to a system based on the sale of labor.
By Rococo Modem Basilisk on February 6, 2017.
On the web, size matters
On the web, size matters
The web has a problem. Most web sites (weighted by volume of traffic) are made by and for wealthy able westerners with fast computers and fast connections, and are borderline unusable by anybody outside that group. What makes this a problem is that these websites are inaccessible for stupid reasons.
Web designers have adopted a cargo cult programming mentality. While cargo cult programming in real languages mostly just makes code hard to read for other programmers (idempotent imports, shared libraries, & the removal of unused symbols limit bloat at runtime), on the web bloat accumulates quickly. We use third party pixel trackers for analytics (often several different ones), CDNs for displaying static text (and RDBMSes for storing static text), CSS for styling (and JavaScript for modifying the CSS, and JavaScript for modifying the HTML, and JavaScript for modifying the other JavaScript), and we use automatic generators for building structures that would be less effort to write by hand. We force styles and behaviors on users based on our large screens and fast CPUs and hoards of RAM, and the users (if they are sufficiently savvy) fight back with extensions that chop out portions of our websites based on lossy heuristics.
We don’t need to be at war with our users, and shouldn’t be. Rendering a blog post shouldn’t involve twenty HTTP requests, a bunch of JavaScript, multiple draws (as new styles override previously loaded ones), and downloading as much content as the original Doom. Choosing bloated cargo-cult methods are, essentially, discrimination: discrimination against anyone with slow internet (i.e., most of the world) or slow CPUs (anyone who didn’t upgrade their computer in the past five years — i.e., the middle class). Too much forced style information (or too many widgets and sidebars) constitutes discrimination as well: against anyone who is blind (and thus must listen to every label and alt-tag in the order in which they occur in the original HTML) or has poor vision (and thus requires higher contrast and larger fonts — ruining any kind of overly-precious color or layout dickery). Fancy style and layout is an art, and has its place, but its place is in print magazines, where style doesn’t actively defeat usability.
It’s ultimately up to you, as a web designer, whether or not you want to exclude these groups. (Many things developed in the Valley are ultimately absolutely useless outside the Valley, and people often have no problem with that: it’s where the money is.) But, if you think reaching a sufficiently wide audience is worth making the occasional design snob feel scandalized, here are some tips:
• Avoid third party trackers/analytics. They are selling you information from their access log, and you have your own. Processing your own access log saves every user an extra request. • Don’t host ads. They won’t make you money anyway, and each one means at least one extra request — usually more. Savvy users will block ads, and less savvy users will thank you for not hosting them. • Where possible, use static HTML. Static HTML is small and fast; CDNs are big and slow. If your site is entirely static, you can use a specially priced plan from a web host that doesn’t include RDBMS access or server-side scripting. (On top of this, static HTML associated with consistent URLs will be properly cached by browsers — meaning that repeated views will not lead to repeated requests. If you pay for bandwidth, this saves you money.) • Minimize style. CSS takes lots of resources to process. Fancy CSS still isn’t consistently rendered across browsers, and is likely to break spectacularly if you drastically change scale or selectively override certain elements (such as font sizes). External CSS, while more flexible, also requires extra requests. If you stick to one small external CSS file — or better yet, avoid using any styling at all — your users will save bandwidth and rendering time. • Fallback gracefully. Under load (on client, server, or network), you can expect JavaScript or CSS to fail to be processed: it might fail to download, or it might take too long to render. Some users can’t make use of it at all (for instance, blind users with screen-readers) or disable it for performance or security reasons. Constructing a website that looks and behaves as close to correctly as possible when only the HTML has loaded will make these users feel confident in your work; a website that looks broken without CSS or JavaScript seems unreliable (and CSS and JavaScript will fail). • Use images only when necessary. Most images used in the design of websites (and even many images used in post bodies) are for primarily aesthetic purposes. However, image loading should not be expected to be reliable: after all, each image is yet another request. In headers and sidebars, consider using text & minimal formatting rather than images, particularly when usability would be more impacted by the failure to load images than it would be by having text in the first place. (Again, some users will have sporadic image loading failures, and others will simply never see the images at all.) In posts, consider whether an image serves a real purpose: would you make your point better if you took more care explaining it? An image should only be included in an informational blog post if the amount of text it would take to adequately explain its content would be larger than the image itself. • When possible, write your website by hand. Generators can save programmer time when doing something fancy, but fancy websites are fragile and generators can create very bloated code. It’s not hard to write simple HTML and CSS, and writing sites by hand discourages bloat. A website written by hand by one person will, generally, be small enough to load quickly on even a very poor connection. (If you require headers and footers, or if your content is highly structured and repetitive, I recommend writing your own specialized simple generator, rather than taking some off the shelf templating engine or CDN and configuring it. A three line shell script can produce small, fast, reliable, and beautiful websites in a way that large systems like WordPress struggle to, and even a beginner programmer can write such a generator.) • Write for usability, not for looks. A visually impressive website is rarely a usable website, because the concerns are very different. Unless your target audience consists solely of design snobs, you are better off making sure your site loads quickly and is easy to use. Don’t be afraid to make it visually uninteresting: your users will thank you for making navigation easier. • Keep it simple. Introducing new toys is always tempting, but those new toys interest you much more than they interest the users. A fast-loading site that does what it’s supposed to and nothing more will be more useful than a slow site that performs flashy but unnecessary tricks.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
By Rococo Modem Basilisk on February 13, 2017.
If you are selling your site to design hipsters, I have no problem with the bloat: they have plenty of money and plenty of bandwidth. If your site is intended for anyone outside that community, usability is paramount. Nobody leaves a useful site on the basis of looks, but they absolutely will leave a useful site on the basis of bloat; if looks are the determining factor for your site’s traffic, then it doesn’t serve a useful purpose.
A piece of JavaScript that loads a link to a properly-made site once it has detected that the bloated version has failed is yet another piece of bloat that can be expected to fail. Why not default to the clean site and provide a link to the fancy one?
It’s perfectly possible to build a clean, good-looking site without relying on large amounts of complicated CSS and JavaScript. Certain behaviors are not possible without these mechanisms, but these are behaviors that should be avoided when possible because they cannot reasonably be expected to work for most users.
By Rococo Modem Basilisk on February 14, 2017.
The religious right is a strange beast, with strange internal contradictions mirroring the greater alt-right that it’s beginning to be subsumed into. Broadly, we can consider Trump supporters to be a diverse and internally schismatic temporary coalition of authoritarian-leaning pro-corporate nationalists, who agree on very little else. (After all, this group includes the religious right — who combine a shallow reading of already-shallow objectivism with the prosperity gospel and consider success in business to be equivalent of the mandate of heaven — and techno-libertarians — who consider business acumen to be an unbiased representation of general skill and promote it as a welcome break from all that religious crap — and neoreactionaries — who would like to import the authoritarian hierarchy of an idealized corporate structure into government — and accelerationists — who want to be as capitalist as possible so that they can bring about a marxist utopia by using up all the capitalism. None of these groups quite fit properly with normal white nationalism, nor with any of the various flavours of fascism going around, nor with the military-industrial complex, although all these groups are scratching each other’s backs for now.)
By Rococo Modem Basilisk on February 14, 2017.
The web is a glorified marketing platform, sure, but do we want it to be?
I certainly don’t — even if engaging in this kind of thing made me a lot of money (it doesn’t, and won’t, because marketing is a race to the bottom and supplying free services as a loss-leader for projected sales of products users will never buy is not a sustainable business strategy), I would not engage in it.
A site that doesn’t provide something useful for its end users doesn’t need to exist and should not exist, even if it makes money for its owner. A feature that is not useful for end users does not need to exist and should not exist, even if it enables the developer to extract capital more efficiently. The fact that most of this money is essentially hypothetical anyway — based on fantastical projected earnings from unsustainable practices (advertising among them) — makes the entire project much more dubious.
If you’re not getting visitors at a rate comparable to Boing Boing, you aren’t making money from advertising (you’re just handing ad money to Google in escrow, which you will never be allowed to extract), so you will never benefit from practices that were standardized based on an ad-based revenue model. You will therefore be not only better-liked by users but actually more financially successful if you pursue an alternative means of extracting capital. (For instance, diving for change in other people’s sofas.)
By Rococo Modem Basilisk on February 14, 2017.
Details of how this subscription model will work seem scarce.
Details of how this subscription model will work seem scarce. I can hardly make a decision based on them.
I love Medium right now. I love writing for it and I love reading it (despite some problems with keeping content quality consistently high). But, the service Medium provides isn’t one that’s difficult to implement: I expect that, as soon as Medium locks itself up, five or six slightly-crappier competitors will pop up from people who didn’t take ill-advised loans from VCs and who decided on a revenue model early, because we’re essentially talking about a blogging platform with minimal customization, and reinventing that wheel is done all the time. Whatever model Medium decides upon will need to benefit me more than switching to a competitor will.
Let us consider some possibilities:
• It may be that Medium will require subscriptions from all registered users, and not provide any revenue redistribution. (This will probably eliminate some casual users and change the cost-benefit analysis of people who currently use the platform for marketing purposes. If this gets rid of low-quality self-help spam, it may be a good thing, but if it scares away people who are currently writing high-quality content — as it probably will — then that may lower the value of a subscription even more.) I’m assuming, under this model, that unregistered users may still read medium posts, but that recommendations, posts, and comments are premium. • Perhaps everyone pays for subscriptions (as in the previous possibility) and there’s no paywall for readers, but authors get some direct payout (from subscription fees) based on number of reads. This would make a lot more sense: Medium is just a platform for distributing user-generated content, and making the users pay to generate content is nearly offensive, but making Medium a subscription-based marketplace for content cuts down on low-quality/spam posts and brings back authors who write high-quality content & are likely to more than break even on what they write. (There are some complications here. Like, DMCA takedowns become a lot more important, and fraudulent takedown requests might start becoming an issue. What happens if a user from Australia posts an extensive quote from Lovecraft, whose works are public domain in Australia but under copyright in the US? Medium presumably already has good lawyers and policies on this, but the stakes are higher when you’re paying authors.) • Let’s say that not only do registered users subscribe, but readers are also shown a paywall (perhaps after a certain number of monthly reads, like on newspapers). This is basically a no-go: the number of people who would put up with this is tiny, and discovery and PR goes down the toilet. Unless Medium officially states that this is what they’re doing, we should assume this is a straw-man version of their model (even though theoretically intelligent people like those in charge of the New Yorker also do this). • Consider a freemium model with multiple tiers. The free version would be what we normal users are used to; higher tiers are optimized for publications, with features like configurable publication pages and configurable post themes, promotion in the newsfeed, and placement in a special list in the sidebar. This might work out for Medium, and might work out better than the various alternatives, if they got the prices right. (After all, some publications are funded essentially with marketing budget for something else in print — How We Get to Next, for instance — and they would be willing to pay a pittance to look better on Medium because their revenue stream is from elsewhere.) But, if the prices are too low, current spammy posts will dominate everyone’s feeds (and regular users will leave), while if the prices are too high, Medium will continue to not break even.
I think subscriptions are the second-weakest business model (after advertising) for Medium, but there are ways to make it work. If they make it worth my while, I’ll subscribe. But, for the current levels of content quality, I probably wouldn’t pay more than a dollar a month.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
By Rococo Modem Basilisk on February 15, 2017.
Has this only been rolled out on mobile? I don’t see anything like this on the desktop site.
By Rococo Modem Basilisk on February 16, 2017.
If you want to get a job in tech & be qualified for it, you need to first be self-taught and then get a degree. Without being self-taught you will not be competent (because a four year degree program does not and can not provide the prerequisites for competence in a field like this), and without a degree many (though not all) businesses will not consider you.
By Rococo Modem Basilisk on February 16, 2017.
When the best option for providing a service is awful, that indicates an opportunity for competition. (Of course, network effects make this harder in practice — though Facebook was able to defeat MySpace which defeated Friendster basically all on the strength of superior UX.)
As much as I hate the term ‘disruption’, if there’s one thing that can and should be disrupted, it’s something like LinkedIn: i.e., a dysfunctional system that performs morally dubious activities, backed by an ancient giant, that continues to be used essentially because no superior competitor exists but barely provides a useful service to anyone except recruiter-spam companies. As long as places like LinkedIn continue to exist and continue to be actively used, their worst habits are normalized. And, normalization of bad ideas is the death of most things on the web.
Here’s the thing about the web: everything, service-side, is incredibly cheap compared to other industries. (Except engineer time, I guess, but that’s systematically overvalued in a weird way, and part of the problem.) Bandwidth is next to free, compared to sending out the same amount of information by mail or telephone; storage is next to free, compared to keeping the same amount of information on paper and renting warehouse space for it.
As a result, we’ve allowed ourselves to get into lazy habits and rely upon really marginal ideas for much longer than is justified: the money keeps flowing, whether or not we screw up. So we keep doing things that never made sense in the first place, like ad-driven revenue for websites, using super wonky / insecure tech (php, perl), using bloated storage formats that are difficult for humans and computers to read (300GB XML blobs), and coding as though performance and size don’t matter.
It’s been more than ten years since Moore’s Law stopped, and high speed internet providers have been avoiding upgrading their infrastructure in favor of consolidating local monopolies for at least as long, so the shit is about to hit the fan on performance across the board; meanwhile, gaming ad-based revenue and breaking poorly-secured systems have graduated from hobbies to industries, so the shit is already hitting the fan on that.
We could never really afford to do what we’re doing, but now we can’t even afford to look away.
By Rococo Modem Basilisk on February 17, 2017.
One way to avoid people using eponymous laws to pull a bavarian fire drill on you is to produce a list of equally pithy opposite positions & credit them to arbitrary famous figures. Neutralize the though-stoppers.
By Rococo Modem Basilisk on February 20, 2017.
The only response I can think of for this is that, while Putin almost certainly wants to think of himself this way (being ex-KGB and all), true Xanatos gambits aren’t possible in real life. Every plan has holes, because there’s too much complication.
In other words, at the very least, we’re looking at a combination of planning (particularly, the construction of double-binds), improvisation, and luck. And, it’s not as though double-binds of this type are impossible to engineer: it doesn’t take a strategic genius, or someone with near-total political power, to produce them.
(The other thing to note is that the weak spot in any strategy is randomness. This isn’t to say that randomness will always win, but instead that any system will eventually succumb to fuzzing if the fuzzing can be sustained for long enough. Placing someone unpredictable in power is dangerous simply because you never know how they might undermine your plans — but that risk is spread out across all possible plans.)
By Rococo Modem Basilisk on February 20, 2017.
The hype around javascript is representative of a more general and widespread problem, which is that inexperienced programmers make poor design decisions based on familiarity and ease of initial development. Javascript is popular for exactly the same reason that PHP, perl, and java are popular: it’s an extremely common first language, an extremely common second language, and there are certain kinds of tasks for which it is not poorly suited as a prototyping language. People use javascript for the same reason that they use HTTP: it’s the first thing they think of, and they don’t bother to analyse whether or not it’s an appropriate tool.
By Rococo Modem Basilisk on February 22, 2017.
Did somebody unload a time machine from 1998 into this comment thread?
Did somebody unload a time machine from 1998 into this comment thread? All serious development is done in open source, while proprietary code is at best a toy — and this has been the case for decades.
By Rococo Modem Basilisk on February 23, 2017.
Here’s a heuristic: if a task is made significantly easier by a framework, it shouldn’t be done in the browser (and probably shouldn’t be done in JS at all).
By Rococo Modem Basilisk on February 28, 2017.
I’m not familiar with anything Nelson might have written on the storage of navigation histories beyond inventing the back button (though I’m very familiar with his work with versioning & with bidirectional hyperlinks). Do you have a URL for what you’re referring to?
By Rococo Modem Basilisk on February 28, 2017.
I’m not the author of the original piece.
By Rococo Modem Basilisk on February 28, 2017.
A modest proposal related to firearm safety
A modest proposal related to firearm safety
There are several major related social problems in the united states right now: a political division along geographic lines, a set of self-reinforcing communication barriers along that division, an asymmetry of firepower along that division, and a set of minority groups whose acceptance is asymmetrical along that division. I propose a programme, along the lines of the mandatory short-term military service found in Scandanavia and Israel, to directly address all of these related problems. I expect it to offend all major entrenched interest groups roughly equally.
To summarize the problem: broadly, we have nominally ‘left’ and ‘right’ cultures — the blue and red tribe, respectively. The blue tribe is more common in urban areas while the red tribe is more common in rural areas (although the strength of this geographic division is debatable, and better debated elsewhere). More importantly, the blue tribe aligns itself strongly with social justice for marginalized groups yet is largely opposed to mechanisms by which such groups might defend themselves, and even in areas that are blue-tribe-dominated, weapons and minor positions of political-executive power (such as placement in law enforcement) are red-tribe-dominated. All of these associations are accidents of history: movements like the Black Panthers show that embrace of weapons for self defense are no less compatible with both left-leaning politics and a position as a persecuted minority than they are with white male right-libertarianism.
I propose a programme by which, in early adulthood (say, age 20), all people who are not too ill to participate would be required to attend two months of intensive firearm safety and training. Locations are assigned at random, rather than by convenience: after all, transportation will be provided by the government, and we would like to use this program to acquaint people who are from different geographic regions and who have different perspectives on life. These training programs will not be segregated by gender (including sleeping arrangements). Because this training program is non-military, there will be no religious or moral exemptions. At the beginning of the program, each participant will be issued a firearm, a portable gun safe, and a gun lock; at the end of the program, participants keep their firearm. (After the end of the program, they may of course sell their firearm, or discard it.)
Immediate results: all minorities are armed with a suitable weapon for self defense and a legal justification for having it (so a law enforcement officer can no longer consider being armed as sufficient for considering a POC a threat); people outside a general culture of firearm safety (like that which exists in the red tribe, where children are drilled on firearm safety from a young age by their whole community) are introduced to it, limiting the rate at which people outside a culture of firearm safety make dangerous mistakes when they are exposed to firearms; people from all walks of life are guaranteed at least one experience with living for an extended period with a group of people very different from them in the presence of deadly weapons without violence.
By Rococo Modem Basilisk on March 1, 2017.
I realize that your post isn’t meant to be a coherent argument, but — the only way we can tell that an idea is good is by trying to kill it and failing. To the extent that a writer is a curator of ideas, we benefit from taking the same kind of aggressive approach toward ideas as we do toward words when editing.
By Rococo Modem Basilisk on March 1, 2017.
Some less modest provisions:
• Those who are in prison at the time that they are being called up for this course will attend it, but their firearm will be held for them until they are released. Since this program grants a firearm license, current laws prohibiting felons from owning firearms will need to be overturned. (After all, most felons are not violent offenders, and even violent offenders have incentives not to re-offend. We shouldn’t allow prosecution to become a fig-leaf for preventing minority groups from defending themselves.) • Participation may be delayed for mental health concerns, but not cancelled. A jury of twelve mental health professionals, chosen by lottery, must come to a consensus that someone is not psychologically fit to participate in order for this delay to occur. The delay may be no more than a year. This is a preventative measure against the tactic, used both in the United States and the Soviet Union, of applying bogus psychiatric diagnoses to political or racial ‘undesirables’ in order to silence dissent.
By Rococo Modem Basilisk on March 1, 2017.
Slowly creep backwards while speaking more softly and with a greater number of increasingly-long pauses. If your partner doesn’t attempt to follow you, the conversation is over and you may leave.
By Rococo Modem Basilisk on March 2, 2017.
Spanning problem-space
Determining how best to improve one’s own suite of mental models & thinking tools is a hard problem: we can’t easily see ideas beyond the horizon, and ideas we haven’t yet invested effort in developing are distorted at best, but determining the value of ideas is necessary because of the scarcity of time & other resources. This is further complicated by the fact that knowledge-seeking is not a single-player game: everyone is constantly refining their suite of mental models, making decisions based on them, and producing material that makes certain ideas more or less accessible, and the value of a mental model is determined in part by the people who share it or share adjacent models, in somewhat complicated ways.
My current idea of how best to improve the value of one’s suite of mental models is based on a couple assumptions:
1. Ideas are adjacent to each other in semantic space based on shared attributes. 2. It is easier to learn an idea if it is adjacent to an idea you’ve already learned. The ease with which an idea is learned is proportional to the number of adjacent ideas already learned. 3. Adjacency in semantic space, seen as a network, is a web, not a tree. Some ideas are adjacent to each other even when none of their immediate peers are adjacent — such as when seemingly unrelated ideas in seemingly distinct fields have striking similarities. 4. A factor in the value of an idea is its adjacency to other valuable ideas. Part of this is ease of communication: when we have a shared terminology and set of assumptions with people, we can share new ideas more easily. When we share few ideas with someone, communicating with them is difficult. 5. Another factor in the value of an idea is its concrete utility, in of itself. For instance, the set of ideas known as ballistics are very useful in predicting the movement of objects. 6. A third factor in the value of an idea is its scarcity. Someone who is an expert in an obscure field will have greater social capital than someone who is an expert in a more commonly-understood field with the same concrete utility adjacent to ideas of comparable value. 7. Some ideas have as their primary concrete utility the capacity to change the value of other ideas by changing something about society. Rhetoric, for instance, can be used to modify ideas about the value of certain other ideas, thus changing things like salary and social capital. 8. Adjacent ideas are not always obvious. Sometimes they are only obvious in retrospect. 9. Adjacency doesn’t necessarily have any relationship to truth or intent, although systematic biases (including toward truth or toward consistency) may favor clusters of similar ideas. For instance, mathematics, because it enforces consistency, finds large numbers of similar patterns in far-flung contexts. 10. Traditional (tree-like) academic paths through idea space are easy to traverse in part because so much effort has gone into lowering traversal effort — the production of teaching material, specialized terminology, and communities and social structures (such as universities). That same ease of traversal lowers the value because it increases the number of people with nearly identical mental toolkits. 11. Autodidacts trade the easy-to-traverse yet diluted conventional path for unconventional connections of unknown value. They risk missing ideas that are relatively hard to pick up without structural aid but that are very useful for opening up further vistas or closing off dead ends (like calculus, or godel’s incompleteness). 12. Successful autodidacts are polymaths. Unsuccessful autodidacts are cranks. It’s hard to tell the difference without mastery of related fields. 13. Undiscovered or undocumented adjacencies between seemingly unrelated subjects are common, but few have much concrete utility. However, those that do are extremely valuable. 14. As a result, someone can optimize the value of their mental toolkit by following traditional paths enough to enable communication but otherwise specifically choosing to persue seemingly unrelated subjects that are rarely persued together, periodically attempting to synthesize them. Random number generators are useful in path choice and synthesis, since the likelihood of producing an unconventional path and the likelihood of choosing paths with hidden adjacencies are both high.
By Rococo Modem Basilisk on March 6, 2017.
It really is better to ask for permission
A problem with slogans is they get adopted outside of contexts where they make sense, either because the people using them didn’t carefully consider whether or not they were true, or because they provide an excuse for doing something that would otherwise not be allowed. Where a monoculture exists, with a group of people with similar values, culture, resources, and problems all make decisions based on the same assumptions, inappropriate slogans can cause systemic biases. The worst offender I’m aware of right now is “it’s better to ask for forgiveness than ask for permission”.
I have no particular interest in tracking down who said this first and in what context. I will give it the benefit of the doubt and assume that initially it was said in a context where it was true — improv comedy, maybe. However, like similar slogans like “move fast and break things”, today it is used almost exclusively in situations where it is not only untrue but also actively harmful.
In order for it to be true for it to be better to ask for forgiveness than permission, the following must also be true:
1. The stakes must be low — in other words, mistakes must not be very damaging (or else apologizing wouldn’t be enough) 2. There must be a single homogenous party from which to ask forgiveness (or else asking for forgiveness wouldn’t make sense, because you would never be forgiven by all concerned) 3. Asking for permission must be more difficult or risky than getting forgiveness — in other words, the party from which you ask permission must be conservative about it 4. Success must be likely (or else you would be seen as a perpetual screw-up for following this advice) 5. Performing the task must be its own reward — in other words, you must see even the failure as valuable
How this is interpreted, however, hinges on how we define ‘better’. When people say this in the tech industry, the most charitable explanation is ‘better’ means ‘better for my paycheck’ rather than ‘better for users’ — in which case, one can expect to apologize to one’s supervisor, not one’s users, if one takes down all production systems for a week.
While this is charitable, it’s hardly an endorsement. None of us should feel better about losing service on the grounds that the developer at fault was forgiven by his supervisor. Any system with users cannot afford the kind of unreliability produced by lack of oversight, because the real stakes are much higher than a single developer’s paycheck.
Ultimately, the attitude embedded in this slogan, taken too seriously, is at the core of many of the worst behaviors associated with the tech industry. Uber repeatedly violates labor laws on the grounds that it can get away with doing so — and asks forgiveness when sued. The creepiness of redpillers and PUAs mostly comes down to the entitlement they feel toward other people’s bodies, assuming they don’t need to worry about consent beforehand because they can ask for forgiveness after they’ve gotten what they want if in retrospect consent wasn’t given. More concretely, this attitude produces shitty dysfunctional code, short-lived companies that sell user data and disappear suddenly, ethically-dubious business practices, and a general culture of “I’ve got mine, bub” that directly contradicts the phony “save the world” PR everyone likes to wear.
Here’s the thing: if you’re doing real work, you have real responsibilities. One of your responsibilities is to make sure your decisions are safe — not for your paycheck, but for everyone else. You have the responsibility to check with colleagues in order to make sure your plans aren’t stupid and damaging, and you have the responsibility to make sure your colleagues aren’t doing anything stupid either. If you aren’t willing to take ‘no’ for an answer, then you should switch to an industry where nothing you do matters, because that’s the only situation where such behavior is morally justifiable.
By Rococo Modem Basilisk on March 13, 2017.
Clues about Medium’s business plan
Clues about Medium’s business plan
Yesterday I got an email. (Presumably many of you also got it, or something like it.) There was nothing in the email that indicated they wanted the information exposed therein kept secret, so I feel comfortable sharing it. Reading between the lines, it clears up some of the ambiguities left by official announcements about Medium’s future & their plans for monetization.
Hi,
I’m Brian, an editor here at Medium. I’m reaching out to you as a popular writer who’s published high-quality stories on Medium.
(I am not a popular writer — I’ve written one paid article that got the kind of traffic paid articles tend to get, and a large number of articles that have views in the single digits. My average view count is probably under 100, and my average recommendation count is less than one. So, this is a big clue: if I’m getting this email, pretty much anybody who has ever submitted to a popular publication is getting one.)
You might have read that we’re launching a new subscription product for our readers shortly. It’s the next step in our vision to build a place on the internet where ideas are rewarded for their value, not simply their ability to attract a few seconds of attention.
As fans of your work, we’d like to offer you the opportunity to pitch your ideas or relevant posts early, and become part of a select group of contributing writers for our initial launch.
(This is interesting. Not only will there be subscription-only stories, but subscription-only stories will be pitched to and vetted by Medium. This essentially means something akin to a Medium-staff-run publication that only subscribers have access to. While this is better for users than a paywall that locks you out after a couple articles, it means that Medium will be competing with Medium-hosted publications on the grounds of editorial standards.)
Our subscription will be an optional upgrade for people to become supporting members of Medium. These people will be able to access additional member-only functionality and new, exclusive content.
(I.e., there’s more to this freemium model than a cool-kids-club of vetted articles, but we have no clues as to what those features are, other than the fact that they’re user features as opposed to publication features.)
Writers across the world will continue to be able to publish on Medium for free, but we know there’s a great deal that never gets written or published by great writers, for lack of it making economic sense to do so. We want those stories — well-researched explainers, insightful perspectives, and useful knowledge with a long shelf life — to exist on Medium as well, and we think our paying readers will want to read them too.
(This is a strange pitch. Nowhere in the email do they explicitly state that writers will be paid for writing subscription-only stories — even though they have a pitch process like you’d expect from freelance — but they suggest that they will be able to make writing articles not currently picked up by publications make economic sense. This implies that they’d be either paying better than existing publications, supporting a greater number of articles, or trying to commission articles on subjects existing publications don’t or can’t cover.)
That’s where you come in, because we thought your writing could be a great fit. So what types of posts have you been burning to write? If you knew you had a paying audience waiting for your ideas, what stories could you tell?
We’re looking for pitches within the following categories to start: US Politics, Technology/Science/Future, Self Development/Productivity, Business/Startups, and Culture. So tell us what you’d like to write about and your rate. If it sounds like a good fit, we’ll get back to you with a thumbs up or feedback as soon as we can. If you’re interested, we’d love to receive your initial pitch before Friday, March 17th.
(Their launch categories are those already covered heavily by existing publications. I don’t see, for instance, short fiction on the list — even though, unlike all these listed categories, short fiction and poetry publications typically have short and infrequent submission windows. It’s also interesting that they want pitches so fast — the deadline is five days after the announcement!)
(I have removed their submission link. I should note that it links to Google Forms, so they’re probably expecting a large enough number of pitches that they want to automate handling them. This is a good sign: even pretty big publications tend to take submissions by email.)
This is just the first opportunity to be part of Medium’s subscription offering. We’ll be following up soon with more writing opportunities, as we continue to evolve our subscription and learn what our members enjoy reading most.
Thanks,
Brian
(In other words, they plan to rotate through different categories and have frequent calls for pitches. I have to wonder why they don’t transition to having open pitching and keeping only internal deadlines, at this scale.)
By Rococo Modem Basilisk on March 14, 2017.
How Seiren re-engineers galge stock characters
Seiren is not among the most popular shows this season (with a MAL score of 6.4 /10, an AP score of 3/5, and an IMDB score of 7.3/10), and it is not among the best shows this season, but it does something interesting and unique that I think is worth a closer look. Specifically, it appears to be an exercise in rediscovery and re-engineering.
Anime has a well-deserved reputation for relying upon stock character types and plot structure. This is not necessarily a bad thing: most of what anime does differently from western productions is about cleverly saving effort and resources without sacrificing story quality, and stock characters can be used to this end. However, stock characters have become over-fitted, to the point that shows that hew too closely to type become predictable and boring (for instance, see this season’s Gabriel Dropout, a show that despite incredible animation and voice acting, was consistently done better beat for beat twenty years ago).
One way to combat this is to draw attention to it (as last years Saekano did). This works fine a couple of times, but by itself quickly becomes boring: the character who is aware that she is a shallow cliche becomes another shallow cliche. Another way is to engage in a dialogue with the type, adding nuances that make categorization fuzzier and possibly bringing in self-awareness but still engaging in the same attributes and behaviors that are associated with the type (which, as Digibro explains in several videos, is part of what NisioisiN does in the Monogatari franchise, and which I think this season’s Masamune-kun’s Revenge does reasonably well). It’s also possible to simply play the types straight, if character study isn’t a major goal: Konosuba doesn’t really try to engage critically with Megumin’s chuunibyo status or Darkness’s masochism, and instead simply uses those cliches exceptionally well in service of the comedy.
Seiren does something different, and (being an anime original instead of an adaptation) I think it does it intentionally. I take as Seiren’s thesis that the stock personality types are themselves dramatically interesting, but the cliche behaviors and attributes associated with those types are unnecessary and can obstruct connection. To this end, it tries to show these character types in a realistic and naturalistic environment, with non-core traits removed — as though an alien had read a description of these character types and written characters adhering to those descriptions without having ever seen an anime.
Seiren’s structure is, on its face, absolutely typical of the basic galge format: a protagonist gets to know several girls, and depending upon his actions he can develop a relationship with one of them. This format is so old that by the 90s attempts to shake it up by injecting time travel and supernatural elements had already themselves become cliche (and Seiren has none of those attributes); School Days, itself ten years old by now, tried to shake this format up by adapting only the bad ends. However, right off the bat, we have some big differences in how this is handled structurally, tonally, and in terms of shared characterization. Our characters are all high schoolers, with most of them in tenth grade; where a typical galge would polarize their maturity levels (particularly by making some characters sexually active to the point of deviance for comedic effect while making others almost absurdly innocent), Seiren portrays all its characters as within the typical range for upper middle class high school students: interested in sexuality but naive about it, with insecurities about their naivete, and with a mix of mature and childish hobbies. Where a typical galge raises the emotional stakes to a fever pitch and creates conflict based on major (often ridiculous) misunderstandings, Seiren shows characters floating in and out of an ambiguous zone between friendship and romance without much fuss, portraying every character as fundamentally wanting to understand and be understood. This is a pervasive thematic and stylistic element: we’re looking at a consciously & conspicuously iiyashi-kei love-comedy, where the stakes are low and even the arguments are calming. This naturalism extends to the art: the character designs are much more realistic than even a Kyo-Ani style, and characters are rarely off-model (which normally is a bad thing since it limits the expressive range, but emotions in this show are pretty muted so instead it adds to the appeal); nobody has an unnatural hair color, and few people even have a hair color that would be unusual among Japanese students.
Moving on to characters, Kamita is absolutely a protagonist-kun: he is physically average, has no strong feelings or preferences, and has no particularly notable personality attributes. However, he still differs from a typical character of this type. His aimlessness is lampshaded in the (widely hated) opening scene of the show, where he is being chided for writing in “stag beetle” on his career survey: while aimlessness is normal for this kind of character, the kind of childish surreal aimlessness you might see in actual tenth graders is rare. Kamita has interests that are normal in people his age but rarely shown or mentioned in anime of this type: he plays casual / nuturing games, acts out professional wrestling moves with his friends, and hangs out with a group of similarly awkward guys. The way his sexual interests are portrayed is also interesting, but sexuality in this show will be discussed at length later.
Tsuneki, the first heroine, is absolutely a tsundere: she puts on a harsh front in order to hide her insecurities, particularly with people she likes. Unlike the other characters, her design consciously references the database: she wears her hair in the same style as Asuka from Evangelion and Rin from the Fate franchise. However, most tsundere behaviors have been removed or modulated: she is never extremely harsh (she never hits anyone or becomes explicitly insulting), and while her emotional reactions are stronger than those of other characters in the show, they are never outside the range of typical or acceptable behavior. As the first heroine (and one who continues to appear in every route), she gets the most characterization, and the origin of her attitudes and habits is made clear: her conflict is a desire to be more mature than she really is, and in addition to going out of her way to act like her idea of an adult (getting a part-time job despite not needing the money and despite school rules against it, attempting to spend a week at a beach house with friends against her parents’ wishes), she attempts to present herself as worldly (particularly in terms of sexual experience) — a ruse that fools her peers but not the audience. While putting on airs and attempting to seem more adult is a common tsundere trait (Rin & Asuka both do it, and it’s extremely common in ‘palmtop tiger’ characters such as those in Toradora and Familiar of Zero), the mechanism here is very different: Tsuneki attempts to be an adult not due to parental neglect (Rin & Asuka are orphans, and Taiga and Louise’s parents are absent) but instead despite active intervention by caring and present parents. Even the way she is introduced flies in the face of every other major tsundere depiction: where tsundere characters are typically either the protagonist’s childhood friend or complete strangers, at the time of Tsuneki’s introduction she and Kamita are on the friendly side of casual acquaintances. Tsuneki’s interest in Kamita is immediately obvious to the audience (though not to Kamita) because of her constant minor sexual teasing, and this interest continues throughout the other routes: she’s the most sexualized character, but she is also consistently depicted as being mostly interested in Kamita (we don’t see her tease anyone else) and having only a pretty minor interest in him (while we see her act somewhat jealously in Miyamae’s route, this is partially because of shared history and differences in philosophy — basically, Tsuneki seems to accept whoever Kamita ends up with). In other words, Tsuneki is a tsundere but neatly avoids nearly all of the non-core tsundere traits, down to even very subtle ones whose universality is invisible until they are missing. Kamita makes sense as a match for her because he is so non-judgemental that she can be honest with him.
Miyamae is a slightly less clear-cut case. Gamer girls are newer as a fixture in anime, and their traits are less well-defined. However, Miyamae subverts many of them. One element here is the way gamer girl characters typically deal with femininity and with respectable behavior: they are often depicted as slovenly — uninterested in looks, grades, and athletics. (For an extreme example of this, look at Gabriel from Gabriel Dropout.) When such a character needs to seem initially attractive, they are instead shown as a secret otaku — someone with a fake ‘ideal student’ personality and a real ‘gross nerd’ personality they only show to family and close friends (see Himouto Umaru-Chan, Saekano, and even to a lesser extent this season’s Kobayashi’s Dragon Maid). Miyamae avoids or subverts all of this: she is a serious student with good grades, good at athletics and pretty enough to be known for her looks by second year students; she is open about her interest in video games, and her interest is initially unknown to Kamita only because he knows her mostly by reputation. She has social problems stemming from her dysfunctional relationship to video games, but her basic underlying problem is that she applies the same kind of contentiousness and seriousness to games as she does to everything else, without prioritization and without an awareness of how her competitiveness might affect others who lack her skills. She is lonely not because she flouts convention but because she doesn’t know how to turn off her drive to improve herself: she’s better than everyone around her at everything, to the degree that even her basic humbleness doesn’t help. Kamita has a weak enough ego that he’s willing to keep her company even though he can never hope to catch up to her, which makes them a good match.
Kyoko, the third heroine, is absolutely an imouto character. While her route hasn’t finished airing, she’s still a pretty interesting subversion of tropes. Having childhood friends belatedly recognize each other as sexual beings is a pretty common trope in western sit-coms (and in h manga), but it’s not terribly common in galge, and it’s done differently here. Usually, the childhood friend character is part of a long-standing one-sided crush (or a two-sided crush where each side is unaware of the other), but here, we explicitly see Kamita and Kyoko discovering new dimensions of interest in each other simultaneously, as part of the general project of both of them coming to terms with a shift away from their childish habits and behaviors. Major dimensions of the typical imouto trope are missing: the age gap is only one year, and Kyoko is neither blood-sister nor step-sister but instead a neighbour; there’s no long period where they are away from each other (they didn’t see each other in school during Kamita’s first year, but being a year apart they wouldn’t necessarily see each other without planning to anyhow, and it’s made clear that she is a frequent presence in Kamita’s life because of her friendship with his sister). Kamita and Kyoko’s relationship is not merely already close but actually still close, which distinguishes this from pretty much every other imouto character relationship I’m aware of.
The depiction of sexuality in this show is strange and interesting. Lots of galge and harem shows have over the top depictions of sexuality, but those depictions tend to be easily classified — essentially another kind of boring cliche. The protagonist who suddenly transitions into a fugue state of sexual deviance used to be pretty common (see Golden Boy, Sora no Oshimoto, and the Monogatari series) and is thankfully fading in popularity. Protagonists who are afraid of their own sexuality are still pretty common, particularly when over the top tsundere characters are in the mix (see The Asterisk War, Love Hina, and this season’s Interviews with Monster Girls). Characters who have a fixation on some particular attribute but are otherwise not terribly sexual are also floating around (see Kyokai no Kanata), and are perhaps even less realistic than the other two. Seiren breaks the trend with a pretty naturalistic depiction of polymorphous sexuality: the things that turn Kamita on are never cliches (he’s not ogling breasts or asses, or even thighs) nor are they depicted as strangely deviant, but instead they’re the kind of minor detail one might fixate on with an actual lover (the slightly damp spot where Tsuneki once sat, the nape of Miyamae’s neck). When Tsuneki flaunts her sexuality, she does it with awareness of this: she sits on Kamita’s desk and then teases him about his discomfort, or she uses overly familiar body language in order to fluster him, rather than wearing a fetish outfit or shoving her breasts in her face (as even otherwise well-developed tsundere characters like Asuka do). With regard to fetish outfits, when Miyamae wears a bunny costume, the costume itself isn’t depicted as highly sexualized by Kamita (although it’s shown that other convention-goers see it this way), and instead the show focuses on Kamita and Miyamae bonding over shared the shared fear of pissing themselves in public. (Two shows this season go to Comiket and cosplay, and both engage in interesting dialogues about sexuality; but, nobody needs an excuse to talk about Dragon Maid.) Outside of the sexualization of characters, we have two animals thematically and symbolically associated with sex: rabbits and deer. Rabbits aren’t terribly unusual as a symbol of sexuality, but I’ve never seen deer used in this way. Kamita’s friend Araki is described as a furry, and it’s explained that he has a sexual fixation on rabbits — to the point where he doesn’t seem capable of being interested in someone without a rabbit theme being present; deer, on the other hand, are never shown to be anyone’s sexual fixation but are consistently present in sexual situations: Tsuneki ends up in Kamita’s room because her attempt to escape the cram school was aborted due to deer chasing her (leading to her appearing wet and half-naked, vulnerable in front of him for the first time); Miyamae and Kamita originally bond over a deer-raising game (which Kamita says has some sexual-seeming sound effects), and they mate their deer; Miyamae goes to Comiket to sell her hand-made deer plushies, whose killer feature is the ability to swap gender using velcro antlers (and while flustered, Kamita makes a big deal about the sexual equality of deer society, driving away customers). On the other hand, when Miyamae starts playing as a bunny girl she goes back to her old bad habits from middle school, frequenting the arcade and skipping school to play video games online; Tsuneki is originally (casually) interested in Araki and after being sent to the cram school one of her friends steals him away, leading to a confrontation in the rabbit pen. From these circumstances, I would say that rabbits symbolize a kind of immature approach to sexuality (and an immature approach in general), while deer represent stable and loving relationships. This explains why Araki’s fetish is never emphasized as deviant but is instead frequently depicted as something he is slowing working through, and why Tsuneki (who both desires and fears adult sexuality) is both obsessed with and morbidly afraid of deer for the entirety of the series.
Another angle to Seiren that’s interesting is the way that it’s very clearly positioned as a coming of age story. Most shows of this type are set in high school as a matter of course: this is the only time when regular people have enough independence to pursue romance but enough resources not to need to spend all day working; while events associated with this period and with coming-of-age themes are often referenced in other shows (the obligatory test of courage episode, summer vacation, beach episode), they rarely actually function well as coming-of-age stories. Seiren broadcasts its intent with that first scene: where most shows use the career survey as a gag (with a character filling in “superhero” or “king of the world” or “famous actress”), Seiren subverts this by making it a gag that doesn’t make sense: there is no stock character we would expect to want to grow up to be a stag beetle (or who would even want to say that as a joke), and so we’re immediately set adrift, being unable to clearly place this character. This character is also adrift (the stag beetle is, along with the cicada, a kei-go for summer: if we want to interpret this decision symbolically, Kamita is saying that he would like his future to be similar to the summer of his life that the series depicts — a pretty abstract and aesthetic position, and one that Kamita might agree with but would never actually be able to explain). With any other show, the career survey answer would tell us which stock character Kamita aligns with; in this show, we need to understand who Kamita is before we can understand his answer. And, fundamentally, this is a childish response: one that befits someone like Kamita, who is simultaneously more childish and more mature than all his classmates. Each of the heroines have a different specific, plot-significant relationship with maturity: Tsuneki wants to be an adult but knows she’s not really emotionally ready; Miyamae, adrift in her own way, has an internal battle between a competitive and nurturing instinct and a tendency to return to habits of her childhood as a video game prodigy when stressed; Kyoko has a childish nature and is completely oblivious about how her behavior seems to outsiders, and her arc will have to involve coming to terms with her own transition to adulthood. In this show, maturity is connected strongly with a nurturing instinct, and for both heroines whose routes have been completed, they chose nurturing professions.
Seiren is also in dialogue with femininity. Not only are all the heroines specifically positioned as an ambiguous mixture of masculine and feminine traits (with Tsuneki being aggressive and sexually intimidating without ever seeming like a tomboy, Miyamae combining mastery over a traditionally masculine hobby with a traditionally masculine competitive instinct and a set of highly feminized skills, and Kyoko’s lack of awareness of her own femininity being emphasized in her route despite the twin themes of professional wrestling and magical girls), but Kamita is also portrayed as having traditionally feminine traits (being more concerned with caring for people’s feelings, protecting existing social groups, nurturing and repairing relationships, and communicating clearly about feelings than any of the heroines, along with being generally passive as a character — something highly feminized in galge-type shows). More overtly, Kamita cross-dresses as a magical girl in the first scene of the first episode of Kyoko’s arc, and he’s consistently shown as more interested in feminine-coded versions of things than masculine ones (for instance, during the Miyamae arc, he prefers the deer game and another schoolbus-driving game — i.e., nurturing games — to the violence-oriented fare Miyamae plays). No characters have traditional male roles here, in part because typically feminine-coded nurturing behavior is treated as a sign of maturity regardless of gender. It’s also important to note that, in the completed arcs, Kamita is shown to have chosen his profession to complement that of his mate: Tsuneki becomes a chef (conquering her fear of deer by becoming an expert in cooking venison) and Kamita becomes a waiter in the same restaurant; Miyamae becomes a school teacher and Kamita becomes a bus driver.
There’s a lot to pull out of this show: subtext, symbolism, and a complex dialogue with the state of the genre. But, does it succeed? I think it largely doesn’t.
It’s not a poorly made show, but because it avoids lampshading the tropes it subverts, it’s easy to take it at face value as a cliche rom com/galge adaptation made by someone who doesn’t understand how to properly use the genre tropes, rather than a conscious attempt to subvert them. By keeping the core elements of the types intact, it can be seen as less subversive than it really is; by dropping the auxiliary elements, it can seem less competent than it really is.
The way sexuality is depicted, while strictly realistic, seems alien because of how divorced it is from the normal depiction in this genre, and the casual pace and low emotional stakes can easily be seen by genre aficionados as a failure to achieve the kind of frenetic energy and melodrama typical of romance comedy anime.
It fares particularly poorly this season, since it’s airing alongside Kobayashi’s Dragon Maid, which covers a lot of the same thematic ground and plays similar games with tone and pacing while being a far better-executed show, and Masamune-kun’s Revenge, which has a similar setting but sticks to pretty standard tropes while being more consistently enjoyable; had it aired this time last year, against Saekano and Nisekoi, it probably would have a better reputation.
Another problem is that, without attempting analysis, the characters feel underdeveloped. All the necessary information exists in the text to consider them fully realized, but because it attempts to cram three whole routes into twelve episodes, none of this characterization is given breathing room. The general subtletly and the explicit avoidance of character-building tropes makes it even harder to quickly model the characters, resulting in difficulting even telling supporting characters apart during the middle of the show. Having Tsuneki’s arc exist primarily in the more limited environment of the cram school, where there are fewer supporting characters, makes sense, but we then bear the full brunt of a large cast of realistically-drawn brown-haired minor characters immediately at the beginning of Miyamae’s arc. Seiren doesn’t take advantage of any of anime’s many very important techniques for minimizing the number of frames necessary to show characterization — in other words, this show could have been live action with very few changes — and it suffers for it.
Seiren is, from what I understand, a kind of stealth sequel to Amagami SS. I haven’t seen Amagami SS, and Seiren doesn’t advertise its connection very well, but it’s possible that for viewers of that show it holds up better in the characterization department. However, Seiren reviews that directly compare it to Amagami SS are largely negative and seem to imply that Amagami is much closer to a standard galge show, indulging in the practices Seiren avoids.
By Rococo Modem Basilisk on March 16, 2017.
It seems like what Medium is doing is creating a subscription-only publication of their own, competing with existing Medium publications. In the email, they made a big deal out of the idea that most users and writers would still not subscribe — which means that, if they want to keep their existing audience, existing publications will be unaffected.
I’d like to see them open up a submissions for fiction and poetry, if only because fiction and poetry publications on medium have short, rare, and unpredictable submission windows and typically pay peanuts compared to nonfiction publications. (For instance, How We Get to Next — not even a huge publication — will pay out nearly $300 for a relatively short article that meets their standards, while fiction of similar length and quality on Electric Literature is capped at $50 if I recall.) If their experiment with nonfiction is financially successful, they could probably afford to assign a couple people to a fiction slushpile and keep submissions open perpetually.
By Rococo Modem Basilisk on March 17, 2017.
So, there’s a nuance here, regarding UBI.
The nuance (which even communists tend to miss!) is that there’s no reason that labor (and feelings of accomplishment related to labor) have to have any association with survival resources.
It’s an accident of history that we have such a direct association between labor and survival: in nomadic tribes, resources tend to go into a shared pot distributed by mechanisms unrelated to the labor it took to gather them, and even post-agriculture, farming families (themselves their own semi-isolated societies) did the same. (And, of course, in a monarchy resources are extracted from a hard-working population and given to an elite for redistribution on the grounds of inherited roles, unrelated to effort ratios.) Capitalism (in the sense of the widespread belief that trading labor for money is normal and natural, as opposed to a quirk of a tiny merchant class) was only about a hundred years old when Marx wrote about it.
UBI necessarily overturns this connection, which already (again necessarily) was already untrue at the margins: the extremely wealthy often do volunteer or charity work, and although this is good for PR, it’s also likely that in many cases it serves primarily to provide a source of fulfilling labor for people who have no need for more money; people unable to perform the type or scale of work necessary to produce a living wage still have productive hobbies (I know several people on disability who do a lot of knitting and crochet, and others who are authors), and although making money off those hobbies would lose them their existing income (without making up for it), they are willing to put time, effort, and scarce resources into sometimes gruelling work in order to feel productive.
If labor satisfaction and wage satisfaction had a deep connection, rather than one of convenience, the rich wouldn’t work for free, and the poor definitely wouldn’t work for free; but, neither of these things are remotely true.
Considering that satisfying labor is rare (or, in marxist terms, modern labor of all kinds typically alienates the worker from his product), we can argue that there is in fact a contradiction between being paid for labor and feeling satisfied by it. Part of this can be attributed to circumstances: if some necessary labor is inherently and universally unsatisfying, then anyone who does it will be unsatisfied; if all labor is satisfying to someone but each person is suited to a particular type of labor, then low margins of error and high risk related to attempts to move between fields result in most people who aren’t comfortably wealthy being trapped in jobs that can’t satisfy them. Part of it is also human nature: satisfaction is extremely sensitive to cognitive dissonance, and experiments show that a sense of satisfaction will be manufactured in response to a situation where the effort to reward ratio is low. (Why do people read Finnegan’s Wake and play Dark Souls? Why does everyone who finishes those things love them? Because casual interest is quickly punished, and because the sunk cost must be justified to account for the vast gulf between casual interest and sufficient interest.)
By Rococo Modem Basilisk on March 20, 2017.
Having signed up, you can tell us exactly what we all want to know: is the premium content Ev’s pushing actually worth $5?
$5 can buy you a brand new 500-page paperback. Is this at least the equivalent of a new 500-page paperback per month? (A good one, I mean — not the kind where you buy it and then grudgingly finish it because you don’t want to have wasted $5. A new Neal Stephenson novel or something.)
High quality print magazines charge about $5 a month, and when they don’t deliver, they lose readers. High quality print magazines have utility separate from their actual content: they can be used to swat flies or line bird cages, which a Medium subscription cannot. At the very least, in order for Medium to be worthwhile, it must be superior to all the high-quality print magazines to which you subscribe (and all the similarly priced magazines to which you don’t subscribe because of their poor quality).
$5 is not like one cup of coffee a month. $5 is like choosing to splurge on expensive cafe-brewed coffee once a month; you can make several months of coffee in your home coffee maker with $5 worth of beans. Does Medium deliver enough to satisfy the crowd that would rather get beans in bulk and have many months of cheap coffee they must prepare themselves than purchase expensive chain coffee?
What Medium is charging is about half of what Netflix charges me. I watch a lot of Netflix. I would expect to get at least half the enjoyment I get out of Netflix out of Medium’s premium content in order to make the subscription fee worthwhile.
This is a pretty high bar. Does it deliver?
By Rococo Modem Basilisk on March 24, 2017.
How Mamoru Oshii ruined the Ghost in the Shell franchise
Mamoru Oshii’s 1995 film Ghost in the Shell is excellent, technically groundbreaking, and hugely culturally important. However, as an adaptation of Masamune Shirow’s manga of the same name, it is an abysmal failure, and one that has negatively impacted all other elements of the franchise. Because of the upcoming american live-action film, I’d like to revisit the problems with the 1995 adaptation, if only because it seems like the 2017 film does to the 1995 film what the 1995 film did to its original source material — with major implications for future franchise entries if it becomes successful.
It’s important to give some background on Shirow, in order to understand why the manga is how it is and what differentiates it from his other work. Shirow (real name Masanori Ota) is known for works that combine heavily sexual content with detailed descriptions of machinery; his work ranges from pornography to hard SF. While earlier works like Tank Police, Orion and Appleseed contained a mix of science fiction and political ideas, Ghost in the Shell is notable in its first volume for its more grounded near-future setting and for much greater technical detail. Ghost in the Shell incorporates elements of previous works — like Tank Police and Appleseed, it’s about a government-sanctioned para-military anti-terrorism task force who have been super-empowered by the use of military hardware.
Where Ghost in the Shell differs from these other works (and nearly everything else in the genre) is the attention to detail baked into the manga: my copy of the first volume (the Dark Horse release) has 81 authors notes in a 10-page section in the back, as well as substantial use of inline footnotes: the first color spread contains four separate inline footnotes, and while the frequency of footnotes is reduced for the following three chapters, chapters four and five are primarily dedicated to world-building, and chapter five contains four substantial footnotes, four explanatory diagrams describing the science behind cyborg manufacture and design processes, and a textbook recommendation — all over a total of six pages. In chapter nine, twelve pages are dedicated to Motoko’s dive into a Puppeteer victim —initially focusing on the mechanics of diving, but eventually going into detail about brain structures & the use of electron spin in storage. From early on, Ghost in the Shell is a manga about ideas that doesn’t shy away from technical details.
Ghost in the Shell also doesn’t shy away from taking a close look at politics and economics. There are two major plots in the first volume; one is the Puppeteer plot, and the other revolves around economic incentives leading to orphans and refugees being brainwashed, cyborgized, and sold as counterfeit robots — in other words, industrially-mediated slave labor supported by government corruption stemming from the price difference between top of the line hardware and the bodies of the poor. Corruption is a focus: every antagonist is shown to have government ties, and while Section Nine is on the side of the angels mostly because of Aramaki & Kusanagi having abnormal moral integrity, its existence is grey-legal and owes itself to the same back-door politicing as 2501 does.
Oshii’s film adapts material from only four chapters of the 11-chapter manga: chapters 1, 3, 9, and 11 (i.e., the Puppeteer story), and only in a severely truncated way. Project 2501’s monologue during the dive in chapter 9 is dropped entirely (including the explanation that 2501 was a high speed insider-trading system), and of his nine page monologue in chapter 11, only elements from six pages are included — of 48 panels, Oshii’s film only adapts material from 13, producing an overly simplified explanation of 2501’s philosophy. There are ten diagrams in this chapter, of which only one is every visually referenced in the film.
A particularly interesting omission is the systematic scrubbing of references to all forms of eastern mysticism from the film: in the manga, 2501’s reasoning leans heavily on ideas about karmic connections, but these elements have been removed entirely. This is strange, since Oshii inserts his own religious references of various types (including bible quotes). One way to look at it is that Oshii’s philosophy (and the ideas he is fixated with) differ significantly from Shirow’s. The Ghost in the Shell manga is not largely concerned with identity: only seven of the 48 panels in 2501’s chapter-11 monologue touch on identity at all, and of them only one acknowledges it as meaningful — of the remaining six, two are related to the idea of karmic connection in terms of our false models of others’ personalities, and four are related to the idea that sexual reproduction produces more robust offspring than cloning. On the other hand, 2501’s monologue at the end of Oshii’s film is mostly concerned with the idea that Kusanagi fears changes to her identity, and most of the scenes Oshii added focus on the idea that Kusanagi considers her body alien to her (including being unembarassed by nudity & having a fixation on its mass-produced nature). These attributes Oshii gave to Kusanagi in the film are not at all in line with her character in the manga, of course: Kusanagi breaks Bateau’s eyes in retaliation for his intrusion into her VR beach party — i.e., for seeing her in a revealing bathing suit unexpectedly — and 2501 notes that, by being unconcerned with the preservation of her own identity, Kusanagi violated his prediction — agreeing to his suggestion that they fuse in one twenty-fifth the time he expected.
When Oshii made these changes to Kusanagi’s character, he influenced later parts of the franchise. With the release of the film, the manga and non-manga portions of the franchise begin to fundamentally diverge. Even as Stand Alone Complex reintroduced the Fuchikoma (as Tachikoma) and brought in members of Section 9 not featured in the film, it kept much closer to the film’s characterization of Kusanagi as business-like and humorless rather than the mischevious troublemaker from the manga, and also kept her portrayal as sexless (in direct contradiction with the manga, which gave her a boyfriend for several chapters and also had her participating in casual group sex with two female friends). While SAC brought in a handful of elements of stories from the first volume of the manga (including ideas about wealth inequality in the GitS universe), no other part of the franchise ever really adapted any story from anywhere in the entire manga continuity outside of the Puppetmaster story (and never substantially differently from Oshii’s version), and every iteration has trouble portraying Section 9 as being in a moral grey area the way the manga was able to do quite casually. While it’s not technically correct to say that Oshii’s film was “the original”, in a sense it’s more accurate: the manga was first, but Oshii’s film has a pretty tenuous connection to its source material and all later elements of the franchise draw more from it than from Shirow’s work.
Trailers for the new american film look like they have lifted imagery directly from Oshii’s series (both Ghost in the Shell 1995 and Innocence) and from the first episode of SAC. However, the voice-over in the trailer implies that the plot has taken another step in the direction of cliche: where Oshii took a character who was comfortable in her own bio-type sensory-film skin and turned her into a human thrust into an alien mechanical body and trying desperately to maintain her sense of self in the face of a mass-produced mechanized society, this film adds yet another tired idea — that she hates being a cyborg and wants to take revenge on the people who saved her life. In other words, plot-wise, this film appears to have more to do with 009 than Ghost in the Shell.
While the 1995 film dumbed down the manga’s ideas and replaced a large portion of them with cliche, current Ghost in the Shell fandom was introduced to the franchise by this film and largely prefers it. A much larger market will be introduced to the franchise by the 2017 film, and by the same logic, will prefer the even dumber version — potentially leading to lots of tie-in media that tries not to stray far from the gutted premise. (Imagine an american live action GitS TV show in the vein of current Marvel and DC shows.)
Meanwhile, the manga continues in the rich, weird direction its always had. Volume 2 was followed by volume 1.5. Volume 2 focuses on a psychic investigator hired by Section 9 who crosses paths with Kusanagi, who while following the trail of a hallucinated racoon-dog is hacked by a south american communist guerilla group and then discovers clones of her original body on a space station. The art’s better and it has even more footnotes than the original. Kusanagi determines whether or not she’s in a simulation by convincing herself that her leg is a perfect cylinder and then checking it in a mirror; a secret government agency prevents earthquakes with feng shuei; Kusanagi has a big cyborg body guard whose chest can hold her normal cyborg body, who she rides in like a mech and at one point hides a diplomat inside. It’s exactly the kind of high-information-density fun that GitS always promised.
By Rococo Modem Basilisk on March 24, 2017.
The biggest argument against total dismissal of the remake is the existence of remakes that, despite bringing nothing new to the table structurally and despite clear profit motive, manage to unambiguously improve upon the original.
The iconic example is Evil Dead 2, which is nearly a shot for shot remake of Evil Dead with the same lead actor and the same director — but with the benefit of a larger budget and more experience behind the camera; both Evil Dead and Evil Dead 2 are arguably adaptations of Equinox, with huge improvements to the script and pacing.
We might also consider as a candidate John Carpenter’s The Thing, an improvement over the earlier adaptation of the same source material but one that introduces brand new elements as made available by the technology: fundamentally, the difference between the 1980 film and the 2011 remake is that the 2011 remake exhibited poor craft. But, when we re-make overwhelming successes, we set ourselves up for failure: most factors that contribute to a film being well-received (beyond basic competence) are not predictable or controllable by any one person involved, and can thus be considered essentially random — in other words, remaking a successful film is like using your lottery winnings to enter another lottery.
Why do we see a greater frequency of truly successful remakes in horror than other genres? Because a low-budget horror movie can have low risk and high reward compared to other genres, and a slightly higher budget can result in huge increases in reception: within the entire low-budget range in the horror genre, the odds of making back your investment are pretty good compared to other genres and the stakes are low enough to encourage experimentation. A truly low-budget success can often be improved with an only slightly higher effects budget or the replacement of a handful of actors with slightly better ones, because a low-budget success means the premise and script are probably pretty solid (whereas the technical skills — the most expensive factor at that budget level — can be improved without changing out what works). A low-budget horror success probably makes, in total, a lot less than a high-budget success in any genre, and is successful with a smaller but more fanatical audience, so even a largely failed remake in this context is not just a good bet but a safe one too, the stakes being low.
The tendency to remake good movies is misguided; even though a remake of a mediocre movie is more difficult to advertise, it’s more likely to succeed, both in terms of improving upon the original and in terms of being profitable.
By Rococo Modem Basilisk on March 27, 2017.
While your analysis of control structures here seems accurate, I can’t really agree with the idea that the red tribe is structurally opposed to blue tribe methods. If we’re using Scott Alexander’s terminology here, then when we talk about the red tribe we’re actually talking about an equally top-down system with a different set of values and many parallel structures (the red and blue tribe each have competing newspapers, cable television networks, magazines, radio stations, and brands of beer; not only that, but the red tribe has even duplicated schooling infrastructure in some cases); when we talk about a group associated with collective bottom-up control, we’re talking about the grey tribe, which (trending both libertarian and civil-libertarian) has values that don’t align well with either of the larger tribes. While the grey tribe is ascending in importance, it does not yet wield a lot of political control — and probably won’t until most of the population born before 1970 dies.
The ascent of Trump isn’t really in line with grey tribe values — in terms of traditional political divisions, the grey tribe is pretty evenly divided, and on the one matter they agree upon (preference for bottom-up control over top-down control) he doesn’t have a good track record. He, and his administration, really represents red tribe authoritarianism: power based on charisma and physical dominance signalling rather than credentials and education. He gained power by spending a lot of time playing the broadcast image game in a way that specifically appealed to the red tribe: acting out a fantasy of power based on in-born superior skills that reside in the gut (i.e., the outsider capitalist figure); if he was a successful businessman, that would have given him blue tribe cred, which would have nuked his red tribe cred, but by playing the role of a failed businessman’s idea of a successful businessman on television he catered to his audience.
This is not to say that the grey tribe has nothing to do with it. The ascent of bottom-up media made top-down structures easier to duplicate and scale; the red tribe was able to separate from the blue tribe primarily by repurposing the bottom-up tech of the grey tribe as top-down and duplicating blue tribe systems en masse at low cost. The current polarization is a result: the red tribe and blue tribe are of nearly equal power, and almost completely isolated from each other. They will attempt to cancel each other out, which increases the relative power of the grey tribe.
By Rococo Modem Basilisk on April 3, 2017.
Portable identity is useful only insomuch as single-stable-identity is useful.
Portable identity is useful only insomuch as single-stable-identity is useful. I don’t think either the federation model in general or GnuSocial specifically is ideal for supporting such a thing. That’s not necessarily a bad thing.
New instances are forming as affinity groups around specific shared interests. It makes sense to segregate one’s social networks around clusters of interests that have minimal overlap; one may maintain six or seven different accounts on different instances, and not bother to (or even avoid) linking them.
Having a single identity connected to all your various social groups isn’t in line with natural human behavior — we are a different person with our friends than with our coworkers, neighbours, or parents — and the idea that connecting these separate identities is valuable is mostly an artifact of data mining; the trend (only about a decade old now) toward single universal identities on social media as the default is not only unnecessary but sometimes actually damaging (for instance, someone who is gay, or into kink, or even who has non-sexually-charged yet culturally maligned interests, might not want their activity in those communities connected publicly to their work life or associations with other community).
Your cost estimations are based on a more centralized model than is really reasonable: mastodon.social is an anomaly, but ideally mastodon instances will be small. A more efficient implementation (even if it doesn’t scale) could bring us closer to a 1:1 user-to-instance ratio, which is the direction I’d like to go: when every end user is an instance admin, problems related to admin / user friction over terms of service disappear, and there’s no fundamental reason why you couldn’t run a single-user OpenSocial node off a phone or as a browser extension. (Changes to caching and routing might even make extended downtime from single-user nodes a non-issue: think DHT or Cord.) Even if we don’t expect nodes on phones, we shouldn’t expect expensive third party hosting to remain the norm: AWS & other cloud hosting services are an artifact of this weird return to hierarchy we’ve stumbled into, and if all nodes are effectively owned by three companies there’s very little benefit to decentralization; even most casual users have always-on internet, and therefore can run their own servers.
By Rococo Modem Basilisk on April 8, 2017.
Federation favors large numbers of temporary non- or partially-intersecting identities, rather than a single static centralized one.
The desire for a single static centralized identity on the internet is largely one created by and for existing centralized social networking platforms & is pretty recent, so I don’t see it as much of a loss (although I’m also not a media personality, so I gain more & lose less from having my identity be fluid & disparate — YMMV).
If the OpenSocial platform continues to grow for much longer at this rate, I predict one of three outcomes:
• Tech for running instances will get lighter & easier to manage, so that the ratio of users to instances will approach 1:1. We’ll get to the point where most users have personal instances running on their phones & using an instance run by somebody else is a little like using the computer at the public library. • Most users will have five or six different accounts on different, themed instances, and keep their interests somewhat segregated. (We already see places like witches.town, a.weirder.earth, rich.gop, and oulipo.social, where specific interest groups congregate & the local and federated feeds are much more useful; it may be that mastodon.social and other general-purpose instances will become a deviation from the norm.) • Some big company will jump on the bandwagon by hosting stock mastodon on expensive hardware, making proprietary changes, and heavily advertising it to less-savvy users who have no interest in federation. (Google plus replacement? After all, they nuked Orkut, Buzz, and Wave.) This would be the worst outcome, since the reasons people are interested in OpenSocial are essentially social and political, not technical, and having existing big players involved would run counter to those social and political rationales.
By Rococo Modem Basilisk on April 17, 2017.
If your goal is to find out how people on reddit actually talk about it (rather than what kind of discussion from *other people* on the subject redditors value), a cutoff at the 100 karma threshhold is probably going to produce horribly skewed results: because karma determines presentation order, it grows non-linearly, which means that only a few very popular comments will get more than single-digit karma. If you want to get a smaller data set but keep it representative, I would isolate comments with exactly one karma point.
By Rococo Modem Basilisk on April 21, 2017.
The return of the No Budget Film Contest
The return of the No Budget Film Contest
In 2010 and 2011, I ran a “no budget film contest”. I was annoyed that even with the existence of cheap video cameras & free video editing software the perception remained that even low-budget indie films had to be expensive, so I wanted to showcase good films made for no money at all.
(Then I got busy with work & ended up failing to announce the 2012 one on time, so it stopped.)
Today, the problem is even more extreme: despite most of us in the west already owning phones capable of shooting better video than the equipment used for Clerks, we aren’t seeing a lot of indie films shot with such an expressly low budget, and the mainstream film industry keeps piling more and more money into expensive bombs. Even youtube channels spend money on expensive equipment & buy time from professional animators and editors in order to make their content look a little more slick.
Just as it’s possible to put on a compelling stage play with a single set and one or two actors, it’s also possible to create a compelling film without spending any money at all. It’s a luxury that independent filmmakers as recently as 15 years ago would have killed for: film (and therefore shooting on film) is expensive, and developing prints likewise, so until the advent of cheap digital cameras and the widespread accessibility of large hard disks it really wasn’t possible. But it’s possible now, and we’re not taking sufficient advantage of it.
Budget can be an advantage, but it can also be a disadvantage: spending money on a film, rather than just time, means a greater emphasis on making sure it has a good reception, which (outside of the recklessly overconfident) leads to a conservative attitude and less experimentation. With no budget, you are free to make your film as strange and experimental as you like, so long as it doesn’t cost you any money.
Today, on April 20th of 2017, I am launching the First Annual New No-Budget Film Competition. The submission deadline is July 4th, and I will announce the winners by September 1st.
Rules:
I will announce the winners by September 1st. First prize will be $5 transferred via paypal. The top three submissions (judged any way I like) will be posted on this blog, and will get the titles of Best No-Budget Film of 2017, Second Best No-Budget Film of 2017, and Third Best No-Budget Film of 2017. They will also get a picture of a golden coin trophy.
How to submit: Upload your finished films to youtube or vimeo (or some other streaming video site), and post the link in the comment thread of this post. The films may be of any length, and you can enter as many times as you like. (If you cannot post comments on Medium for some reason but still want to enter, contact me on twitter or mastodon)
By Rococo Modem Basilisk on April 21, 2017.
Folk horror as speculative sociology
H.P. Lovecraft and the Wicker Man
[1]
Folk horror is having a Renaissance, as the novelty cycle revisits the seventies at two iterations’ remove & the SF community starts again to seriously analyze the dialogue between the weird and the hauntological. The spring season, with Easter, Walpurgisnacht, and May Day, is a good time to revisit this, and, as expected, various publications have — not just the usual suspects like Scarfolk, but also The Guardian, which published a piece whose analysis I’d like to pick apart a bit.
Newton’s analysis suggests a rural versus urban dimension (and, by extension, a modernity versus tradition dimension), and while this exists in the text, I consider it shallow. I’d like instead to argue that, rather than being in the tradition of gothic and romantic horror, folk horror has more in common with the point at which weird fiction intersects with science fiction.
We could choose no better an entry point than H. P. Lovecraft, whose horror stories are best seen as science fiction whose science is ‘anthropology’ (the same way J. G. Ballard is a science fiction author whose science is ‘psychology’).* Nobody really considers Lovecraft to write folk horror, but Lovecraft’s formula — similar across much of his work to the point of bordering on self-parody — is very similar to how Newton describes folk horror’s core narrative:
The films feature a recurring archetype: the arrival of a stranger, the discovery of a secret cult, then a vicious murder, perhaps a sacrifice, designed to propitiate pagan gods. The metropolitan visitor, the outsider from the mainland, comes into a situation strange to them and to us. Here the enlightened laws of the nation do not pertain. In these forgotten spaces, there are other laws: rules and rituals that are both familiar remnants of some tribal memory yet utterly strange. The locals understand, while we do not.
Lovecraft’s formula, written in a similar way would be:
The stories feature a recurring archetype: the discovery of a secret cult, by a stranger, and the discovery that this cult’s beliefs are substantially factually correct. The metropolitan visitor, the outsider from the university, comes into a situation strange to them and to us. Here the enlightened laws of scientific common sense do not pertain. In these forgotten spaces, there are other laws: rules and rituals that are both familiar remnants of some tribal memory yet utterly strange. The locals understand, while we do not.
Essentially, the core difference between Lovecraft’s stories and the typical folk horror is that in Lovecraft’s stories the superstitions of the locals are demonstrated by science to be justified, while in folk horror these alien beliefs are implied not to be held even by the high priests. Lovecraft, to make up for the assumption that superstitions will be dismissed, must make the superstitions true; in folk horror, the superstitions are dangerous for reasons unrelated to their truth.
Newton goes on to say:
Their rootedness in place becomes uncanny. Once, almost everyone was so rooted. But now — in the discontinuous world of modernity, where relationships are casual and work comes and goes — such belonging feels strange and even sinister.
I would argue that the rootedness is in many cases intentionally illusory: private doubts play a big role in the genre, as a motivation for extreme behavior as well as an excuse for escalating the stakes. Our protagonist is often brought in by an insider with secret doubts, after all.
What really triggered my analysis is this:
being inside a myth is terrifying, a fall from the industrialised, supermarket world into one possessed by abysmal powers
I think Newton gets this almost exactly wrong. It’s not that being inside a myth is terrifying: mythology is the natural home of man, and it’s questionable whether or not it’s even possible for us to venture outside that domain. Instead, the source of this terror is being inside someone else’s myth — an alien myth — and drowning in it, without possibility of escape or assimilation.
Our fish out of water character is thrust into a world shaped by minds no less alien for the sake of being human, and it’s not xenophobia or ignorance but the cold mechanism of someone else’s unexamined culture that chews him up. He cannot know of this secret cult’s beliefs and signs because it is secret: he is an accidental anthropologist who only realizes how far he is from home once his life is already forfeit by the alien logic of the cultists.
Hot Fuzz illustrates this brilliantly, being a send-up of folk horror that also functions as excellent folk horror: the alien belief of the cultists is a slightly more extreme form of a common belief — the patriotism of someone who wants to maintain a good image of their homeland, even if that image doesn’t correspond with reality. The cultists aren’t fully alien; people like this exist in London too, or else there would not be spiked benches. The only difference is that these people separate their image from their reality more effectively, by meeting at night in disguises rather than in public at town hall meetings, and so their behavior can become more extreme because they no longer need to integrate these two sides of their personality.
It’s here that folk horror becomes interesting from an analytic perspective. Much as science fiction injects strangeness into familiar situations in order to force a new perspective on them, folk horror takes all of the inhuman mechanics of human society — all the casualties of belief — and externalizes them so that we can recognize them.
This only really works when there’s a solid understanding. Compare the original The Wicker Man (a great representation of how hippie rhetoric can hide a control-freak nature) to the remake (which, despite having an identical script, somehow became a warped rant against a straw feminism), and then compare both to The Love Witch (which plays with ideas about femininity and female empowerment in paganism from a feminist perspective).
This externalization of normal social behaviors as occult is also something that separates folk horror from adjacent works. As much as The Wicker Man has in common with Mario Bava’s Kill Baby Kill, the latter is not folk horror: the supernatural phenomenon is not only real but also alien to the residents, who don’t have a much firmer grasp on the mechanics of the haunting than the protagonist. An American Werewolf in London is a similar case: the protagonist doesn’t collide with a social formation but with a supernatural formation with a social aspect formed around it (much like in Lovecraft, where the physical reality of the creature justifies the folk beliefs even to outsiders); an imaginary version of that film, wherein that wolf-haunted town hunts down residents without any textual evidence that the werewolf is real, would certainly qualify.
We ourselves live in societies constructed around imaginary ghosts, designed to avoid imaginary monsters and appease imaginary gods. We live as though these things have fury, even though the fury that touches us is that of the society. Considering the superstitions of others, and their victims, we can get closer to considering our own: is it fundamentally more sane to sacrifice people for money than for the harvest, given a similarly powerful and dangerous system of social enforcement? What justifications do we use for hurting people that would be horrifying to an outsider?
It’s all in the name, of course: folk horror is about the folk — not merely folks, but the collective. We don’t cease to be folk by moving to the city. We just take on new mythologies.
*I’m going to gloss over the flaws in Lovecraft’s anthropology, which is discussed at length elsewhere. I cannot hope to summarize the discussion around it, other than to say Lovecraft’s ideas were not accepted or acceptable in his era and circumstance. It is not the substance of Lovecraft’s ideas about people unlike himself that interest me here, but instead the form of the anthropologist-hero in horror, along with the idea that horror is driven by awareness of alien ideas rather than by the direct threat of violence. The Love Witch and The Haunting of Hill House both use warped or alien ideas about life as a source of horror, without othering the source of these ideas by class or culture to a significant degree.
By Rococo Modem Basilisk on May 1, 2017.
Maybe it’s because “Marine Antoinette” sounds so much more natural that “Macron Antoinette” is failing to catch on? After all, supporters of Le Pen would nuke their own cause by accidentally saying the former — something I’d expect to happen automatically, if they try to repeat it enough without automation.
By Rococo Modem Basilisk on May 1, 2017.
I came to the same conclusion from another direction:
Thinking in terms of maintainability under mental load (i.e., debugging on little sleep with a deadline / whatever), idioms trade space in code for mental load, so long as those idioms are appropriately applied.
In other words, if you write code with the awareness that every idiomatic structure is one chunk and every major divergence from idiom counts as a chunk (and everything that looks like an idiomatic use but actually relies on some minor detail to change the behavior non-trivially — i.e., obfuscation — counts as several chunks, since at the very least the programmer must identify the idiom then identify the actual behavior), appropriate use of idioms can reduce the code size in terms of number of chunks (and thus reduce mental load). This corresponds to engineer-time much better than number of lines or number of characters do.
When people complain about boilerplate, they are usually complaining about the misuse of idioms (either enforced by language or style guides or enforced by a programmer’s ignorance): a java program written in ‘good’ java style for solving a problem not well suited to that set of abstractions can easily contain more trivial classes / beans / interfaces than an equivalent program in a more well-suited language contains lines of code — in other words, the chunk count actually increased by trying to force the solution into an inappropriate set of idioms, because in addition to solving the actual problem the program also solves the additional non-trivial problem of idiom conversion.
When I am not developing interactively or writing throw-away code on a deadline, I avoid features that allow me to compress many mental chunks into a small number of characters (particularly in languages like python, where such features are extremely powerful), because I know that no matter what I do, a bug in such a heavy line will be many times harder to fix (and the behavior many times harder to reason about) as soon as I’ve GC’d my mental model of it & no longer have a detailed internalized map of execution in working memory.
I have a coworker who favors such constructs, and finds idiomatic code hard to understand. However, he thinks about code in terms of how the interpreter or compiler works (learning languages by reading their implementations) and his chunk size is determined by implementation AST — in other words, he has an unusual way of thinking about and reading code that is unconcerned with programmer intent. I would recommend anyone who does not model programs in this way to embrace the force multiplier of careful idiom use.
By Rococo Modem Basilisk on May 2, 2017.
Invisible Architecture I
A Space of Death and Madness
[1] Bas-relief from the entry to Yale’s Sterling Memorial Library, whose structure & style is modeled off gothic cathedrals, showcasing magic symbols from around the world and across history
Dario Argento’s best-known film, Suspiria, centers itself around a building: the Black Forest Dance Academy in Freiburg. The first film of the Three Mothers trilogy, it sets the formula: three witch-matriarchs, each tied to a magically-powerful building, and each with a coven of thralls around her. These buildings are designed by a man named Varelli, and it is Varelli’s confessions, in the form of a rare book called The Three Mothers, that constitute the primary defense against each witch. Despite this, in Inferno, the second film of the trilogy, we see Varelli — his life unnaturally extended — is now in thrall to his own creation, mute, senile, and unable to resist the will of The Mother of Darkness. Varelli is architect as Faust-figure: contracted by demonic forces to create closed worlds of death and madness, he acts as an unwilling bridge between their mythic realm and the solid world of granite and stained glass, and he is necessarily canny to their nature and plans.
This same theme is present in two other, otherwise very different films: High Rise (2016) and Ghostbusters (1984). In each case, a brilliant architect creates a space of death and madness that perversely bridges previously-separate worlds, which bewitches its inhabitants and corrupts the architect himself. In High Rise, the only uncorrupted person is the psychiatrist Laing — presumably named for R. D. Laing — who takes limited, active, intuitive control of his environment by painting his apartment a dull grey color and regains his sanity from it. He is notable, however, for his emotional disconnection from the environment. In Ghostbusters, similarly, the uncorrupted figures are emotionally distant and have an engineering-focused approach to the world.
The idea that architecture can have an essentially magical effect on the world is, therefore, quite mainstream — any idea that appears in popular films across genres must be considered mainstream. The idea of architect as magician is not unknown to occultists (and has proximity to the idea of the genius loci), and we can also draw connections to psychogeography and the semi-mystical ideas about the power of architecture held by figures like Le Corbusier and Frank Lloyd Wright, but its presence in these stories holds special dimensions and attributes.
The idea of a perverse house, haunted by its own design or history, is essentially a gothic one. But such a building is created by tragedy (Overlook Hotel, Hill House), madness (Chapelwaite, the Winchester Mystery House), or incompetence (anything on McMansion Hell). Even psychogeography suggests that the influences upon the character of a city are typically unconscious and emergent. What we see in these examples instead is a space engineered to be perverse — something that the Situationists, even with their gnostic tendencies, didn’t predict.
Did they need to predict it, though? The imposing totalitarian architecture of Albert Speer certainly represents a precedent for the considered and documented engineering of psychological responses via spaces, although this is still closer to Le Corbusier than to Ivo Shandor.
Instead of trying to compare the Situationists’ unitary urbanism of a nootropic Disneyland to Ruinenwerttheorie, perhaps it’s better to come in from the direction of the occult.
The idea of a genius loci tied to a figure or line is hardly outside the mainstream; it’s common enough in folklore that it was a staple of early Disney films. A corrupted landscape is associated with the rule of a corrupt figure, as in Sleeping Beauty, The Lion King, & Lord of the Rings. The emergence of a divinely-approved ruler reshapes the landscape favorably & his death darkens it, while conflicts related to that ruler create surreal pocket-worlds full of madness & numinous religious imagery (as in Excalibur & Le Mort D’Artur). But the power of an architect is not like the power of a king. The corruption of a land by its king is tied deeply to divine blessing: a righteous king makes decisions in line with the will of god, and so the king acts as a repeater and amplifier for god’s will. On the other hand, an architect, like a corrupt king in this system, is always actively creating changes to the world out of line with nature, whether or not it is in line with a correct idea of a better future world.
It may also be useful to look at alchemy and freemasonry, here. The architect creates a space in the image of his mind and then creates a mind in the image of his space (“we shape our tools and thereafter our tools shape us”), presumably refining the design — smoothing each ashlar, diluting and coagulating. Without invoking the mandate of heaven or some circular justification for calling everything natural, we can hardly say that this careful and mindful practice of self-modification is in line with the mindless and mechanical ‘natural’ flow — it’s a magical act on the spectrum closer to writing a novel than to mowing the lawn, in terms of imposition of will upon the world.
We can see within the perverse spaces of architect-wizards a shallow anti-masonry, and a distrust of magic in general — perhaps this is true of Argento, who claimed he was inspired to write Suspiria by discovering a Steiner school nearby. But we can also see it as a warning about ungrounded introspection: madness can come from looking too closely at the strange loop in our mind and letting the feedback amplify tiny momentary glitches in our thinking into huge permanent distortions. We can build monuments in stone out of the knots in our heads, if we aren’t careful.
Once again, High Rise is the odd duck here. Our architect is not really a pawn or an antagonist. He creates a mad space out of a desire to create any kind of new social relation (most of which will necessarily seem mad). Our protagonist is not working against the madness of the space, but instead is merely immune to it. And, finally, the gateway he created did not connect the mundane with the supernatural but instead connected the upper and lower classes in a pathological way — close enough to eat each other but far enough to breed distrust.
It’s easy to read High Rise as anti-Marxist, anti-utopian, anti-anti-psychiatry, or anti-Spectacle. But in this context, I read it as having two messages, one rare and the other common: social relations, like those created by engineered spaces, are extremely powerful and hard to predict and don’t call up what you can’t put down.
By Rococo Modem Basilisk on May 2, 2017.
Visible & Invisible Architecture
A Space of Death and Madness
[1] Bas-relief from the entry to Yale’s Sterling Memorial Library, whose structure & style is modeled off gothic cathedrals, showcasing magic symbols from around the world and across history
Dario Argento’s best-known film, Suspiria, centers itself around a building: the Black Forest Dance Academy in Freiburg. The first film of the Three Mothers trilogy, it sets the formula: three witch-matriarchs, each tied to a magically-powerful building, and each with a coven of thralls around her. These buildings are designed by a man named Varelli, and it is Varelli’s confessions, in the form of a rare book called The Three Mothers, that constitute the primary defense against each witch. Despite this, in Inferno, the second film of the trilogy, we see Varelli — his life unnaturally extended — is now in thrall to his own creation, mute, senile, and unable to resist the will of The Mother of Darkness. Varelli is architect as Faust-figure: contracted by demonic forces to create closed worlds of death and madness, he acts as an unwilling bridge between their mythic realm and the solid world of granite and stained glass, and he is necessarily canny to their nature and plans.
This same theme is present in two other, otherwise very different films: High Rise (2016) and Ghostbusters (1984). In each case, a brilliant architect creates a space of death and madness that perversely bridges previously-separate worlds, which bewitches its inhabitants and corrupts the architect himself. In High Rise, the only uncorrupted person is the psychiatrist Laing — presumably named for R. D. Laing — who takes limited, active, intuitive control of his environment by painting his apartment a dull grey color and regains his sanity from it. He is notable, however, for his emotional disconnection from the environment. In Ghostbusters, similarly, the uncorrupted figures are emotionally distant and have an engineering-focused approach to the world.
The idea that architecture can have an essentially magical effect on the world is, therefore, quite mainstream — any idea that appears in popular films across genres must be considered mainstream. The idea of architect as magician is not unknown to occultists (and has proximity to the idea of the genius loci), and we can also draw connections to psychogeography and the semi-mystical ideas about the power of architecture held by figures like Le Corbusier and Frank Lloyd Wright, but its presence in these stories holds special dimensions and attributes.
The idea of a perverse house, haunted by its own design or history, is essentially a gothic one. But such a building is created by tragedy (Overlook Hotel, Hill House), madness (Chapelwaite, the Winchester Mystery House), or incompetence (anything on McMansion Hell). Even psychogeography suggests that the influences upon the character of a city are typically unconscious and emergent. What we see in these examples instead is a space engineered to be perverse — something that the Situationists, even with their gnostic tendencies, didn’t predict.
Did they need to predict it, though? The imposing totalitarian architecture of Albert Speer certainly represents a precedent for the considered and documented engineering of psychological responses via spaces, although this is still closer to Le Corbusier than to Ivo Shandor.
Instead of trying to compare the Situationists’ unitary urbanism of a nootropic Disneyland to Ruinenwerttheorie, perhaps it’s better to come in from the direction of the occult.
The idea of a genius loci tied to a figure or line is hardly outside the mainstream; it’s common enough in folklore that it was a staple of early Disney films. A corrupted landscape is associated with the rule of a corrupt figure, as in Sleeping Beauty, The Lion King, & Lord of the Rings. The emergence of a divinely-approved ruler reshapes the landscape favorably & his death darkens it, while conflicts related to that ruler create surreal pocket-worlds full of madness & numinous religious imagery (as in Excalibur & Le Mort D’Artur). But the power of an architect is not like the power of a king. The corruption of a land by its king is tied deeply to divine blessing: a righteous king makes decisions in line with the will of god, and so the king acts as a repeater and amplifier for god’s will. On the other hand, an architect, like a corrupt king in this system, is always actively creating changes to the world out of line with nature, whether or not it is in line with a correct idea of a better future world.
It may also be useful to look at alchemy and freemasonry, here. The architect creates a space in the image of his mind and then creates a mind in the image of his space (“we shape our tools and thereafter our tools shape us”), presumably refining the design — smoothing each ashlar, diluting and coagulating. Without invoking the mandate of heaven or some circular justification for calling everything natural, we can hardly say that this careful and mindful practice of self-modification is in line with the mindless and mechanical ‘natural’ flow — it’s a magical act on the spectrum closer to writing a novel than to mowing the lawn, in terms of imposition of will upon the world.
We can see within the perverse spaces of architect-wizards a shallow anti-masonry, and a distrust of magic in general — perhaps this is true of Argento, who claimed he was inspired to write Suspiria by discovering a Steiner school nearby. But we can also see it as a warning about ungrounded introspection: madness can come from looking too closely at the strange loop in our mind and letting the feedback amplify tiny momentary glitches in our thinking into huge permanent distortions. We can build monuments in stone out of the knots in our heads, if we aren’t careful.
Once again, High Rise is the odd duck here. Our architect is not really a pawn or an antagonist. He creates a mad space out of a desire to create any kind of new social relation (most of which will necessarily seem mad). Our protagonist is not working against the madness of the space, but instead is merely immune to it. And, finally, the gateway he created did not connect the mundane with the supernatural but instead connected the upper and lower classes in a pathological way — close enough to eat each other but far enough to breed distrust.
It’s easy to read High Rise as anti-Marxist, anti-utopian, anti-anti-psychiatry, or anti-Spectacle. But in this context, I read it as having two messages, one rare and the other common: social relations, like those created by engineered spaces, are extremely powerful and hard to predict and don’t call up what you can’t put down.
By Rococo Modem Basilisk on May 2, 2017.
All fat middle-aged white guys are secretly the same person, like Saint Gulik.
By Rococo Modem Basilisk on May 3, 2017.
The end-game of the voice UI (like that of the chat UI) is the command line interface.
The end-game of the voice UI (like that of the chat UI) is the command line interface. So, it’s useful to take cues from currently-existing good CLI UX. (For instance, look at the differences between zsh & command.com, and the trends in the evolution of borne-compatible command shells since 1970.)
To start out with, there are a handful of differences between interfaces centering around how learning curves are managed. While all major command line interfaces front-load some learning (i.e., the user is expected to learn quite a bit early in the experience in order to understand basic operations — something that GUIs are loathe to do, and something that therefore limits how nuanced the control user have over GUIs can be with standard widgets), unix shells made big leaps in discoverability early: as of the GNU announcement in 1984, Stallman was already saying that any command shell should be expected to have auto-completion features, and online documentation systems like man & apropos already existed (soon to be joined with the hypertext system info) — this at a time when both the Macintosh & the Amiga were still under development & most users had never seen a GUI, and during a period when users were generally split between Microsoft BASIC & MS-DOS in terms of their command environment.
No virtual assistant I am aware of will list available functions or list the set of invocations they accept — in other words, there is no help system comparable to man or apropos — and since error reporting is nearly nonexistent, this means interacting with unfamiliar features is the equivalent of playing a classic text adventure. “I see no lamp here,” says Siri. This kind of interface is fine for a game (where part of the fun is figuring out the systems and thereby beating the snarky & frustrating UI), but if we intend to do real work with virtual assistants they need to operate a lot more like a good CLI and provide detailed and specific technical information about their functionality. (The Arctek Gemini, a robot released in 1982, had a voice-controlled virtual assistant that could do this; why can’t Apple?)
The second big factor is that, if we want to do real work with these interfaces, we need to reduce the amount of fuzzy matching and move toward a system in which every word is expected to be meaningful. Picking one or two key words out of a sentence will give few false positives when performing simple tasks, but cannot scale: looking at programming languages that resemble english (such as SQL) gives us a good idea about how pronounceable keywords can be combined in relatively natural ways to allow the formation of specific, precise, and complex queries. Having some front-loading of learning is necessary for truly nuanced queries, but simple ones could still sound like natural speech. Combined with a built-in help system, the mechanisms for using this could be made very discoverable.
Mode indication is a problem in speech interfaces that have complex behaviors. We expect a great deal of stacked context, and even today’s systems, which are capable of doing next to nothing, tend to fail miserably at consistently keeping track of stacked context or falling in and out of different modes predictably.
Finally, a big problem is that almost everybody who has learned to type can type faster on a real keyboard than they can speak. On-screen keyboards on smartphones, disabilities affecting the hands, and contexts where keyboards are not handy or usable may justify speech-based interfaces even when they are otherwise not ideal; some of these cases are better-served by chording keyboards or by improvements to predictive text systems.
If we want speech-controlled virtual assistants to graduate from toy to tool, we can’t allow ourselves to be seduced by flashy but ultimately hollow flat-pack futurism, and must instead admit that this tech will be inappropriate in most situations: voice control will be rude in public and unnecessary in the home, no less distracting while driving than actually typing on the phone, and less effective in the workplace than speaking to a coworker about the same topic.
The killer apps for this tech at the moment seem to be fielding questions from pre-literate children and taking orders while an adult is cooking; a move toward having clear and precise syntax could make it possible for more complex queries to be asked (especially those in the domain of what non-technical users assume current voice assistants can handle but that they can’t — expecting “will it rain next Tuesday” to work because “will it rain tomorrow” does), and build-in help systems can encourage curious children to learn the kind of thinking that underlies programming, giving them a leg up in school when it comes to mathematics and grammar.
By Rococo Modem Basilisk on May 8, 2017.
Considerations for programming language design: a rebuttal
Considerations for programming language design: a rebuttal
Walter Bright, who designed D, wrote an article called “So You Want to Write Your Own Language”. I love D and I respect Mr. Bright, but I disagree with nearly everything he says in it, mostly because D itself and Digital Mars’s work in general is (and should be) extremely atypical of the kind of programming language design that occurs today.
Digital Mars has been writing cross-platform compilers for decades (and when I was first getting into C on Windows in the early naughts, Digital Mars’s C compiler was the one that gave me the least trouble), and D was positioned as an alternative to C++ as a general purpose language. The thing is, most languages being developed today aren’t going to be C-like performance-conscious compiled languages that need to build binaries on multiple platforms and live in environments with no development toolchains (all major platforms either ship with a GNU or GNU-like UNIX dev toolchain or have a system like cygwin or homebrew that makes installing one straightforward), and trying to compete with C++ is not only technically difficult but also foolish for wholly non-technical reasons.
Design
The second piece of advice Mr. Bright gives is to stick relatively close to familiar syntax, in order to maximize audience. This makes sense if you are trying to compete with C++, as D does. But D is a great illustration of the problems with this approach: D does everything C++ does better than C++ does it, and yet it has failed to displace C++. The reason is that C++ is a general purpose language, like Java, C#, and Go (in other words, a language that performs every task roughly equally poorly) and the appeal of general purpose languages is that they allow one to trade time learning a language that’s a better fit for the problem for immediate implementation difficulty.
In other words, most things that are implemented in C++ are done that way not because C++ is the best tool for the job, but instead because C++ is marginally better suited than the three other languages that everybody on the dev team has known inside-out for twenty years. D can’t compete because the whole point of a general purpose language is to appeal to devs who failed the marshmallow test & want to minimize the initial steepness of the learning curve.
The domain of general purpose languages is crowded, and each of them casts a wide net: being technically better across the board than all of them is difficult, and without really heavy institutional support (like that given to C++ and Java by university accreditation boards & given to C# and Go by large corporations) being technically better will only give you a small community of dedicated fans and an isolated ecosystem. Writing a general purpose language is a bit like forming 2-person garage startup positioned against General Mills.
Instead, it makes more sense to target some underserved niche: general purpose languages are necessarily bad at serving most niches, and the syntax changes necessary to serve such a niche better are generally all too obvious to anybody actually developing in such a niche. Iteratively identifying and relieving pain points in real development work is sufficient to vastly improve a conventionally-structured language. Cutting the gordian knot by introducing a wildly unconventional syntax that’s better suited for certain kinds of work can be even easier, since such a syntax is allowed to be poorly suited for work outside its domain, and since the syntax of conventional general-purpose languages is made complicated & hard to reliably implement by decades of pressure to be suitable for all kinds of very dissimilar problems.
Syntax is the difference between a tea spoon and an ice pick. Rather than aspiring to being vice grips or duct tape, build a language that is exactly the tool you need, and allow it to be a poor fit for everything else.
When beggining a language project, it makes sense to try to get it minimally functional as quickly as possible, so that you can dogfood it & start identifying where your model of suitability has flaws. So (in direct contradiction to Mr. Bright’s list of false & true gods) it makes sense to start off with the easiest suitable syntax to parse and as few keywords as you know how to proceed with. As you write code in your language, you will identify useful syntactic sugar, useful changes to the behavior of corner cases, and keywords you would like to add — and none of these are likely to be the ones that were at the bottom of your bucket list initially, unless you’re sticking quite close to some known conventional language.
To mimic Mr. Bright’s format, here are some false gods of syntax:
1. Tried and true / universally-familiar syntactic constructs. A language should not sacrifice the ease with which it caters to its niche in order to cater to people who don’t want to really learn it, but instead should have a syntax that minimizes the effort necessary to solve problems within its domain and that corresponds predictably to its operation. If a language is syntactically similar to some other language, this similarity should reflect a semantic similarity (i.e., a stack language is justified in looking like forth and a declarative logic language is justified in looking like prolog). Avoid making a functional language look like an imperative one or using constructions that are misleadingly/shallowly similar to ones from another popular language, and instead choose metaphors for best fit. 2. Over-design / biting off more than you can chew. Start off with a minimally viable language and determine what needs to be added from experience. Over time the gaps will be filled in and your language will look less like an esolang and more like a general purpose language, unavoidably; making a sturdy foundation is easier if you start off with only a few elements and making them quite solid. (I’m not encouraging the kind of masturbatory minimization of keyword lists you sometimes see in forth implementations, where only one or two keywords are hard-coded and everything else is written in the language; however, starting off with functions, conditions, i/o, and simple mathematics is generally sufficient to start writing real code, and it’s generally possible to get a simple language from zero to capable of hosting a fibbonacci sequence function in an afternoon.) 3. “Readability”. Reading any language is a learned skill; while you should know how to parse the language you’re designing (don’t make it too hard for yourself since you’re going to spend a lot of time puzzling over the proper behavior of code you yourself wrote), caring too much about lowering the initial learning curve of new developers will put more work on your plate and limit your flexibility. I know people who consider C++ easy to read, and even a very straightforward language like forth or brainfuck looks like line noise to someone who doesn’t know the trick to reading it. If your language is well-suited to some problem, you need to trust that developers will figure out how to read it, and having a simple syntax that doesn’t cater too much to beginners will help more seasoned developers understand complicated code later.
Here are some true gods of syntax:
1. Suitability. Be an ice pick, not duct tape. 2. Simplicity. Parsing is hard enough without ambiguity and corner cases (for both humans and machines); create a simple foundational syntax with as little ambiguity as possible, and make sure every behavior you add is compatible with that foundation. 3. Iterative development. Making a really minimal language is straightforward, but making a language that is useful for complex problems is hard; it’s much easier to add features slowly based on need than it is to determine all of the useful behaviors before starting development, and it’s much easier to test a small number of base features before building on top of them than to test a large complex language all at once.
Implementation
Regex is a very powerful tool, if you know how to use it. Lexing involves identifying character classes, which is exactly what regex is best at. I wouldn’t recommend writing large chunks of your language as regular expressions, or using nonstandard extensions like lookahead & brace matching (which break some of the useful performance guarantees of vanilla regex anyway), but regex is invaluable for lexing, and with care it can be invaluable for parsing as well.
If you are writing your language implementation in C, using lex (or GNU flex) can be a great time saver. I wouldn’t recommend using Bison / YACC, personally — it’s possible to thread arbitrary C code into lex rules, and implementing parsing with this feature and a set of flags is much easier for simpler syntaxes than using a compiler-compiler. With regard to portability concerns, any major platform will have a development toolchain containing lex these days.
Of course, if you go the forth-lite route and have nearly completely consistent tokenization along a small set of special characters, this is much easier. Forth-lite languages can be split by whitespace and occasionally re-joined when grouping symbols like double quote are found; lisp-lite languages with only a few paired grouping symbols can easily be parsed into their own ASTs.
It is possible to design a series of regex match expressions and corresponding handler functions, and iteratively split chunks of code from the outside in. I did this in Mycroft, and I recommend it only if your syntax requires very little context: the two different meanings of comma and period in Mycroft (as well as handling strings) made parsing this way complicated in some cases, but this kind of parsing is very well-suited for languages like lisp where all grouping symbols are paired and very few characters are special-cased. This is a style I find quite natural, but I understand that many people prefer thinking of parsing in terms of a left-to-right iteration over tokens; one benefit is that grouping symbol mismatch will be reported in the middle of a block rather than at the very end (i.e., closer to where the actual mismatch probably is located, for large blocks with many levels of grouping).
In all of these cases, you should make sure that information about both position in the file & the tokenization & parsing context are available to every function, for error reporting purposes. If your syntax is sufficiently simple & consistent, this will be enough to ensure that you can produce useful error messages. Never expect any general purpose parsing or lexing tool to understand enough about your language to emit useful error messages; instead, expect to thread your own context through your implementation.
I agree with Mr. Bright about the proper way to handle errors: rather than trying to make the compiler or interpreter smarter than the programmer, it makes more sense to exit and print a sensible error message. However, he doesn’t touch upon the difference between compile-time and run-time errors here, and in an interpreted language or one that is compiled piecemeal at runtime (which will be typical of the kind of language somebody writes today) the distinction is a little more fuzzy. Rather than, as Mr. Bright suggests, exiting upon encountering any error, I recommend keeping information about current errors (and the full call stack information related to them) and going back up the stack until an error handler is reached; if we completely exhaust user code and get to the top level, we should probably print the whole call stack with our message (or some large section of it). This is the style used in Java, Python, and sometimes Lua. It’s straightforward to implement (Mycroft does it this way, with ~10 lines of handling in the main interpreter loop and less than 40 lines in a dedicated error module), and it can provide good error messages and the framework for flexible error handling in user code (which itself can be implemented easily — see Mycroft’s implementation for the throw and catch builtins).
With regard to performance: it’s not that performance doesn’t matter, but instead that performance matters only sometimes. Machine time is much cheaper than enginer time, and so a language that makes solving certain kinds of problems straightforward is valuable even if it does so slowly. A general purpose language has to run fast and compile fast in order to compete, but notoriously slow languages have become popular because they served particular niches, even in eras when processors were much slower and speed mattered much more (consider perl in the text-processing space — although, because of accidents of history, it took a positon better served by icon — or prolog in the expert system space; prolog was so well-suited to the expert system space that expert systems were prototyped in prolog and then hand-translated from prolog to C++ in order to avoid the performance overhead).
Unless you make huge dumb performance mistakes (and as the success of Prolog, PHP, Python, and Ruby makes clear, sometimes even big dumb performance mistakes aren’t fatal), if you scratch an itch the performance won’t be a deal-breaker, and optimization can come later, after both behavior and idiom are appropriately crystallized. Using a profiler is overkill in early iterations of the language, and since there’s a general trend in hardware toward many lower-power cores, you may get significantly more performance benefit from making parallelism easy to use (as Go does) than from tuning the critical path.
Lowering
Lowering is just an inverted way of looking at keyword minimization. When several mechanisms perform the same basic task with only small differences (as is the case with the three big looping constructs in imperative languages that Mr. Bright mentions), it makes sense to write a general function and then write syntactic sugar around it for the other cases, and if for some reason you have failed to do it that way in the first place then refactoring it (i.e., lowering) makes sense. However, I would recommend spending more initial effort identifying how to write well-factored builtin functions in the first place, rather than iteratively refactoring redundant code, because you will save debugging time & avoid problems related to minor differences locking in separate implementations that should be merged (for instance, in Python not only are dictionaries separate from objects, but there’s also a distinction between old-style and new-style objects, even though implementing objects and classes as a special case of dictionaries as Javascript and Lua do would make more sense).
Runtime library
Nearly all of Mr. Brights points about runtime libraries only really apply to languages competing with C++. He focuses on low-level stuff necessary for cycle-heavy processing on single cores, but this is almost entirely irrelevant to most of the code that gets written these days.
Here are my suggestions:
1. String I/O should be unicode-aware & support utf-8. Binary I/O should exist. Console I/O is nice, and you should support it if only for the sake of having a REPL with readline-like features. Basically all of this can be done by making your built-in functions wrappers around the appropriate safe I/O functions from whatever language you’re building on top of (even C, although I wouldn’t recommend it). 2. It’s no longer acceptable to expect strings to be zero-terminated rather than length-prefixed. It’s no longer acceptable to have strings default to ascii encoding instead of unicode. In addition to supporting unicode strings, you should also probably support byte strings, something like a list or array (preferably with nesting), and dictionaries/associative arrays. It’s okay to make your list type do double-duty as your stack and queue types and to make dictionaries act as classes and objects. Good support for ranges/spans on lists and strings is very useful. If you expect your language to do string processing, built-in regex is important. 3. If you provide support for parallelism that’s easier to manage than mutexes, your developers will thank you. While implicit parallelism can be hard to implement in imperative languages (much easier in functional or pure-OO languages), even providing support for thread pools, a parallel map /apply function, or piping data between independent threads (like in goroutines or the unix shell) would help lower the bar for parallelism support. 4. Make sure you have good support for importing third party packages/modules, both in your language and in some other language. Compiled languages should make it easy to write extensions in C (and you’ll probably be writing most of your built-ins this way anyway). If you’re writing your interpreted language in another interpreted language (as I did with Mycroft) then make sure you expose some facility to add built-in functions in that language. 5. For any interpreted language, a REPL with a good built-in online help system is a must. Users who can’t even try out your language without a lot of effort will be resistant to using it at all, whereas a simple built-in help system can turn exploration of a new language into an adventure. Any documentation you have written for core or built-in features (including documentation on internal behavior) should be assessible from the REPL. This is easy to implement (see Mycroft’s implementation of online help) and is at least as useful for the harried language developer as for the new user.
After the prototype
If your language is useful, you will use it. Maybe some other people will as well. Writing a language is a great learning opportunity even if nobody uses it, since it improves your understanding of the internals of even other unrelated languages.
Unless you are selling some proprietary compiler or interpreter (something that even Apple & Microsoft can’t get away with anymore) adoption rates don’t actually matter, except for stroking your ego — which is great, because there are a lot of languages out there and relatively few people are interested in learning obscure new ones.
If your language gets any use, it will grow and mature into something harder to predict as behaviors change to be more in line with the desires of users (even if the only user is yourself). When languages grow too fast or are built on top of a shaky foundation they can become messy, with new behaviors contradicting intuition; some languages have taken a tool-box approach and embraced this (notably Perl and to a lesser extent TCL), and other languages merely suffer from it (notably PHP and C++). It makes sense to try to make sure that new features behave in ways that align as closely as possible to idiom, and to develop a small set of strong simple idioms that guide all features; that way, pressure to become general purpose won’t make the learning curve for new users too rocky.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
By Rococo Modem Basilisk on May 22, 2017.
If you understood this essay as advocating for some language, you didn’t read it carefully.
I’m familiar with D, and I like it a lot. I’m also familiar with the story behind how D came about.
I’m not even remotely advocating for Mycroft. Mycroft, as a language, is a poor fit for basically all problems, and is also very broken. However, it provides clear examples of solutions to several language design problems. I don’t recommend even solving problems related to language design in a way similar to how I did in Mycroft — especially parsing, which barely works for the reasons I mentioned.
My point is that D is an atypical case, with regard to the kind of programming language that someone who has never written a programming language before would begin writing in 2017. In other words, Mr. Bright’s original article, if read by some beginner programmer, would lead them to attempt to write something very much like D — and one D is more than enough, not to mention that writing another D is much harder than writing another Lua or Forth.
Outside of the extremely competitive “general purpose language” group (red in tooth and claw, with Google, Apple, Microsoft, and Oracle all pushing heavies), statically typed compiled languages of the type D represents are vanishingly rare & all good entries begin by resembling esolangs. Language design is one of those circumstances where letting your freak flag fly is a recipe for minor success & being too normal (as D is prone to be) is a mistake.
Let me reiterate: D does everything C++ does better than C++ does it, and that isn’t nearly enough for D to accumulate more than a small rabid fanbase.
Fail to compete with C++ and your users will fit on a bus; find even a small niche (like Prolog, MUMPS, Unicon, or Color Forth) and your user base will require its own conference.
By Rococo Modem Basilisk on May 22, 2017.
If you can solve your problems without actually changing the syntax, then you don’t need to build a language.
All languages are domain-specific, so I sort of hate the use of the term DSL to refer to what amounts to a big library with specific idioms.
The point of learning a new language is that a different syntax changes the natural way to think about problems; if a DSL makes a big difference, then that’s fine, but it’s not really comparable to language design. Writing a DSL may also reduce pain points incrementally in a way that prevents you from making much greater strides with syntax changes (in the same way that Zizek suggests the existence of charity prevents people from being serious about reform).
If a DSL is really enough, then that’s fine. But, if you’re considering actually writing a language, think carefully about whether or not a vastly different syntax would be better. If a DSL reduces the problem by 20% and a new language reduces it by 80%, a new language is justifed.
I absolutely condone writing interpreted languages in other interpreted languages. Doing so saves a lot of early development effort. A well-designed language can always be reimplemented in a faster way later (while a poorly-designed language is going to be very difficult to make even small optimizations for).
By Rococo Modem Basilisk on May 23, 2017.
ANTLR may be superior in some way; it’s also strictly unnecessary.
I’d argue against noobs (the people ostensibly targeted by Mr. Bright’s original article) thinking that they need to buy a book to learn a necessary technology in order to build a language. There’s too much mystification of language design as is.
My point, in the implementation section, is not that using YACC is a good idea (it isn’t) or that using LEX is necessary, but that (contra Mr. Bright) using standard UNIX dev tools isn’t some kind of huge portability concern.
Generally speaking, I think the best way to implement a parser is to design your syntax so that you don’t actually need something as complicated as LEX (let alone YACC) — avoid supporting infix if possible, and definitely avoid supporting implicit order of operations— and then use the simplest tech available (which may be as easy as a string split, or may involve several regex matches). But, threading logic through LEX is fine too, although it means you’re writing your interpreter in C which is probably a mistake.
Just as a personal quirk, I like to minimize the dependencies in any project I work on, favoring features that are either standard or exist in all dominant implementations. So, between ANTLR and LEX I’ll generally choose LEX, simply because it’s 70s tech that’s already on everybody’s computer. But, if you aren’t writing your initial implementation in a high level language with built-in support for extended regex, you’re going to have a bad time to begin with.
By Rococo Modem Basilisk on May 24, 2017.
The Mythic Function of the Zombie Apocalypse
Politics of Contagion
[1]
Our standard movie monsters deviate from their early folkloric roots in a number of major ways, but the most notable might be the general move from bewitchment to infection: where strigoi, revenants, zombi, and loup-garou are generally the result of targeted curses, post-Universal-era vampires, zombies, and werewolves are created by being bitten.
We might blame this on the general demystification of western culture, where belief in the ability to be cursed by a witch is rare — and where even self-described witches often consider curses prohibitively dangerous to the caster— or on contagion being a more visually interesting mechanism in film, (though I would argue against this, citing the use of curses in J-horror and post-Suspiria Giallo, not to mention high-budget fantasy like Harry Potter). Instead, contagion spread because in an age of mass-media propaganda and heterogeneous populations, it’s a more flexible metaphor for our political anxieties.
This innovation, the idea of monstrous contagion (which probably can be credited to Stoker), has its most interesting manifestation in the zombie apocalypse narrative.
Political ideas are, functionally, heuristics about how best to run the world. Such heuristics can be reasoned about, and we can talk about what kind of world benefits best from certain political positions. Scott Alexander suggests that the set of tendencies we associate with the far Right , (a heavy focus on physical defense and self-sufficiency, careful gate-keeping and control over population, and a distrust of social services), makes the most sense in a dangerous environment, while tendencies we associate with the far-Left, (a heavy focus on equality, including trying to ensure care for the sick and poor), spreads in an environment that’s safe and resource-rich.
In Alexander’s terms, Right-wing values are for surviving in an unsafe environment, (specifically, one with war, disease, and widespread trickery), while left-wing values are for thriving in a generally-safe world, (where things are generally trustworthy and the marginal cost of risky gambles is lower). This form of Apocalypse myth will appeal to people with Right-wing values: they present a rough world where only people with Right-wing values are able to survive. By suggesting that those conditions emerge in the very near future, such as in the first three Mad Max films, they justify a survivalist impulse.
Zombie apocalypse stories are a refinement of this genre, made to appeal to specific, timely ideas. I don’t think that this is necessarily the result of the filmmaker’s ideology; instead, shifts in the political climate have changed what kind of media resonate. Rather than embracing the same kind of 50s family-based patriarchy that you’d see in early nuclear apocalypse movies, (as parodied in Blast from the Past), the new cozy catastrophe features a band of free agents. Repopulating the world is explicitly made out to be a bad idea. We see some of the seeds of this kind of narrative in the more subversive takes on nuclear apocalypse stories that were showing up around the time Night of the Living Dead came out. A Boy and His Dog seems like a particularly good case.
Zombies themselves vary wildly. Sometimes they hunt by smell and other times they hunt by hearing or sight. They exist on a spectrum from totally mindless automatons to merely distracted animals. Sometimes they are dead and sometimes they are merely ill. Sometimes they move slowly and other times they move like lightning. The cross-genre commonalities specifically construct an environment that favors current Right-wing values:
1. By the time anybody notices, there are a lot of them. Any remaining humans are surrounded by enemies and are justified in hoarding weapons and building walls 2. Zombie-ism is infectious or effects all dead. Remaining humans are justified in being suspicious about other remaining humans, and justified in treating them as potential threats. Not only is living in large groups or caring for the sick a drain on scarce resources, but it also dramatically increases the likelihood of contagion within the human community from inside. 3. Only small pockets of humanity remain. There are no social structures still in place to provide resources or care for the ill, so people are justified in behaving ruthlessly. 4. The zombie infection creates clearly visible physical changes. Zombies cannot pass among humans unnoticed for long, and all zombies are threats, so treating people as threats based on appearance is not merely acceptable but actually necessary
There are also common story-lines that appear in zombie apocalypse media. For instance:
1. Story: A lover or family member of a survivor is infected, and the survivor keeps the infected family member around for longer than is safe out of an irrational attachment. Moral: Emotional connections are potentially dangerous; only ruthless and detached behavior is safe. 2. Story: Our small band of survivors finds a village that is doing well enough to start re-building parts of society, but that village is already harboring infected people or has unacceptable practices. Moral: Large groups harbor dangerous free-riders, and in a resource-poor environment social services can only be provided at an unacceptably high cost.
Why does it matter that a genre has such a clean mapping to an ideology? The stories we tell and consume have a feedback loop with our expectations. It’s not that we can’t distinguish between fantasy and reality, but instead that we will, when suspending disbelief, treat internal consistency as evidence of generalizability, even when the premise presents an extreme or pathological situation.
A pathological premise — one that, when looked at carefully, has unintuitive side effects — can be extremely useful for widening the imagination, so long as the pathological premises you consume are sufficiently varied; however, when a popular genre embeds a pathological premise into its definition, the side effects, (whose value lies in being hard to predict from experience with the real world and with other stories), become common sense. The zombie apocalypse genre embeds a pathological premise that is a superstimulus for Right-wing values: it presents a wholly unrealistic premise that, if taken seriously, justifies what half the population of the western world already believes.
It’s evident that many people, (including groups that should know better), have assimilated ideas from the zombie apocalypse narrative. This is particularly clear when looking at the kind of organizations that have used the structure of these stories, (rather than recurring images from them), to advertise themselves. The CDC released zombie preparedness information, as a ‘fun’ way of teaching people about CDC informational pamphlets for things that actually exist; of course, the biology of a zombie virus is improbable in ways that specifically prevent organizations like the CDC from being useful in those stories, (such as spreading too quickly and having too high of an infection rate). This CDC project was in collaboration with FEMA, who used it as an excuse to pass on general disaster preparedness information — but the zombie apocalypse story differs greatly from what happens in actual disasters by discouraging people from banding together. While the zombie apocalypse tie-in may have increased the audience for this information, the popularity of the zombie apocalypse narrative, (and before it, the nuclear holocaust narrative), seriously damages our intuition about what real disasters are like. It’s too easy, likewise, to generalize the lessons of a zombie apocalypse story to any kind of civilizational collapse — it’s too easy to take the lesson that the strange behaviors of zombie apocalypse survivors are part of ‘human nature’ rather than the result of very specific, nearly impossible pressures.
Fiction is a very powerful tool. By manipulating the expectations of its audience it changes their behaviors. So, we must be careful of the kinds of stories we tell. Luckily, even in the genre of zombie films, we have some mutations that break the set of assumptions discussed above.
Night of the Comet is a zombie film with some characters who turn slowly but without infection. Instead, all the zombies are created by a single event (dust from the tail of Halley’s Comet causing desiccation), and those characters who turn slowly had less exposure to the dust. In Night of the Comet, characters who are turning can be identified; the threat is quickly over, and banding together with other survivors is not only necessary but actually desirable — in a rare properly-happy ending in zombie apocalypse films, our protagonists begin to rebuild society by getting hitched and adopting foster children.
Dead Snow is a zombie film without zombies. Instead, it features traditional revenants — dead Nazis are given a magical pseudo-life, along with extreme strength, by the power of their greed. The revenant is an interesting candidate for the role of anti-zombie, because a revenant is necessarily rich (there’s an untapped mine of stories involving the Walt Disney Company as a revenant — Walt Disney’s corpse protecting his stash of IP, growing bigger and stronger over time). Dead Snow doesn’t do much ideologically with this premise, but again in the absence of infection, banding together is encouraged.
We don’t live in a world where the zombie apocalypse is possible or where its assumptions are valid. We live in a world where some people are trustworthy and others aren’t, and where working together is often but not always worth the risks. Ours is a messy universe and we deserve stories that prepare us for that complexity. Be careful of stories that tell you to discard your most important tools.
By Rococo Modem Basilisk on May 30, 2017.
Tempting Fate: an asymmetric card game for psychics
Tempting Fate: an asymmetric card game for psychics
Tempting Fate is a game for two players.
Game layout
[1]
Terms
The querent is the player who takes an offensive position.
Fate is the defensive player.
A path is a line of cards set in front of Fate. The goal of the game is for the querent to defeat Fate by attacking and extinguishing paths.
A token is an item, such as a bead or coin, used to indicate that a card has been attack. One token per attack point is used. The bank is a pile of unused tokens.
A zener card is a card containing one of five symbols: a circle, a cross, three wavy lines, a square, and a five-pointed star. Decks of zener cards were historically used to test for ESP.
A tarot deck is similar to a typical playing card deck, but with an extra suit (the trumps or major arcana). Tarot decks are well-known divination tools. The four non-trump suits in a tarot deck are called the minor arcana, and they are coins (or pantacles), cups, swords, and wands.
Rules
One player, the “querent”, takes an offensive position and plays with a standard 78-card tarot deck. The other player, “fate”, takes a defensive position and plays with 100 zener cards (20 of each type, or five sets).
Normally, one is expected after a game to switch roles and play again.
Before each game, both players shuffle their respective decks. Each player has a five card hand.
Players alternate turns. Fate moves first.
Each of Fate’s turns consists of three actions. Acceptable actions are:
1. Add a card from the hand to one of the five possible ‘paths’. A card cannot be added to a path with tokens on it. The cards are placed face down. 2. Add a card from the top of the deck to one of the paths, without looking at it. (The same rules apply as in action #1.) 3. Remove a token from a card.
Each of the querent’s turns consist of one action: ‘attack’ the furthest-out card in any path, using a card from his hand or the first card from the top of the deck (without looking at it).
At the end of each turn, the players bring their hands up to five cards by taking cards from the top of their decks.
Points
The points for zener cards are as follows:
Circle — 1 Cross — 2 Wavy lines-3 Square-4 Star-5
The points for tarot cards are:
Cards one/ace to five correspond to their face value Cards six to ten correspond to their face value minus five Page — 1 point Knight-2 points Queen-3 points King-4 points Fool-0 points Magician-1 point Papess/High Priestess-2 points Empress-3 points Emperor-4 points Pope/Heirophant-5 points Lovers-1 point Chariot-2 points Strength-3 points Hermit-4 points Wheel-5 points Justice-1 point Hanged man-2 points Death-3 points Temperance-4 points Devil-5 points Tower-1 point Star-2 points Moon-3 points Sun-4 points Judgement-5 points World-1 point
In other words: the minor arcana are all worth their rank modulo five, while the trumps are worth their rank modulo five, excluding the Fool, which is worth zero.
Attack rules
The only cards that can be attacked are the ones in their paths furthest from Fate (i.e., closest to the querent).
When the querent attacks a fate card, he chooses a tarot card from his hand or from the top of his deck and lays it in front of the card he is attacking, before flipping the fate card face-up. If the tarot card is worth a greater or equal number of points than Fate’s zener card and the zener card is not immune, Fate’s card is defeated and both cards are discarded. Otherwise, tokens equivalent to the value of the querent’s card are placed on Fate’s card and only the querent’s card is discarded.
If a card is attacked that already has tokens on it, the point value of the querent’s card is added to the number of tokens; if this is greater than or equal to the point value of Fate’s card, then both cards are discarded and the tokens returned to the bank.
Immunity
Each zener card has immunity to a particular suit:
The circle is immune to the suit of coins/pantacles The cross is immune to the suit of swords The wavy lines is immune to the suit of cups The square is immune to wands The star is immune to trumps
Furthermore, each zener card is immune to any trump with exactly its point value.
The Fool
The Fool can attack and defeat a Fate card without turning it over. Such a card will be returned to the bottom of Fate’s deck. The Fool is worth zero points against any Fate card that is face-up.
Ending conditions
The game ends when either the querent removes the last card in any path or any path becomes more than five cards long. If the querent removes the last card in a path, the querent wins. If a path becomes more than five cards long, Fate wins.
The game also ends when all cards are extinguished, in which case Fate wins, or when one player extinguishes all of his or her cards, in which case the other player wins.
If a player refuses to take all of the actions associated with his or her turn, that player throws the game.
For shorter games, players can agree to end the game when the number of discarded cards exceeds some number. In this case, the winner is the one whose discarded cards have the greatest combined point value.
Post-game divination
You can treat a game of Tempting Fate as a way to jointly cut the deck, and then use your favorite tarot layout.
Alternately, you can perform a reading with only the discarded cards.
You can also shuffle your hand and produce a 3-card spread.
By Rococo Modem Basilisk on May 31, 2017.
Lol Koibito Cafe.
By Rococo Modem Basilisk on June 7, 2017.
Luckily, businesses (and business concerns) are basically completely irrelevant.
If your startup is decentralized then your startup is making a poor business decision, but calling it a startup is a good indicator of that too: “startup” is a term for “small business” used exclusively by people who think VC funding is a good idea.
If your open source project uses decentralization, on the other hand, then it’s the product of sensible design. After all, an open source project cannot necessarily rely upon the persistence of any particular hardware, nor can it rely upon having a team actively update it to adapt to changes in infrastructure: a flexible peer to peer system is the only way to make sure such a project (if it depends on networking with other nodes) survives the inevitable expiration of whatever companies provide it with early support.
Now, yes, blockchain tech is overused. It’s overused because capitalism rewards con artists who jump on misunderstood buzzwords. (When money isn’t involved, there’s no net benefit to maximizing the number of users running your code, so marketing becomes a non-issue and these incentives toward trendy but ill-fitting tools go away.)
IPFS is a good example of a useful technology that has incentives at odds with capitalism. IPFS provides content addressing, which is great: it means permanent static addresses (something the web fails to provide, leading to the necessity of hacks like the wayback machine, and leading to anti-patterns like domain sitting & malicious redirects of formerly-popular sites). IPFS also does some caching and routing, which is also great: it means that even if your original host goes away, popular content remains accessible from the same address indefinitely. But IPFS runs counter to capitalism, since the bait-and-switch and the ability to centrally control distribution are necessary parts of extracting money from information flows. IPFS presents something similar to ‘free energy’: a system for hard-to-meter and hard-to-interrupt data sharing that’s about as straightforward as the web; if I worked for Facebook I’d probably be opposed to it, and if I worked for the MPAA it would terrify me. As someone who uses computers, though, it presents a set of very useful tools for solving everyday problems.
By Rococo Modem Basilisk on June 14, 2017.
The community around the newsletter is still pretty active on slack, right?
The community around the newsletter is still pretty active on slack, right? Why not queue up a bunch of guest posts from that group, and accept submissions for guest posts from subscribers?
(This might mean keeping the patreon paused — after all, it wouldn’t be you writing each email.)
I would certainly be willing to write a few if you did this. And, you could probably do it that way indefinitely. If you got the impulse to write your own, you could just inject it into the queue like anything else.
The work would come down to making sure submissions are on topic, which is easier than actually writing them.
By Rococo Modem Basilisk on June 15, 2017.
Contributor represents the easiest way out of our adtech dystopia, and it’s a shame that it was discontinued
I paid for Contributor when it existed, because it represented a small-scale change in incentives. (A big flaw is that Contributor would never, even at the highest rates, replace all ads. If it replaced all ads, it would mean that most of the reasons for running ad blockers would go away.) Contributor doesn’t fix the incentive to split long articles into many small pages, but it fixes the incentive toward disruptive and annoying ads, while lowering the barrier for transitioning from ad-driven to micropayment-driven monetization. After all, the mechanisms and rates don’t change (unlike something like Youtube Red, where youtubers get paid based on a totally different algorithm for Red subscribers than for ad impressions).
Contributor was discontinued around the time Youtube Red had its first big marketing push — which is funny, since Contributor never affected youtube ads.
My ideal model for something like Contributor would be a system by which I pay (from some store of escrowed credit) exactly the fees that the system would normally charge advertisers for the ads that I would be seeing. I would be charged when my pool of credit went below some minimum balance. Revenue from my adsense account would optionally go into that same pool. Google seems to have the capacity to implement it this way, although maybe there’s some complexity in the ad-bidding system or in contracts that I’m missing.
By Rococo Modem Basilisk on June 20, 2017.
The perils of identity
The perils of identity
I try to remind myself (despite using words like “I” and “myself”) that, in a meaningful sense, “I” don’t exist. There’s no essential eternal personality that drives my behavior; no soul sitting in a driver’s seat making executive decisions. Instead, my behaviors are just that: behaviors, resulting presumably from an interference pattern between many competing semi-autonomous processes in my nervous system. The sense I have of myself is a model I have created by observing my own behaviors and predicting my future behaviors (something we know from experiment is very unreliable), and I don’t have any special access to my real motivations that an outsider doesn’t — instead, I merely have a greater capacity for self-delusion. To myself and to others, I am a loose collection of habits and biases, and the mechanisms that produce those habits are of only academic interest.
I remind myself of this because I’ve seen the kinds of mistakes that tend to happen when I forget — and the kinds of mistakes that happen when others forget. It’s easy to apply selective memory based on failed predictions: “I would never have done that, so something else must have caused it; I’m a good person so there must have been extenuating circumstances.” People’s models of their own behavior are subject to cache poisoning, and comparing to vague normative models (“I am a good person”) produces perverse incentives toward systematic self-delusion. However, you are what you pretend to be — or, more accurately, your behavior determines which classification you and others would be best off applying in order to predict your future behavior — and so if you have a goal model (such as being a good person) you’re best off focusing on your failures rather than your successes. Successfully achieving such a goal is hard, miserable work — falsely believing yourself to be wonderful is easy and fun, since it’s all reward, while holding yourself to a high standard requires mostly punishment — but at the same time, the bar is often very low because your peers give into the lure of intellectual dishonesty.
When we get into groups the problem gets worse. Tribalism is processed on pre-linguistic parts of the brain, and tribal behaviors will happen automatically and without observation if you don’t make an effort to consciously observe your own behavior. We perform gatekeeping, signalling, and shunning the unbelievers automatically: what we emit seems like language, but it doesn’t contain meaningful statements, or the statements it contains are tangential to the intended signal and their truth values irrelevant.
I recently saw a bumper sticker that said “Frank Sinatra didn’t need a website”. It’s a trivially true statement: Sinatra died prior to the invention of the web, so he achieved his fame without websites. Genghis Khan didn’t need a Big Mac; Jesus didn’t need a hot rod; Joan of Arc didn’t need a ballpoint pen; Hitler didn’t need pokemon cards. Buying a bumper sticker and putting it on your car isn’t something that happens without quite a lot of activity, and manufacturing a bumper sticker requires even more, so putting an irrelevant and trivially true statement on your car is rare. Obviously someone saw that statement and decided it meant something about their personality — presumably, they identify with the first half of the twentieth century and see technologies from the second half as belonging to the outgroup. Yet, the actual content of the statement runs counter to the intended meaning here: Sinatra didn’t have a website, sure, so he relied upon the Mafia, the print media, the TV, radio, and record industries (all of which were new at the time), and Las Vegas (a completely new city built from scratch for the purpose of skirting regulations, with new entertainment industry structures and new social technology like lounge clubs with performer exclusivity contracts); had Sinatra been born later, he would have taken advantage of the web the same way that he took advantage of radio and television. In other words, this bumper sticker says nothing meaningful to anybody aside from indicating a vague and incoherent bias on the part of the owner of the car. Such a bias can be transmitted more effectively through symbols: rather than talking about Frank Sinatra, why not get a decal of Humphrey Bogart? By separating our sense of identity from our sense of group identity, we can reason about our own biases, and not only can we hedge against them but we can seek to communicate them in a more effective way.
Political memes are a major form of group signalling on Facebook. We all have a friend who uses them too often. Signalling your politics isn’t without value: political positions are, theoretically, proxies for your stance on meta-ethical issues. Does life have inherent positive value or does it gain value from its potential? Is it better to be forced to live well or to have the opportunity to live poorly? Does intent matter or do only consequences matter? Is the purpose of justice revenge or rehabilitation? Is it the responsibility of the community to support itself, or is the community simply a temporary structure for holding individuals without mutual responsibility? These questions are theoretically answered by somebody’s political position, and they impact many different kinds of interactions which wouldn’t seem to have a political dimension at first glance. However, without the ability to step back from identification with one’s political position, one is not able to reason about intent: Am I trying to convince other people to adopt my position? Am I trying to change or reinforce group norms? Am I trying to indicate my position to peers in order to allow them to predict my future behavior? Am I trying to indicate my position to strangers so that people who agree with me will talk to me? Without the ability to step back from identification, one cannot even reason about whether or not one’s stated political position is even an accurate reflection of one’s beliefs: I don’t believe that the purpose of justice is revenge & I think communities have a responsibility toward their members, so maybe I’m no longer a republican but actually a communist. I think people should have the ability to choose how to live their lives even if they make poor decisions, so maybe I’m an anarchist. I think a really crappy life is worse than no life at all so maybe I lean toward anti-natalism. This kind of introspection is part of what makes labels like these remain useful.
Imagining yourself to be a consistent and stable personality can be comforting, and it can save effort in the same way that stereotypes save effort: as long as deviations from the stereotype are unimportant & you don’t enforce the stereotype by punishing people who don’t fit it, you can quickly predict behaviors of large groups based on heuristics. Likewise, feeling like you belong in a group and that members of that group are on the same wavelength as you are is comforting. But you aren’t a stable personality, and groups change; if you aren’t sufficiently intellectually honest you will lack the flexibility to change the way you think of yourself in response to changes in your behaviors, and you will be unable to leave a group that gives less and less of value back to you.
By Rococo Modem Basilisk on June 20, 2017.
I don’t think this is an internet thing per-se.
I don’t think this is an internet thing per-se. What you describe seems to happen with subcultures in general, but the internet increases speed and scale so that these tendencies are amplified (and does a lot of the time-binding necessary for creating institutional memories useful for inter-group rivalries automatically).
4chan & other imageboards are an interesting case regarding time binding because they don’t typically have a good archive facility the way their most direct competitors (reddit-alikes and discussion forums) do & they lack reasonable logging defaults the way that irc does (while irc servers don’t keep logs, every irc client has a logging facility and most enable it by default, so any conversation on irc is probably being logged). This means that the institutional memory is being cultivated by insiders — even the parts of it that are used by outsiders — which is a little like a secret society publishing heavily-redacted copies of its meeting minutes. But the upper limit on speed and scale on imageboards is so much higher (and effortposting / reading the whole thread is considered a newbie mistake, further accelerating this) that both the mutation rate of cultural signals and the degree to which those signals get overemphasized is much larger.
We’ve had heavily intellectually-incestuous groups in the past, often driving big major cultural forces. We call them art movements or skunkworks. Now nearly every affinity group is like that because communication is fast and cheap. The material impact of this change in scale will probably have even stranger side effects than the ones you describe here.
By Rococo Modem Basilisk on June 22, 2017.
Labor in Japan
Labor in Japan
(This is a response to a reddit thread, which was locked while I was composing it. Since misinformation about work conditions in Japan is widespread, I’m posting it here. I don’t have any particular special insight here, other than that I pay attention to news stories about labor problems in various japanese industries & to how those issues are covered in japanese media.)
Japan isn’t doing capitalism “correctly”; if anything, ideas about expected behavior have caused it to be more warped. Normal white collar office work in Japan looks a lot like the warped work culture of SV and of the AAA game industry in the US, with unpaid overtime being extremely common. (This is a big driver of Karoshi.)
The Japanese media industry relies pretty heavily on exploitative labor practices, because it’s considered low-status even though it accounts for a lot of exports.
For instance, comics & fiction typically get serialized in cheap magazines in a kind of pulp model, where people are signed and paid per chapter but will lose the ability to continue if they submit chapters late, and the pay rate isn’t really sustainable for what amounts to 80 hour weeks (which is normal for weekly manga written & drawn by a single artist).
Live action film and television is also low-status, although I don’t know about pay or working conditions offhand.
Animators are independent contractors paid by the cut (and required to do corrections for free, often on a very short deadline — in other words, if they receive inadequate initial instruction & draw something that doesn’t match the previous or following scene, they will need to completely redo the animation for free, and this sometimes happens several times).
The pop music industry in japan is extra exploitative because it has a heavy focus on girl groups / pop idol acts consisting of minors, with anybody over the age of 14 or 15 typically getting expelled from a group and replaced; the big players in this industry take copyright litigation a lot more seriously than is done in the west, and merchandising and tie-in stuff is done at a much larger scale, while the artists are considered disposable figureheads.
Sure, Japan has collectivist tendencies which sometimes manifest as thinking of the greater good. However, more often, those tendencies are warped by capitalist incentives into tolerating exploitative practices out of a feeling that this toleration helps one’s immediate peers or family (ex., not doing free overtime would be screwing over your coworkers, even though these death marches only exist because everybody does free overtime).
By Rococo Modem Basilisk on June 22, 2017.
Criticism that’s primarily transmitted by teachers is limited in scope; after all, most people who have read this series have not done it because of its place on some curriculum (and, at least in the US, where various politically powerful groups see it as problematic for very different reasons, children often read it in spite of educational institutions).
The most sensible mechanism for transmitting criticism is probably another work of fiction — one that makes the implicit problems in the Harry Potter universe more explicit. Such fiction is relatively rare (the closest I’m aware of is Lev Grossman’s The Magicians and Elizer Yudkowsky’s Harry Potter and the Methods of Rationality, and of the two, The Magicians comes closer to a real criticism of the pedagogy despite making the focus a significantly more progressive institution and barely touching upon the arbitrary segmentation).
There are lots of low-hanging fruit here. Consider a Harry Potter like story where the Dumbledore character is the antagonist — an initially charismatic and likeable character who is later revealed to be using his institutional power to extend a vendetta against the Voldemort stand-in who was a scapegoat for Dumbledore’s actions taken as part of his rise to power. Or, consider the possibilities of expanding the focus on the Hagrid character — someone who becomes a tool of an institution because the difference between his gentle nature and his imposing physical presence makes him shunned by most of society and therefore easily manipulated by anybody who shows him kindness. Where The Magicians focuses on breaking down the idea of the institution as necessarily mostly-good (by amplifying the kind of dangers already present in Harry Potter’s institution) and breaking down the idea of the chosen one (by making the protagonist an outsider whose drive causes him to keep showing up even though he almost inevitably fails), and HPMOR mostly focuses on breaking down traditionalism, serious criticism of class and institutional power in Harry Potter is rare.
If we had a proliferation of stories that deconstruct the assumptions of Harry Potter, people would naturally come to occasionally read them in conjunction with each other; furthermore, teachers could assign them together (in the same way that teachers often assign students to read both 1984 and Brave New World).
The Harry Potter universe is a master-class in dysfunctional power structures that almost every reader managed to sleep through. Exposing it through narrative means has enormous value.
By Rococo Modem Basilisk on June 27, 2017.
This comment wasn’t intended as snark.
Please, somebody who subscribed, tell me: is it actually worth $5 a month?
By Rococo Modem Basilisk on June 27, 2017.
Is popularity even desirable?
Looking at my Medium front page, I see endless articles promising me tips (as a writer of Medium posts) for how to gain more views. Nevermind the content of these articles, or whether or not the tips are effective: why do these exist? Obviously, for some people, the easiest way to gain more views is to promise to tell people how to gain more views — and as far as I can tell, this seems to be working.
Because I’m not terribly concerned with maximizing views at the cost of writing content that I personally find interesting, I’m not going to tell you how to maximize views. Instead, I’m going to step back and try to figure out under what circumstances (and to what degree) popularity is even valuable.
Most authors on Medium are not writing primarily for paid publications or for subscription users. Most authors are, in fact, readers whose posts are mostly comments. Medium made an excellent design decision when it eliminated the distinction between a top-level post and a comment: the result is that comments look more like top-level posts (i.e., thoughtful, carefully structured, and often longer than a few sentences), and therefore, comment threads aren’t the kind of low-effort purgatory they are on Youtube (or even Reddit). Authors on Medium therefore inhabit this middle realm, writing something that hovers between article, essay, and comment, and doing so with a competence level in that no-man’s-land between talented amateur and budding professional.
Even people who write for money often write for free on Medium. (I have written paid Medium pieces but nearly all of my pieces are not commissioned; professional journalists like Sonya Mann write stuff like the excellent Exolymph here — and while that has a Patreon, I don’t think the primary goal is profit — and professional authors like Nassim Nicholas Taleb and Mitch Horowitz write new stuff for Medium, often not behind a paywall. Hell, Doc Searls writes new stuff and reposts older articles here all the time.) It’s my suspicion (and if any of those tagged deign to respond, you can correct me on this) that any potential profit made directly from Medium posts is incidental, even to many professional authors and journalists, and that even when one of the purposes of one’s online presence is marketing-related, this kind of writing is substantially different in intent from book tours, resume-padding freelance articles for well-known print-publications, and other kinds of more widely visible or explicitly self-promotional work. As an example of this kind of thing outside of Medium, Charlie Stross and Peter Watts both keep personal blogs full of interesting tangents, and while these blogs occasionally make announcements, they seem to primarily serve as a way to record passing interesting thoughts by the author and engage feedback from fans. This serves a certain kind of marketing purpose: reminding people who are already likely to buy these guys’ books that the authors are thoughtful people with interesting ideas even when new publications are nearly a year away, and maintaining engagement, while also giving fans a sense of personal connection and skin in the game, even if that sense is going to be mostly illusory except for extremely dedicated commenters. But, it also provides other functions: even professional authors benefit from daily practice, and the work done for seemingly off-topic posts (such as Peter Watts covering the occasional neuroscience paper, or Charlie Stross making near- or far-future predictions based on current political and economic trends) adds material to the mental compost heap that may fertilize later novels.
To what degree does wide popularity of their individual posts benefit these people? Stross and Watts are mid-list SF authors: while they’d benefit materially from suddenly graduating to the big leagues and selling as many copies as Neal Stephenson and William Gibson, their blogs really aren’t structured around cultivating that kind of growth (and probably couldn’t be); instead, they cater specifically to the kinds of fans who are interested in these authors’ niche side interests. Were their blogs’ popularity to suddenly skyrocket permanently, rather than being a positive, it would actually probably be a net negative for the authors in several ways: where the comment sections had previously been mostly full of dedicated core fans, many of whom recognize each other and have years-long relationships with each other and with the authors, after the explosion of new users the culture would change because the vast majority of users would be unfamiliar with most of the author’s work and have no prior connection to the community; furthermore, server costs and the time and difficulty related to (currently volunteer-based) moderation would skyrocket, and any corresponding book sales that might occur are unlikely to translate into paychecks for months. In other words, there’s a pretty narrow target range for the popularity of these blogs: median traffic for a 3-month period probably shouldn’t grow by more than a third between two consecutive three-month windows, or else the culture of the comment threads is liable to be destroyed.
Likewise, while I am familiar with Sonya from Exolymph, most people are probably only familiar with her work for Inc — and don’t even know about her newsletter or her blog posts. In other words, she’s attached to an infrastructure that, by itself, provides far more promotion for her professional work than the stuff she does here. The same is true of Klint Finley— I’m familiar with him from Technoccult and Infictive (and through mutual friends in the PDX occult community) but most people only know him from his work for Wired; his posts here are mostly about tabletop RPG design, and so almost definitely don’t cater to the Wired audience. This goes double for Doc Searles (since Dr. Dobbs’ Journal is a major institution) and for popular published non-fiction authors like Horowitz and Taleb: none of these people need to self-promote, and the likelihood that their work on Medium is producing a significant direct increase in sales is extremely low. (In the case of those writing books, maintaining an audience that will buy these books consistently is something that blogging can do, but the majority of even consistent buyers probably are unaware of the author’s blog or uninterested in it, particularly when the subject matter differs substantially from the author’s books, as it does with Taleb’s blog.)
Presumably, the non-fiction authors on here are getting something out of Medium much like the SF authors mentioned above get out of their personal blogs: a limited-audience space in which to experiment with new ideas, where the audience is primarily composed of the most dedicated subset of repeat readers. For journalists & authors who work in several genres, this audience is even more rarefied: Sonya & Klint’s audience of tech-industry-gossip readers are unlikely to want to follow their blogs here, whereas people who are interested in several of the subjects they write about will follow them to see that intersection. While Medium has no author-level server costs associated with it, the other negative effects of a massive audience increase are still going to be present.
If people who get paid to write don’t benefit from maximizing their audience on Medium, why should someone who doesn’t get paid to write benefit?
There are a couple possible reasons, but I think all of them are somewhat limited in scope.
One reason is that authors who are looking to become professionals can build up a portfolio of high-quality work, and having some of that work become very popular means that the likelihood that an editor will stumble upon some work and invite the author to submit pitches is somewhat increased. However, an author’s most popular work is often not their best: what an editor would like to pay for is not the same as what a casual reader is willing to click a little heart button on. (For instance, with the exception of a paid piece written for a marketing agency associated with a popular book and TV show, my most successful post on Medium in terms of both read count and like count is three lines of snark about Steve Jobs and Steve Ballmer.) Furthermore, editors already publicly post calls for pitches, and responding to one of these calls for pitches with a good pitch and a link to previous work is far more likely to result in a commission than blind luck and an audience in the thousands.
Another reason is that writing for a large audience means needing to write a little outside your comfort zone, adding information that your core audience would normally know but that a general audience would need explanations for (while carefully avoiding insulting the intelligence of either) and avoiding constructions that are too heavily tied to particular subcultures. However, while this can be a useful skill, successful authors often write for a specific niche. Furthermore, the kinds of tips presented in these tricks-for-maximizing-audience articles often depend upon the assumption that you don’t already have a niche core audience to expand from, or that you are willing to cater to a wider audience at the cost of your niche. In other words, if the goal of getting a larger audience is to get experience in appealing to two groups at once, then these tips articles are completely useless.
The third reason — and the one I find most convincing — is that having a large audience is a personal ego boost. When people click that little heart, the author feels like their value has been confirmed. That said, if your popularity comes entirely from a bullet point list of SEO tricks, isn’t that popularity completely disconnected from your actual value as a writer? If you have sacrificed your ability to write something meaningful in exchange for clicks, haven’t those clicks entirely ceased to indicate the importance of your ideas?
This kind of ‘marketing of marketing’ mindset isn’t limited to writing. Most forms of creative work are subject to it — particularly musical acts and open source projects. Much like novelists, most people who start a band or write an open source project will never get paid for it. This is fine: these are things that don’t come with the expectation of profit — our culture has made it clear that when you do these things, you are sacrificing your time and energy for passion. The ‘marketing of marketing’ mindset, however, suggests that thinking in terms of cost and benefit is sufficient (if supplemented by SEO tips) for slowly transforming a hobby into a business. Some people — if they are both skilled and lucky, or if they are unskilled but very lucky — manage to do this, and working very hard slightly increases the chances of success. However, no matter how hard you work or what your innate skill level is, the odds of success are so low that you should not make decisions with the expectation of profit in these fields unless you have already been consistently profiting for an extended period of time. Furthermore, nothing is more effective at killing your love for a hobby than the stress created by a persistent false expectation of profit. Expecting to turn your hobby into your career is a dangerous path, and should be tread very carefully.
When music is in that intermediate zone of semi-professionalism, strange exploitative business structures take the reigns. (This is not unique to music: ask any waiter in Hollywood what their real job is; take a look at what vanity publishing houses sometimes promise to aspiring novelists. However, the music industry’s particular genre of exploitation is both widespread and wide-reaching, and it tends to directly affect even people who go on to become successful, while both the music industry and the publishing industry have more direct routes for people who have a leg up.) If you are making music because you love to make music*, why spend a bunch of money touring and playing at dive bars (which won’t get you exposure and won’t pay for equipment costs) when you can just record your material and release it free online (which will get you only slightly more exposure and won’t pay for equipment costs but at least doesn’t waste gas money that could instead be used on rent)? If you’re making music for the love of it, why try to get signed to a record company (who will give you an advance that they don’t expect you to recoup and then keep you on the hook for the difference) when you can stick your material on bandcamp or soundcloud (and avoid going into debt with an organization whose practices border on qualifying as a crime syndicate)? If you’re making music for the love of it, isn’t having twenty people who buy every album you put out enough to make you feel justified in trying to balance a day job and a recording schedule?
Open source software has a similar, yet even stranger, situation going on. Most pieces of open source software are small and have extremely few users. They are open source because turning them into products would be pointless and a waste of effort. They are the best kind of software: small, clean, suited to a very specific task, easily understood, and easily maintained. When a talented software engineer writes a very small shell script to solve a niggling problem that seems to bother only him, this is the end result. There are so many of these because this kind of thing happens several times a week for many software engineers, and because a simple and carefully written script can avoid becoming obsolete or requiring maintenance for years or even decades by sticking to standard features. However, if you weigh open source software projects by number of users, the distribution changes substantially: projects with large numbers of users tend to be large, complex, difficult to maintain, and full of technical and social challenges.
When a post on Medium promises tips to get your open source project a larger number of users, it mystifies me. Increasing the number of users for your open source project is an even worse idea than increasing the number of commenters on your blog. A small project with a handful of highly-technically-skilled users and a single maintainer requires approximately zero marginal effort: it can be written, dumped into the public domain, and forgotten; anyone who uses it can also fork it and make any modifications they like; it is easy enough to maintain a fork that there’s no point in migrating changes upstream. But, add a single non-technical user to the user base for such a project and suddenly you get bug reports (for non-bugs or low-priority bugs or things that the other users performed a one-line change to fix) and feature requests (for features that are completely out of scope, only marginally related, mathematically impossible, or would by themselves require several times more code to implement than the entire existing project). Grow to fifty or one hundred users and some of them will inevitably be non-technical — and just to separate the wheat from the chaff you will probably need to bring on a couple trusted people to be maintainers or administrators. Grow to one thousand users and your project now has a “community” that is more complex and drama-filled than many small towns. Yet, one thousand users isn’t remotely enough for somebody to start paying you to work on the project. By the time an open source project has the capacity to start making the maintainers money, it has already been a nightmare timesink for years.
Having something you created become popular can be a great ego boost: it cements a feeling of connection between you and the rest of the world that can often otherwise be quite tenuous. But, it’s not always — or even typically — logistically desirable. Before giving into the instinct to increase the popularity of something you are working on, step back and ask yourself whether or not the benefits are worth what you give up.
*Note: I’m not criticizing side hustles here. A side hustle, however, needs to have a high profit-to-effort ratio to be worthwhile (even if that profit is inconsistent, and even if that effort it tempered by joy). Instead, I’m criticizing the idea that a creative side hustle should aspire to become a consistent source of income and a full time career. Side hustles, particularly creative ones done partly or mostly out of passion, benefit greatly from the kind of scheduling flexibility that can only come from not being the primary source of income upon which one’s life and livelihood depends: if you’re tired you can avoid taking another commission rather than tainting your hobby by hate-working through a burnout period. Your creative side hustle probably benefits a little bit from exposure, but not only is exposure not necessarily worth doing for free what you would normally charge for, but it also is not necessarily worth doing at all what you would normally avoid.
By Rococo Modem Basilisk on July 3, 2017.
Some chess variants
Some chess variants
Twisted chess
3d chess (with 8 boards) but each board is rotated 90 degrees vs the one above it (so traversing z twists you into a different context). Possible moves have forward replaced with down. All boards are fully loaded and turns rotate round robin between boards (each makes a move on first board, then second, etc). Pawns that make it to the far side of the lowest board become queens. Winner is the first to take (not check) 5 kings.
8–1 chess
You have 8 boards. (Each board is adjacent along one of 8 dimensions.) Each move involves a move on each of the 8 parallel games, but a valid move is to swap a piece with another (adjacent) board. Swapping occurs by switching with a piece on the same spot on another board (or, if there is no piece there, swapping with the notional ‘empty space’ piece).
Winner is first to take (not check) 5 kings.
(The kings are taken because checkmate is hard to calculate when any piece can spontaneously move into hyperspace.)
If you want to make it more interesting, you can have black move first on even-numbered boards.
(“8–2” chess would have a second order of 8 boards adjacent to each of the first 8, for 64 boards. Still playable but harder.)
By Rococo Modem Basilisk on July 14, 2017.
Punk & cosplay
Punk & cosplay
This morning I wrote a toot about cosplay. I knew it would be unpopular, but it spawned a lot of discussion based on a number of unintended readings.
What I said was:
If you didn’t make your cosplay costume, you’re cheating.
What I meant was not that only expert tailors should cosplay. Instead, I was trying to criticize the encroachment of commerce into fandom spaces.
I don’t mean to say commerce in fandom spaces is new. It’s not. It presents a unique kind of toxicity to fandom spaces, however — one I’d like to unpack here.
Costly signalling
I consider the function of cosplay to be costly signalling. That is to say, the feeling of belonging and comunity that cosplay can create is caused by the shared perception that the cosplayer has sacrificed something for the group.
Costly signalling can take all kinds of forms. Sometimes, the cost is in money. Other times, the cost is in lost opportunities, social status, damage to the physical body, or the accumulation of blackmail material. The power of hazing comes from costly signalling, as does the power of fashion movements, internal gang solidarity, and music subcultures.
Cosplaying, in the sense that it would be considered embarassing by a normie, constitutes a form of costly signalling by itself: by dressing up in public, you have sacrificed social status with the outgroup in exchange for social status with the ingroup. There’s a second cost associated with cosplay: what is put into the costume itself.
Scaling and marginal cost
Most forms of value sacrificed in costly signalling are more or less stable. Embarassing yourself in public (as in hazing rituals like streaking) applies more or less equally to everybody — even to people who would be immune to first order embarassment (since social stigma is also involved). While access to medicine isn’t universal, costly signalling related to bodily mutilation tends to be relatively low-risk (amputation of the pinky is used as costly signalling in the Yakuza but trepanation is not; tattoos are commonly used but scarification is rare).
The big exception to the general rule that the cost of costly signalling scales is when the cost is in the form of money: the amount of dedication indicated by making a purchase is based on marginal cost, so its meaning is inversely proportional to the size of the bank account of the person producing the signal.
As a result, stable communities don’t tend to center around signals that can easily be gamed with money. Communities whose signals can be bought tend to become divided based on net worth: after all, the poor can’t afford signals that would be meaningful to the rich, and the rich cannot distinguish between signals within the budget of the poor. And, of course, those who benefit from these monetary signals (charities, clothing brands, country clubs, media conglomerates) will try to cater to the rich fans, since the rich fans have more money to blow on signalling.
The cosplay expectation treadmill
How does this relate to cosplay? Well, cosplay is a domain where the norm was originally the creation of costumes by those wearing them. When everybody has approximately the same budget, the quality of a costume is a good indication of dedication to the group — outside of a handful of outliers (people with unusually high or low skill), the association is linear.
The emergence of semi-professional costumers in the cosplay sphere has disturbed this mechanism. It is now possible to pay for a costume that is better-looking than what you could make with an amount of effort equivalent to the marginal value of the money you paid — in other words, by paying for the costume, you’ve gamed the quality/dedication axis. Because high quality costuming skill is rare, purchases of costumes center on a relatively small number of semi-professional costumers. The costumers with the highest quality are able to benefit from volume discounts, so that at the same cost they can use superior materials, making their quality to cost ratio even higher.
Over time, the association looks less and less linear — even assuming everybody starts off with the same budget, those who have bought costumes from the best costumers have much higher quality costumes than you would predict from their dedication level.
While the appropriate interpretation is a sub-linear association, the way people actually interpret this is a linear association shifted downward: if people with hollywood-quality costumes are only casual fans, then people whose costumes actually look home-made mustn’t be fans at all! This interpretation is stupid, but common.
The situation gets worse when you consider that real fandoms are economically diverse. There are Evangelion fans who can’t afford to subscribe to Crunchyroll and other Evangelion fans who can afford to buy the entire figurine line (or fly to Japan to visit the museum). Painting a leotard like a plugsuit could bankrupt the former, but the latter could have a walk-in closet full of professionally made plugsuits. Cosplay says a lot more about the dedication of the former than that of the latter.
The punk connection
If you want to see what happens when commercial interests infiltrate a DIY space, you need look no further than Hot Topic.
Punk fashion has a lot in common with cosplay. Both are forms of costly signalling; both developed in the 70s; both were associated with a subculture centered around media; both were primarily DIY; both were blown up in the media as indicators of a dangerous, embarassing group of undesirables.
The big difference is that punk became fully commercialized much earlier.
Commercialization has side effects other than substitution of effort for money. Once someone is making money off an ingroup signifier, it makes sense to maximize how many people purchase that signifier; the easiest way to do this is to lower the non-monetary cost of that signifier (i.e., to make the signifier socially acceptable to the outgroup). This kills the utility of the signifier, and turns it into a meaningless fashion statement.
Sometimes, rather than maximizing the number of people purchasing the signifier, the company producing it tries to maximize the return on each unit. In other words, the non-monetary cost stays the same and the monetary cost skyrockets.
Typically, both happen — leading to the subculture fracturing into a “casual” portion full of poseurs and a “hardcore” portion accessible only to the very wealthy. This is how we get hipster subcultures: the idolization of overpriced items like vinyl records, the normalization of ten dollar cupcakes.
Fixing the cosplay signalling axis
There’s an easy way to reduce the degree to which the abovementioned factors affect the cosplay subculture: aggressively devalue ready-made cosplay while aggressively affirming the value of low-effort cosplay. Treat a green t-shirt as more impressive than a store-bought Ninja Turtle costume. Affirm the importance of creativity and dedication while treating the exchange of money as taboo.
This is an up-hill battle. Much like the maker community, the cosplay community has already become heavily dependent upon semi-professionals, and has already expanded significantly through the normalization efforts of commercial concerns. We haven’t reached Hot Topic levels of selling-out, but we’re going in that direction, and the inertia is substantial.
If we want to put the breaks on the commercialization (and thus dissolution) of cosplay-as-community, we need to bring out the big guns. This means considering the exchange of money as shameful in cosplay communities, to a degree in excess of the actual target level of shame.
By Rococo Modem Basilisk on July 26, 2017.
Hostility, inclusivity, and target mismatch in open source community management
The reputation that open source communities have for hostility is justified. Open source communities, particularly those surrounding older projects or those whose members are mostly part of the old guard, tend to be elitist (in the sense that they expect a high level of competence from even new contributors and react with hostility to contributions that don’t meet their standards). Whether or not this is a bad thing depends on which of several groups you belong to within the open source sphere.
There are a couple different ways to look at the purpose of an open source community. One is the FSF attitude: that licensing is a social and moral concern, and so opening up software is valuable in of itself. The second is the Cathedral & Bazaar attitude: development methods associated with open source communities are more efficient at creating reliable software than those associated with traditional proprietary shops. The third I will call the BSD attitude (not because I think BSD maintainers necessarily subscribe to it, but because people who take this attitude often use permissive licenses like the BSDL): open source development methods are sensible because, in the absence of a profit motive, there’s no point in making any particular effort to lock down the source code for your personal projects.
If you hang out with contributors or maintainers of open source projects, you will meet all three types of people. The thing is, these goals have very different ramifications.
The CatB attitude and the BSD attitude both favor hostility, in a sense. Within the CatB attitude, the primary goal is to produce good software — so, low-quality submissions aren’t worth considering, and nurturing beginners to the point where their contributions aren’t a net resource drain is a potentially risky business decision. Within the BSD attitude, the point is to minimize effort, so taking even high-quality submissions is often not worthwhile: forks are encouraged over pull requests, since looking at patches is too much work. In the FSF attitude, however, software freedom is a social justice issue to be weighed against other issues, and the codebase is just the foundation upon which the much more important community is built — in other words, the health of the community takes priority over the functionality of the code.
This is not a defense of elitism on grounds other than technical competence. In theory, discrimination along irrelevant lines damages all three of these goals [1]. Instead, this is to say that for two of the three types of open source contributors, being welcoming to contributors who aren’t already producing professional-level code literally runs counter to their goals. The kind of performative hostility toward low-quality submissions that Linus Torvalds (for instance) is known for is useful in this context: so long as people have a reasonably accurate sense of their own competence & the degree to which their code meets the goals of the project and adheres to the community’s best practices[2], these outbursts act to raise the average quality of contributions — and both good patches that are never submitted and the loss of potentially good future contributors is considered justifiable given the huge number of current contributions.
I don’t consider any of these attitudes necessarily wrong. I value high-quality code, inclusive communities, and my own free time. I think a lot of conflict comes from these concerns being juggled within the same project by people who are unaware that they’re dealing with a completely different set of goals. In particular, people who are primarily concerned with inclusiveness tend to assume that all open source projects are (or should be) primarily concerned with inclusiveness (and see people who are primarily concerned with submission quality as callous), and people who are primarily concerned with submission quality tend to assume that all open source projects are (or should be) primarily concerned with submission quality (and see people who are primarily concerned with inclusiveness as naive). Since many open source projects have adopted community guidelines as a way of codifying intended behavior, it makes sense to explicitly specify the priority of the project in those guidelines.
For my own case, I generally take the BSD attitude with regard to projects I administrate or maintain: releasing my code publicly is what I do because, in the absence of a good reason to keep the source code secret, I feel like only an asshole wouldn’t make a useful utility generally available. To that end, I am considering adding a blurb to the README for my projects indicating that, while I am happy for people to use my projects, I do not intend to provide support or take patches, and that those who want to distribute their own changes should create a fork. (This is something notably done by mewo2’s terrain generator project.) For larger or more long-running projects, I will probably simply do the pull-request hack.
For those of you who take the FSF or CatB attitude, I recommend inserting an indication of your intent into your READMEs as well. Something along the lines of “This project’s community is paramount, so contributors are expected to support each other, to the point of mentoring less experienced community members” or “The quality of this codebase takes precedence over its community, so please expect poor-quality submissions to be summarily rejected”, respectively.
[1] Obviously there are toxic and dysfunctional communities that act against their own goals, and when money is being funneled in or there’s corporate influence, these goals often get put on the back burner with profit motive — often in the form of mere continued existence — becomes the primary motivation between actions; in the case of Silicon Valley VC funding, continued existence may actually depend upon racial or sex discrimination, in the form of maximizing investment by making sure most contributors physically resemble Marc Zuckerberg.
There are also systemic biases that make it harder for certain groups to achieve competence, but of the three attitudes we discuss, only one considers trying to correct for these biases to be within the scope of their projects.
[2] The ability to evaluate one’s own competence is associated with competence — in other words, the Dunning-Kreuger effect applies. What people forget about the Dunning-Kreuger effect is that above a certain level of competence, perceived competence is systematically underestimated. So, if the base level of confidence is lower for some group (for instance, women in the open source community), the most competent members of this group may decide their abilities are below the accepted level — in other words, only a band in the middle of the competence spectrum will actually contribute.
This is a problem that huge projects concerned primarily with code quality won’t bother to address: the bottleneck is typically the time spent by maintainers evaluating patches on the border of acceptability, and there are enough excellent patches just by the law of large numbers that systematically missing high quality patches from a particular group isn’t a big deal.
By Rococo Modem Basilisk on August 1, 2017.
The thing about pair programming is that it’s similar to other closely-collaborative creative activities. If you perform pair programming with someone you wouldn’t marry or write a novel with, you’re going to have a bad time, because it requires the same kind of skills and chemistry.
Pair programming involves having somebody point out your stupidest, most embarassing mistakes and then acknowledging and fixing them. And then you switch roles. Most of us only have the kind of relationship that allows us to be so critical without permanent damage with one or two people.
(Now, having someone of a greater skill level behind you and not switching roles is not pair programming: it’s one on one instruction. That’s fine, but it’s different because the power dynamic is different.)
By Rococo Modem Basilisk on August 4, 2017.
Are legendary generals just lucky?
Are legendary generals just lucky?
There’s an infrastructure around legendary generals (and other figures charged with the strategies, tactics, logistics, and general management of war) — not just around teaching their techniques to new officers, but around applying cultural importance to them. We have statues of generals, universities aimed at producing effective military commanding officers, and until pretty recently history has largely been the story of the decisions of military officers rather than the story of bottom-up organization by civilians. But, it occurs to me that for all the effort we put into military colleges, the generals that get the most cultural reverence either didn’t attend them or did poorly. Are we focusing on underdog stories? Are we elevating particular generals in the popular consciousness for reasons unrelated to their effectiveness? Or, is performance in a school designed to produce effective military officers largely unrelated to the effectiveness of military officers?
There’s plenty of reason to suspect the last case, because there’s plenty of reason to suspect that military command falls into the category of something one can be bad at but not good at. Lots of things are like this, but I’d like to focus on one that I think has a lot of similairities: stock picking.
Like war, stock picking is an extremely complex adversarial system. Certainly, in both, one can pick strategies that are much worse than random: you can have all your soldiers shot for cowardice before their first sortie (losing by default) or spend all your money on the single highest-priced stock and sell as soon as its price drops precipitously. Yet, we’ve found that a completely random strategy in stock picking does better than professionals. Is the same true of war?
There are plenty of similarities. Both are a little like ‘quantum suicide’: a good recipe for creating the impression that some people are much better than others at something is to start with a large pool and drop everybody after their first big mistake; in a situation where outcomes are totally random, someone will rarely but reliably win everything or almost everything by chance, and that person will never be able to know whether or not skill played a part. We can’t estimate well whether or not somebody’s successes were fully attributed to chance in such a situation (although we would expect that some were, and we could guess by the distribution of successes). It’s difficult to determine, in that circumstance, the difference between a very difficult problem that only very few people can reliably solve and an impossible problem that nobody can reliably solve (though with a large enough pool we can find the difference between the actual success rate and the expected rate for a fully random one), and when a system is adversarial we have a good excuse for ‘skilled’ people ‘losing their edge’.
There’s a pretty big difference, however. We can test empirically how well a stock picker does: it just takes a big pool of money, and while money is rare, it’s not as rare as cannon fodder. Nobody’s willing to put a random number generator in charge of an army, even if we had pretty good evidence to suggest that a random number generator would reliably perform better than the best human general.
I’m neither a statistician nor a military historian, so I’m not really qualified to crunch the numbers on this. However, it’s a question that I’ve never seen asked, let alone answered. And there’s some import to it: if skill at war isn’t possible, our already-unreliable mechanisms for ranking military leaders are completely pointless; expensive systems for officer training would benefit from being overhauled for less of a focus on tactics and strategy and more of a focus on keeping up morale; war becomes even more of a numbers game with respect to resources; we have no more excuse to have statues of generals in our squares (and we should probably replace them with statues of common soldiers, who, in the absence of value in top-down strategy, at least contributed their lives).
By Rococo Modem Basilisk on August 16, 2017.
Is this the first high profile example of a unicode homoglyph attack in the wild?
Is this the first high profile example of a unicode homoglyph attack in the wild? It’s the first I’m aware of, at least.
By Rococo Modem Basilisk on August 18, 2017.
Fun fact: if we go by formal declarations, Japan is still at war with Russia.
Fun fact: if we go by formal declarations, Japan is still at war with Russia. They haven’t exchanged fire under the banner of the russo-japanese war for a hundred years.
By Rococo Modem Basilisk on August 28, 2017.
The Lone Gunman
The Lone Gunman
There’s a lot of debate about gun control in the United States. However, both sides, by participating in the conversation at all, have a central confusion. The gun control debate isn’t (or at least shouldn’t be) about guns at all.
Gun control advocates and anti-gun-control advocates typically focus on the use of firearms in a very specific situation: when firearms are used in mass violence. The debate centers around mass shootings on one hand, and on the other hand, upon self defense against a large group of targets. Regulation debates focus on automatic and semi-automatic weapons and large clips. This is strangely at odds with reality. After all, even a machine gun is significantly less effective at mowing down large numbers of targets than a bomb — or a car. The firearm is a weapon oddly unsuited to mass murder: even for semi-automatic weapons, the ideal use case is against a single easily identified stationary target from relatively far away. As a weapon, a gun is a great deal like a bow and arrow, although a gun can shoot farther with more accuracy and with greater force, and it’s faster to reload its projectiles. This should be enough to immediately reject both sides’ arguments from the perspective of materialism: any constraints placed on guns should be placed doubly or triply on automobiles, pressure cookers, fertilizer, boats, and weak poisons. The argument isn’t about guns as physical objects.
If the best analogy to the gun as a physical object is the crossbow, then the best analogy to the gun as a symbol is the katana. Physically, the katana is a very limited weapon: it’s a sword, long and heavy enough to take a great deal of effort to wield yet with clearly shorter range than a projectile weapon or even a spear or lance; created via a laborious process made necessary by the poor quality of Japanese iron deposits and the relatively primitive state of Japanese metalworking techniques, even katanas legendary for their high quality steel would be laughed at by medieval european blacksmiths. Yet, because of the association between the katana and the samurai class (enforced by multi-century rules about who was allowed to own these weapons), the katana has incredible symbolic power. In an age where actual warfare in Japan was largely being performed by domestic copies of imported Portugese flintlocks, a sword ban was instated to keep the samurai down. Even today, Japanese cinema is full of sword users, and invents magical techniques by which the sword might act as a ranged weapon. Despite its impracticality as an actual weapon, the katana has an incredible symbolic power to the Japanese (and to some westerners) that keeps it from being ignored. The katana represents a romanticized view of the samurai, and especially the ronin — in other words, it represents the image of a lone warrior who maintains his pride despite disgrace and whose power comes from intense training and self-discipline.
It is another such image that keeps the idea of the firearm relevant in a world where most actual warfare is performed by bombs of varying degrees of autonomy: the image of the lone gunman.
Let us examine the action hero. He is a middle-aged white man — never young, never black, never blonde or a red-head. He is very much like the standard FPS protagonist. He is muscular, poorly shaven, and is usually either ex-police or ex-military (although occasionally he is still affiliated, but not considered a part of the in-group). He works alone. He fights a large and organized force of well-equipped enemies; he does not do so out of some traditional defense of “justice” or “the law” (because he is too cynical to believe in such things) but instead for some intensely personal reason (usually to protect or avenge a family member, who is most often female). Even as the enemy uses bombs, noxious gasses, poisonous injections, throwing knives, or other weapons, our action hero protagonist uses firearms; to the extent that he uses any other weapon, he does so out of necessity, improvising, after he loses his gun or runs out of ammunition, and the weapon he improvises is almost never more destructive than a gun. (This is mirrored in samurai flicks, particularly in parodies — in the first episode of Gintama, the title character destroys a highly advanced alien-made nuclear weapon by hitting it with a wooden sword, having refused to accept a laser gun previously.) The action hero doesn’t plant bombs, although he may allow the enemy to be blown up by their own bombs; when encountering a piece of destructive machinery, even after defeating its operator, the action hero will not choose to use it, except perhaps as a transportation device, and any destructive effects of such a device will be accidental — our action hero won’t steal a tank, and although he might steal an attack helicopter he won’t use the helicopter’s bombs or machine guns.
Our gun control advocates fear the action hero to some degree; after all, the action hero works toward the goal of a safe society only incidentally. Our gun control advocates also fear those actual human beings who have been possessed by the action hero / lone gunman archetype: school shooters, right-wing terrorists, and corrupt cops. To some degree this is justified: while the action hero himself does not and cannot exist, those who have sublimated themselves into this archetype can do quite a lot of damage before their luck runs out. However, in another sense, this is foolish: the terrorist who packs a machine gun instead of a bomb is a bit like the man who tries to take on the army with a sword; he has confused symbolic strength with literal strength, and the limitations of his weapon will prevent him from doing nearly as much damage as he expects. In a sense, those who fear these groups should feel lucky that they suffer under the delusion that their weapon of choice is ideal; were they to replace their media consumption with proper training and think clearly about weapons as tools, they would be far more dangerous.
On the other hand, those who fear gun control identify strongly with the action hero, or at least believe that they could become his manifestation under the right circumstances. People who hoard guns against what they see as an oppressive government are operating on action movie logic: a small group of people with automatic weapons cannot even defend themselves against a national army, although a single con artist could probably decimate a national army with some poison and a great deal of courage.
The lone gunman, though he is often associated with the religious right’s reformulation of Randian Objectivism, in a sense is a stranger bedfellow with Objectivism than the religious right itself is. No Randian hero, the lone gunman is a loser who does not win, but instead causes others to lose. He never profits from his actions, nor does he intend to; he comes into the story already damaged and rejected by a world that he doesn’t fit into, and his goal is to save someone (usually a family member) from a threat that appears after the beginning of the narrative, or to take revenge for that threat. He plays only negative sum games: his goal is to return to the same level of dysfunction he is used to, having caused harm to some third party (usually some variety of “foreign terrorist”). The family he rescues is one he is almost invariably estranged from, just as he invariably has a warped relationship with the career that gave him the training he uses: while usually a former soldier or police officer, if he happens to be a current officer he is a pariah.
I would place the beginning of the lone gunman figure in film with the release of Die Hard. The elements of Die Hard that were originally (in the style of the Last Action Hero) a satire or subversion of action movie tropes eventually became the defining traits that separate the lone gunman from older 80s-style action hero figures, and these traits are important to note: the lone gunman, though skilled, is not ‘fit’; rather than being a well-rounded person who happens to excel at violence, this figure is a loser and outsider who (in a strange warping of the hero’s journey) discovers that he has a talent for violence when he is thrust into a situation where he uses it. He may be an ex-police-officer, but he can fight off hundreds of current police officers who have better training. Much like how, out of context, the stories of popular detective characters appear to be about a person who supernaturally attracts criminal acts to happen around them, the lone gunman appears to attract swarms of unrelated attacks.
I would like to also distinguish the lone gunman figure from another star in our constellation of men of action, the hardboiled detective. While the hardboiled/noir protagonist appears to have much in common with the lone gunman — both are losers thrust into lives of violence to which they are unnaturally acclimated, within the matrix of a society they cannot integrate into — the hardboiled protagonist’s cynicism is always a put-on. A hardboiled protagonist, being a “shop-worn Galahad”, has more in common with the ronin figure or with the hero of westerns: he may pretend to have purely selfish and material reasons for his actions, but he acts according to a strict moral code he would rather not admit he adheres to. The cynicism and nihilism of the lone gunman figure is real, and in an inversion of the hardboiled protagonist, the lone gunman acts as if his behavior is justified by familial loyalty or revenge, when it is clear that revenge is just an excuse for immersing himself in a world of violence. Where all other action hero protagonists are acclimated to violence by necessity and are at least as estranged from violent exchange as they are from the rest of the social world, the lone gunman has a greater connection to violence than with the every-day. All other forms we have discussed are rejects who carry a set of moral guidelines from a world that no longer exists or is closed to them; the lone gunman has never had a home, but finds one in the process of taking revenge, and his moral sense is warped accordingly.
In other words, the lone gunman breaks from the tradition of justified violence, instead engaging in violence that justifies itself: loss for loss’s sake. Hardly sociopathic; this is instead the logic of a perpetually frustrated death wish. That this resonates with society is interesting but not impossible to predict: prescriptive codes of ethics, to the extent that they are narratively interesting, must be problematic (a hardboiled protagonist who will “never hit a woman” is foiled on several fronts, not least by wicked women who take advantage of him); furthermore, prescriptive codes of ethics also don’t age well, particularly now that widespread and fast communications across demographics have brought about a nearly scientific style of inspection of moral and ethical issues in the public sphere. An everyman whose abilities are unknown to him at the start, the lone gunman can become an aspirational figure for those who have no skills but suspect that they may discover that they too can mow down faceless waves of military police if given the opportunity. Finally, the lack of interiority in the lone gunman figure — the reliance on a supernatural luck, the lack of planning or aspirations, and the absence of intellectual rather than material challenges — is easily mistaken for unflappable cool: it is not that the lone gunman is unflappable out of some internal wellspring of strength, but instead because there is nothing inside him to flap.
By Rococo Modem Basilisk on August 31, 2017.
Announcing the winners of the 2017 No Budget Film Contest
Announcing the winners of the 2017 No Budget Film Contest
First place: The Ledger of St. Dermain, by tENTATIVELY, a cONVENIENCE
Second place: A GOMBOSTŰRENGETEGBEN, by SYPORCA WHANDAL
Third place: TEXAS CHAINSAW MASSACRE SWED, by William Sellari
[1] Your trophy, a picture of a coin
tENTATIVELY, a cONVENIENCE, please contact me with your preferred email address for taking paypal transfers.
By Rococo Modem Basilisk on September 1, 2017.
Congratulations on first place.
By Rococo Modem Basilisk on September 2, 2017.
Congratulations on second place
By Rococo Modem Basilisk on September 2, 2017.
Congratulations on third place.
By Rococo Modem Basilisk on September 2, 2017.
The winners have been announced: https://medium.com/@enkiv2/ announcing-the-winners-of-the-2017-no-budget-film-contest-71fd39341063?source= user_profile---------1----------------
By Rococo Modem Basilisk on September 2, 2017.
The winners have been announced: https://medium.com/@enkiv2/ announcing-the-winners-of-the-2017-no-budget-film-contest-71fd39341063?source= user_profile---------1----------------
By Rococo Modem Basilisk on September 2, 2017.
Market Myths: Good, Bad, and Bazaar
Market Myths: Good, Bad, and Bazaar
The stories that hold up western* capitalism
[1] kebba sanneh
The truth value of a myth doesn’t matter, where efficacy is concerned. However, some myths have become so strongly internalized that they become difficult to identify as myths; they are mistaken for “common sense”. For most of us, the ideas underlying western* capitalism are like this. It’s difficult to separate ourselves from these myths and gain the appropriate distance, so I’m going to engage in a little bit of ‘debunking’ — specifically, I’m going to take some time pointing out parts of the capitalist model that don’t match with reality or history, during the course of analyzing its structure and function. This doesn’t take away from the immense power and importance of capitalist mythology, nor does it indicate that I consider all of the ideas associated with capitalism to be strictly false.
On tautology
Academics tend to treat tautologies as a lesser form. Tautologies are shallow, by their nature. It’s quite reasonable for a system optimizing for novel and interesting ideas to reject tautologies. Nevertheless, some really important ideas can be rephrased as tautologies — as Charles Fort points out, natural selection is better summarized as “survival of the survivors” than “survival of the fittest” — and one can make the argument that any really true argument is in some sense circular. There’s no shame in a circular argument that depends only on true premises. In fact, this is one way to look at all of mathematics — which is true because of its internal consistency, and only accidentally coincides with physical reality.
When someone dismisses a seemingly profound statement as “just a tautology” they omit important information. An obvious tautology contains no information. However, a non-obvious tautology is just about the most profound thing imaginable — it takes a complex, incomplete, vague collection of loosely related ideas and replaces it with a much smaller and simpler set of rules, which (if the tautology is reasonably close to correct) is both at least as accurate as the original set of ideas and easier to reason about. A non-obvious true tautology refactors huge sections of our mental models. Obviousness is a function of existing knowledge, so what is an obvious tautology to some people will be non-obvious to others. It should come as no surprise that people seek out ideas that present themselves as non-obvious tautologies.
The drive toward seeking non-obvious tautologies can lead to mistakes. Looking for simple and efficient models of the world is a mechanism for enabling lazy thinking. When lazy thinking is correct it’s strictly superior to difficult thinking, but lazy thinking often comes with lazy meta-cognition. If we jump on ideas that look like non-obvious tautologies too greedily, we fail to see hidden assumptions.
Market efficiency is a very attractive model. Under certain circumstances, we can expect things to actually work that way. If a large number of competing producers really do start off completely even in capability, we really can expect the best product to price ratio to win out. To accept it completely means ignoring hidden assumptions that serious thinkers should at least consider.
One hidden assumption in market efficiency is that competitors start off even in capability. This is almost never the case outside of a classroom demonstration. Companies enter established markets and compete with established competitors, and companies established in one market will enter another. Both of these mechanisms make use of existing resource inequality in order to reduce precisely the kinds of risks that lead to efficient markets, and while perhaps in the long run poor products might lose out, with the extreme spread of resource availability the “long run” can easily last until long after we are all dead. Given no other information, if age is not normally or logarithmically distributed, we can reasonably expect something to last about twice as long as it already has. With corporations, the tails of this distribution are further apart — we can expect a startup to be on its last legs, and we can expect a 50 year old company to last 75 more years, because resource accumulation corrects for risks. A company that has a great deal of early success can coast on that success for a much longer period of poor customer satisfaction.
Another hidden assumption is that communication is free within the set of consumers and between consumers and producers but not within the set of producers.
Free communication within the set of producers is called collusion, and the SEC will hit you with an antitrust suit if you are found to engage in it. People do it all the time, and it is usually worth the risk, since it reduces market efficiency down to almost zero.
Free communication between producers and consumers is also pretty rare: even failing producers typically have too many consumers to manage individually and must work with lossy and biased aggregate information; successful producers have enough resources to be capable of ignoring consumer demand for quite a while, and often encourage ‘customer loyalty’ via branding. (In other words, cultivating a livestock of people who will buy their products regardless of quality — ideally enough to provide sufficient resources that appealing to the rest of the customers is unnecessary). Customer loyalty can have its benefits compounded if wealthy customers are targeted: “luxury brands” are lucrative because something can be sold well above market price regardless of its actual quality or desirability, and sometimes the poor price/desirability ratio is actually the point (as a form of lekking / conspicuous consumption).
Free communication between consumers is becoming more and more rare, since flooding consumer information channels with fake reviews and native advertising is cheap and easy. There used to be stronger social and economic incentives to clearly differentiate advertising from word of mouth, but advertising’s effectiveness has dropped significantly as customers develop defenses against it and economic instability has encouraged lots of people to lower their standards. Eventually, consumer information channels will become just as untrusted as clearly paid advertising is now considered to be, and communication between consumers will be run along the same lines as cold war espionage.
Motivated reasoning
Considering that the hidden assumptions in market efficiency are dependent upon situations even uninformed consumers know from experience are very rare, why would people accept it so easily? The inefficiency of markets has no plausible deniability, but motivated reasoning lowers the bar for plausibility significantly.
During the bulk of the 20th century we could probably argue that anti-communist propaganda played a large role. I don’t think that’s true anymore. Nevertheless, in many circles faith in the invisible hand actually is increasing.
There’s another kind of circular reasoning — one that operates on the currency of guilt and hope. If one accepts market efficiency, it tells the poor that they can rise up through hard work, and it tells the rich that they earned their wealth. This is remarkably similar to the prosperity gospel, which claims that god rewards the righteous with wealth and therefore the poor must have secret sins. It also resembles the mandate of heaven, which claims that all political situations are divinely ordained and therefore disagreeing with the current ruler is sinful.
The similarity between the guilt/hope axis of the market efficiency myth and the prosperity gospel explains the strange marriage between Randian Objectivists and Evangelical Christians found in the religious right. We can reasonably expect many members of this group to be heavily motivated by the desire to believe that the world is fair. It’s not appropriate to characterize this movement as lacking in empathy — empathy is a necessary prerequisite for a guilt so extreme that it makes an elaborate and far-fetched framework for victim-blaming look desirable.
For the poor of this movement, at least on the prosperity gospel side, it might not be so terrible. Motivating a group of people to do the right thing has a good chance of actually improving life generally, even if their promised reward never materialized; second order effects from accidental windfalls are more dangerous, though. (For instance, if you disown your gay son and then win the lottery, you’re liable to get the wrong idea about what “doing the right thing” means).
That said, while the above factors encourage people to trust more strongly in an idea of market efficiency they already accept, bootstrapping the idea of market efficiency is much more difficult.
Natural law, myth vs legend
Market efficiency draws power from an older myth: the idea that money is a natural and universal means of exchange. This is historically and anthropologically dubious. David Graeber, in his book Debt: The First 5,000 Years, makes an argument for the idea that systematic accounting of debts predates the use of actual currency and furthermore only became necessary when cities became large enough to necessitate something resembling modern bureaucracy. Regardless of how accurate that timeline is, we know that gift economies, potlatch, and feasting are more common in tribal nomadic societies than any kind of currency exchange, and that feasting in particular remained extremely important in Europe through the Renaissance.
The legend that backs up the myth of money-as-natural-law takes place in a town. A shoemaker trades shoes for potatoes, but doesn’t want potatoes, so he organizes a neutral currency so that potatoes and apples can be traded for shoes. Graeber points out that this level of specialization couldn’t be ‘natural’ — the town is an appropriate place to set it, since specializing in a particular crop or craft would have been suicidal in the bands of 20–50 people that most humans lived in prior to around 2000 BC.
Our first examples of writing, of course, coincide with the first permanent settlements to have a large enough population to justify heavy specialization. Our first examples of writing are, in fact, spreadsheets recording debt and credit. This, along with the evidence that the unit of currency (the mina of silver) was too substantial for most people to afford even one of (and probably was mostly moved between rooms in the temple complex), is part of Graeber’s argument that independent individuals carrying money for the purpose of direct transactions (i.e., our conception of money) probably only became common later, when imperial armies were expected to feed themselves in foreign lands.
So, on the one hand, it seems to have taken a very long time for the ‘natural’ ‘common sense’ concept of money to take hold among humans. On the other hand, people exposed to the idea of money tend to adapt to it quickly and we have even been able to teach apes to exchange tokens between themselves in exchange for goods and services — in other words, it’s a simple and intuitive system that even animals we mostly don’t consider conscious can grasp.
If something is considered natural law, it’s very easy for people to believe that it is also providence. If something is straightforward and useful in every day life, it’s very easy for people to consider it natural law.
Moral economies
Thoughtful economists tend to recognize the caveats I present here. Some behavioral economists have done great work on illuminating what kinds of things aren’t — or shouldn’t be — subject to the market. This, in turn, illuminates the market myth itself.
It’s possible to think of social relations as economic in nature. Indeed, this is a pretty common model. Transactional psychology presents social interactions as the exchange of a currency of strokes, for instance. Nevertheless, Khaneman presents an experiment that shows social relations aren’t, and shouldn’t, be fungible.
The experiment went like this: a busy day care center has a problem with parents picking up their children late, and instates a fee. Parents in turn respond by picking up their kids late more often, and paying the fee. After the fee is eliminated, the percentage of on-time pickups does not return to the pre-fee state.
Khaneman interprets the results in this way: initially, parents thought of picking their kids up late as incurring a social debt (they were guilty about inconveniencing the day care), the fee reframed it as a service (they can pay some money in exchange for their kids being watched a little longer, guilt-free). But when the fee was eliminated, they felt as though they were getting the service for free.
This result looks a whole lot like the way fines for immoral business practices end up working.
If we consider that, typically, we can make up to people we feel we have wronged, we consider social currency to be somewhat fungible. Nevertheless, exchanging money for social currency is still mostly taboo — paying for sex is widely considered taboo, and even those of us who feel no taboo about sex work would find the idea of someone paying someone else to be their friend a little disturbing. If my best friend helps me move furniture and I give him a twenty dollar bill, he might be insulted. If I left money on the dresser after having sex with my girlfriend, she might be insulted. (Or consider it a joke.)
We could consider the ease with which money is quantified to be the problem. We rarely can put a number on our guilt or joy. On the other hand, we can generally determine if we feel like we’ve “done enough” to make up for something — our measures of social currency have ordinality, if not cardinality.
Instead, the disconnect is that money is, by design, impersonal. I cannot pay back my guilt over Peter by giving him Paul’s gratitude toward me. This is where transactional psychology’s monetary metaphor for strokes falls apart: a relationship is built up via the exchange of strokes, and that relationship has value based on trust. Meanwhile, any currency has, as a key feature, the ability to operate without trust or even with distrust. Money makes living under paranoia possible, and sometimes even pleasant. But exchange of strokes has its own inherent value, and the trust it builds likewise: it cannot be replaced with money because money’s value is based only on what it can buy.
Speculation
The belief in market efficiency, and the emotional and moral dimensions of that belief, have some unfortunate consequences in speculation. Paradoxically, these consequences are opposed by the myth of money as natural law.
With speculation, one can create money without substance. Promises, bets, and hedges can be nested indefinitely to create value held in superposition. A stake in a speculative market is both credit and debt until it is sold. This is natural, since social constructs are eldrich, operating on fairy logic. This is both a pot of gold and a pile of leaves until I leave the land of the sidhe. Of course, there’s every incentive to oversell, so more often than not it’s a pile of leaves: when too many superpositions collapse, so does the market.
Naive materialism, when it intersects with the idea of money as natural law, finds the eldrich nature of money in speculation disturbing. Isn’t money gold? Or coins? How can something be in my hand and then disappear? So, we get arguments for the gold standard along moral lines: “it’s immoral for something that’s real to behave like fairy dust, so we should limit its growth to match mining efficiency”.
The eldrich behavior of money has some paradoxical results. Being aware that money is a social construct tends to decrease its value (clap your hands if you believe!). The question “if someone burns a million quid on TV, does the value of the pound go up or down” is very had to answer. (If you think you know the answer, substitute a million for a trillion, or for twenty.) On the other hand, being aware of its eldrich nature also tends to slightly decouple one from potentially-destructive drives.
Belief in market efficiency leads successful speculators to believe themselves skilled. While skill at speculation might be possible, statisticians who have studied the problem have generally come to the conclusion that the success distribution is adequately explained by market speculation being entirely random. Unwarranted confidence can lead to larger bets, which (if results are random) means half the time the money disappears into thin air. This does not require malfeasance, misrepresentation, or willful ignorance (as with the 2008 housing crisis). Believing that speculation involves skill is sufficient to cause the market to have larger and larger bubbles and crashes.
*“Western” is neither precise nor correct here. These myths seem to be present in western Europe, North America, Japan, and South Korea. Both China and the former Soviet states have different mythology I’m not qualified to analyse. In the absence of a better term than “western capitalism”, I will use it.
By Rococo Modem Basilisk on September 3, 2017.
In my experience:
1. Not really — relatively few articles are locked down, and they’re not (on average) better than the free ones. If you would like to read everything from all the premium paper magazines on here (like The Economist), then maybe it’s worthwhile: they all lock their articles down (even Aeon, which doesn’t have a paywall on its own site), so it’s probably cheaper to pay for Medium than to pay for six or seven paywalls. Also, I got the impression that writing locked-down content was limited to members. 2. I haven’t noticed any differences in feed customization after paying. 3. Absolutely. 4. If you don’t hold down your mouse button, claps behave identically to recommendations. I do this, because I have yet to see anything that deserves more than one recommendation. From what I understand, the distribution of claps among locked-down stories controls the distribution of profits to them, and it seems lie the number of claps on non-locked-down stories doesn’t affect anything except ranking (which I expect makes ranking strictly worse — since I may agree that some article should be recommended or that it shouldn’t, but three hundred claps from one person is going to bias the ranking in a way that’s much harder to ignore) 5. Yup. 6. A lot of the locked-down articles are in series but I’ve never seen it in the free articles and I have no idea why. I don’t really see what benefit it provides either, over just having prev/next links at the end of your article.
By Rococo Modem Basilisk on September 6, 2017.
Yeah. Just as I think honest economists are aware of & careful about these myths (supporting them only with caveats), I think some of the more introspective people working in finance also “get it” & see the risks. I wouldn’t expect that would make them perform better than chance but I would expect them to avoid pitfalls that would normally cause other traders to perform worse than chance — so if they’re actually doing better than chance (rather than, say, tracking the market) that’s interesting.
By Rococo Modem Basilisk on September 6, 2017.
Do most students really start their undergraduate experience completely ignorant of the vast variety of models in critical theory? I certainly didn’t, and neither did my peers (in a STEM curriculum, no less).
(Related: I have no idea how so many otherwise intelligent and well-educated people manage to maintain the belief that poststructuralism posits a fully mutable truth. The poststructuralist attitude — that socially constructed and socially enforced ideas are subject to power relations — is very different, and while there’s a lot of discussion about what ideas are accidents of culture rather than material or metaphysical truths, it’s enormously misleading to suggest that poststructuralists in general consider all truths to be socially constructed. This kind of straw-poststructuralism gets held up a lot by people who must have at least read some of the literature, and I don’t know how.)
By Rococo Modem Basilisk on September 8, 2017.
I have standardized on giving everyone exactly one clap — equivalent to a recommend.
I have standardized on giving everyone exactly one clap — equivalent to a recommend. I want to avoid contributing to runaway applause inflation (the way that ratings inflation makes rankings on ebay completely worthless).
If an article isn’t in some way exceptional (in other words, if it wouldn’t make its way into my recommendations under the old system) it gets no applause. After all, my recommendations show up both on my profile page and automatically tweeted & sent to other social networks — so, out of respect for my followers there, I refuse to spam them with articles that are merely adequate. Adequate gets no claps from me.
The mechanics of how this turns into actual payment is unclear, so the way that my one-clap policy affects payment is unclear. The sensible thing seems to be to normalize by actual clap distribution per user: in other words, every clap a user gives over a month is worth the portion of their subscription fee dedicated to payment divided by the total number of claps they gave out (so my one-clap policy is equivalent to your five-clap policy). It may be that they’re doing a slightly easier but less sensible strategy, wherein the total pool is evenly divided by the total number of claps without normalization by user — which would only produce sensible results if applause behavior was comparable between users.
As a writer — I haven’t been able to determine whether the number on my stats page representing applause is the number of unique recommenders or the actual amount of applause, for new articles. (Maybe everybody is giving me one clap? Maybe they changed the label but forgot to update the variable it was pointing to? Maybe because the article was drafted before the transition to applause but published after it the old version was grandfathered in? Or maybe you need to actually visit the story to get the ‘real’ applause count?) Anyway, calculating value-per-clap isn’t possible without more details about the pricing algorithm. Ev Williams has written elsewhere that predicting the value of an article isn’t possible; this probably still applies if you know the number of claps.
Claps clearly still work on the old recommendation system, hence the lack of negative claps. (The ability to un-recommend is gone unfortunately.)
Ev said that during the last billing period (so, I guess the tail end of August) 75% of authors with members-only stories got paid something and the range was between ~$3 and ~$300. 75% sounds like it might be the percentage of members-only stories with a non-zero number of claps, which would mean that $3 might have ended up being the value of one clap for the tail end of August. If I recall correctly, the period during which the applause system existed wasn’t a full month, so the sample size would be smaller than a full month and the prices higher (since the entire portion of the subscription fee dedicated to paying for stories directly was allocated after claps were implemented, right? Or, were recommends used to allocate funds previously?)
By Rococo Modem Basilisk on September 8, 2017.
I never noticed any of these things. I am so excited for a rewatch now. Thank you!
By Rococo Modem Basilisk on September 8, 2017.
I think that people asking the wrong questions really saved us here.
I think that people asking the wrong questions really saved us here. We benefit greatly when violent people are focused on spectacle rather than body count, because the body count is minimized — how many people are alive today because your average school shooter doesn’t realize that they could do way more damage by stealing a school bus, or renting a moving truck?
Obviously, there is no big-truck-control debate the way that a there’s a gun control debate. I don’t think there really can or should be: guns have pretty limited application, in a way that motor vehicles, pressure cookers, and fertilizer don’t. We also have restrictions on swords and long knives in some places. It’s sort of like drug scheduling: although many things are misfiled, the idea that we would determine how to regulate something based on a combination of desirable and undesirable uses makes sense.
Since guns exist only to fire slugs at high speeds, and the set of desirable applications for high speed slug propulsion is small (essentially, the recreational destruction of inanimate objects, and depending on who you’re talking about, a pretty small subset of animals and people) and the set of possible applications is large (a gun will fire a slug into anything or anyone), it makes sense to have regulations. The exaggerated cultural importance of firearms in the united states makes regulations both more important and more culturally powerful. (By analogy, consider that Japan had guns at the time of the sword ban, and yet we know that as a sword ban because the sword was the iconic weapon. Swords weren’t even the most important weapon during the warring states period!)
(We also, of course, don’t limit regulations to only the most important cases. For instance, there are strict regulations on alcohol used in lab work, even though surely this source could never account for a large percentage of alcohol-related deaths.)
I think we (as people who care more about not being shot than we care about shooting) benefit from fighting for gun control, but I don’t think most of those benefits actually come from lowered rates of firearm possession. (And, I think other techniques might actually be more effective at lowering the rates of firearm deaths than partial bans.)
By Rococo Modem Basilisk on September 9, 2017.
If you decide to collate the results, I’d find that fascinating.
By Rococo Modem Basilisk on September 12, 2017.
Prize money is sent. If you don’t receive it, leave another comment.
By Rococo Modem Basilisk on October 3, 2017.
The way to the hangins that spiders liked breaking over holloway’s was satisfyingly gothic in fashion and succulent. Tom’s extracurricular activities seem increasingly distressing and unjustified. She replayed her sinking in dream logic: reasserted itself among crumpled yellow rectangles of marching images all powdery rattling soot clotted sockets plaster birds bathing in woody marshroots and faceless policemen with too many fingers. Hold fast: intricate rhythms timed flashes turning skimming through violent stones.
By Rococo Modem Basilisk on October 13, 2017.
Those of us working in prose generation have come to the conclusion that neural nets and other statistical methods (like markov chains) are not the best ways to generate interesting fiction. Statistical methods require a lot of training data for each unit of output, and need to be tweaked to be optimized for different scales. Parameters that generate an interesting title would not be capable of generating an interesting first sentence, and in order to be successful, fiction needs to simultaneously optimize for three potentially conflicting goals at many different scales: novelty, coherence, and evocativeness.
While generative fiction has been around for a long time (there were machines writng screenplays for westerns in the 50s), a lot of current activity among hobbyists and non-academics occurs around the NaNoGenMo community, established in 2013. From my experience there, it seems like the most effective methods involve a combination of generative grammars (i.e., rules for producing meaningful arrangements of words, built along the same lines as sentence diagrams), planners (i.e., pieces of code that are provided with an environment, a starting point, and a goal, and then produce a series of actions), and a context & tradition that encourages reader projection (experimental-looking styles encourage readers to see mistakes as hidden intentions — comic books and prose styles that are reminiscent of Joyce or Burroughs end up seeming much more interesting because of this, and can get away with a lot more randomness). Nevertheless, rarely to the results remain interesting for more than ten pages. My personal favorite project used generative grammars and planners to produce a pretty convincing imitation of mediocre Star Trek fanfiction — for about 30 pages.
By Rococo Modem Basilisk on November 3, 2017.
Doesn’t this really seem more like speech-to-phoneme-stream?
Doesn’t this really seem more like speech-to-phoneme-stream? In languages where spelling diverges strongly from pronounciation (like English & French), we should expect phonetic pronounciation to spelling translation to be at least as complex as speech to phonetic pronounciation, if it used the same models (which it basically never does — having a NN guess the spelling for an unfamiliar word would be a nightmare!).
By Rococo Modem Basilisk on November 6, 2017.
It might be reasonably controlled; it might not.
It might be reasonably controlled; it might not. It’s not like PNAS hasn’t published really dubious stuff before. All the major journals have been bitten by peer review failures, sometimes spectacularly; the scientifically literate audience is aware of these.
I recommend avoiding writing misleading descriptions of robust experiments that make them seem dubious. It discourages readers from trusting you or the experiment — which reflects poorly on the experiment.
You’re making a fairly controversial argument (although it’s one I agree with), and it bothers me that it’s being presented in such a sloppy way. Your post will probably do net damage to the popularity of your thesis.
(Yes, somebody could go ahead and read every paper you reference. Why would they, when your post makes them seem flawed — and makes it seem like you don’t recognize those flaws? You’d be better off, at that point, simply posting a list of papers to read and allowing readers to come to their own conclusions!)
By Rococo Modem Basilisk on November 6, 2017.
OK, but what do you gain from misrepresenting the papers you are using to support your argument as weak? Describing controls is typically straightforward; mentioning controls is always straightforward.
It’s fine to expect readers to check your references. That’s very different from crippling your own arguments by presenting your references as not worth checking.
You probably don’t need to worry about people taking such a technical article at face value. You don’t have a reputation on the open web — nobody can tell if your PhD is in a related subject or even legitimate — so most people who look at your post will read the first couple paragraphs, see a description of a dubious-sounding study, and decide that you’re probably a crank and close the tab.
Instead, erring on the side of presenting your sources carefully makes more sense. Making your argument clear and well-structured benefits you as well. Identifying and addressing possible counterarguments is vital. Most people who have complaints will not comment — they will merely dismiss your argument, and dismiss you and the studies you cite by extension! If you do not address their counterarguments quickly enough, they will close the tab, and you will be left with only the people who read uncritically — precisely the audience you don’t want.
By Rococo Modem Basilisk on November 6, 2017.
You mention a study where 12 volunteers were subjected to E.
You mention a study where 12 volunteers were subjected to E. Coli & didn’t get sick; how was the control set up for this experiment? Was there another 12-volunteer control group that developed food poisoning? Was one group given biofeedback training? If you just hook EKGs up to people and watch them not get food poisoning, I’m not sure how that constitutes any kind of scientific study — with such a small group, maybe they all had strong stomachs by chance!
By Rococo Modem Basilisk on November 6, 2017.
I’ve never understood Turkle’s rationale for this idea.
I’ve never understood Turkle’s rationale for this idea. It seems like the empathy we feel for machines is real, the same way that the empathy we feel for animals is real — whether or not that empathy is justified by an inner life is irrelevant because we don’t have access to that inner life — so we are exercising that empathy even when it’s directed at the completely inanimate.
Now, maybe growing up exercising empathy for something and then being thrust into a culture that denies its inner life and demands that you act unempathetically toward it could be damaging, insomuch as children could overgeneralize. Turkle’s solution — avoiding empathy toward things that lack an inner life — seems to be flawed: isn’t it less dangerous to encourage erring on the side of too much empathy rather than not enough? (I understand that empathy can be weaponized — con artists are proof enough of that. But, empathy toward different agents with conflicting needs can even out some of the potential pitfalls, bringing the incentives down to the same risk level as other UI dark-patterns.)
By Rococo Modem Basilisk on November 8, 2017.
By Rococo Modem Basilisk on November 9, 2017.
She Awaited the Turkeys
She Awaited the Turkeys
The load-bearing wall groaned behind her. She would need to move again soon.
Houses used to last a lot longer. This was the third in as many weeks, and she had put off leaving for longer than was wise: the previous tenants had left furniture, and she had almost convinced herself that the smell of rotting carrion was actually the nearby sewage treatment facility.
Taking a claw hammer from the pocket of her mangled overalls, she peeled some of the boards back from the doorjam. Covering her body with a plastic tub, she pushed her way through three or four feet of bloodied feathers and claws. The smell no longer bothered her, but without the tub she would be smothered before she could be crushed, and the tub provided valuable protection from the rain of small winged bodies as she made her way to her next shelter. This area was developed during the last real estate boom, and so almost any house she found would probably be abandoned. She risked a glimpse at the sky, but it was pointless — as usual, the sun was blotted out. For her efforts, she received a white-capped chickadee in the eye.
When she was young, her parents and friends thought it was a blessing, and treated it like a parlor trick. They’d make jokes about Disney princesses and sing that Carpenters song. It wasn’t until she was ten years old that the rate had accelerated to the point of being distressing: her family had to replace the sliding glass doors on the porch with something opaque, and shortly afterward painted the outside of the house a dull rust color to hide the blood. When the roof of that house finally collapsed, they were still in denial, unprepared; only she escaped.
She had been in this development for a year — or maybe two. It was hard to keep track anymore. The birds kept coming in thicker. She wore rubber rain boots that went up above her knees, tucked into her pants; nevertheless, some songbirds, already mostly rotten, fell inside as she shuffled through some of the taller mounds and became squished between her leg and the outside of the boot, beaks and claws and little bones pressing into her flesh. As she pushed through a front door, she felt an unusually large thump on her tub: a hawk, maybe.
She pushed the door closed, reinforcing it with boards and nails with a practiced ease. Then, satisfied, she turned around to survey the rest of the building. But, the back end of the house had already collapsed: she must have already stayed here!
She heard a banging to her left, and it jogged her memory. This was the place with the wild turkeys.
She had thought having turkey would be nice — an easy meal. She had underestimated their strength. That time, she had barely escaped. She had been much stronger then — inside for nearly a month.
Unable to imagine herself summoning the strength to pull out the boards and trudge through another deluge, she slumped, her back against the door. She awaited the turkeys.
By Rococo Modem Basilisk on November 9, 2017.
The only path out, as far as I can see, is to use system 2 thinking to perform analysis on automatic (machine-assisted) more-or-less-objective summaries of our system-1-driven behaviors, then use external mechanisms to nudge system 1 toward better decisions. (We see a little bit of this on the macro level with behavioral economics and on the micro level with ‘quantified self’-driven habit regimens. I think we could do this in a much more granular way if we took heavy advantage of environmental priming & distance reading.)
By Rococo Modem Basilisk on November 10, 2017.
“Thank you for being Prime,” the impossibly-beautiful anime hologram stewardess says, as six-axis manipulators emerge from beneath the arm wrests and unbuckle my seatbelt, gently pushing me into the aisle. “Welcome to Mars; please quickly but calmly make your way to the jetway.” As I stumble into the spaceport, legs numb with gravity adjustment sickness, I trip over a trash-can.
The trash can blushes — pink lights in its undercarriage — and says “excuse me” before zipping away. I make my way toward the bathroom but the entrance is blocked by another machine who has helpfully brought me my suitcase (and is now using it to prevent another rushed-looking traveller from leaving). The man gives me a quick apologetic look, then savagely kicks my suitcase.
The urinal will not open its carapace to allow me to piss; it senses that my karma score is not high enough — a side effect of A short-lived dispute with a shady third-party seller in 1997. I urinate in a nearby (seemingly non-intelligent) trashcan instead. I notice that I am not the first; it will need to be emptied or overflow in about three flights.
By Rococo Modem Basilisk on November 19, 2017.
Are you familiar with the concept of “transpointing windows”?
Conceptually it’s similar to what you’ve done with hyperlinks here (and it’s part of early conceptions of hypertext — being an important part of Xanadu, though not Memex). Basically, you have some n corpora, in windows or columns, and associated sections are highlighted and then connected with a line or a rhombus; when a highlighted section is selected, associated sections in other windows scroll to align to it.
(There’s a little more complexity: highlighted sections can overlap, and you can have connections to multiple non-contiguous sections of the same corpus, both of which present problems that make UI behavior necessarily less obvious.)
By Rococo Modem Basilisk on November 20, 2017.
As he brushed past a cluster of hyper-dandies, their tattoos glowed and squirmed, pantomiming making way for him while the bodies they were attached to remained focused elsewhere. Hypnotics, he assumed, from the discoloration under their eyes the tattoos couldn’t quite hide. He had heard about some new popular one, but before he could recall its name he became distracted by an impromptu drag race between rival food trucks, jerk chicken almost flattening him into the gravel.
The other truck sold synthetic giraffe. It had seen better days — the novelty of exotic synthmeat had worn off for the locals, and there weren’t many tourists around any more, what with the floods and that monkey flu scare a few years back.
He stood himself up, studiously ignoring the short pile of loose brown cloth trying to sell him a string of fried animal hearts on a crinkle of opalescent glass, and cursed the jerk truck’s grandmotherly kindness under his breath. The heart vendor turned and spat.
A salaryman staggered, some ways down the street, talking to a ghost. “Those pufkoffs are transferring me out of The City,” he slurred, too loud. The ghost made a sympathetic gesture and mouthed something inaudible.
It was always The City, never New Tscsk’klpsk. The problem with adopting alien names for cosmopolitan but mostly human places was half the population didn’t have the phonemes, or the hardware to produce them.
When he was young, the punks would call it “the pile”. Back then, you could identify the punks by those crazy swirls of color on the sides of their head. He could never afford the earrings that produced that effect — which was for the best, because it turned out continuously tattooing your eardrums would cause you to go deaf real fast, something somebody should have noticed earlier — and you couldn’t make it look the same any other way because of how the skin grew.
Maybe as an adaptation for the pounding in their heads, there was a special evocative way they used language. Everything was short and packed full of images. So he called it “the pile” still, in his own mind at least.
By Rococo Modem Basilisk on November 20, 2017.
Being intellectually engaged by your job is a rare privilege — one that is really only possible in the 20th and 21st century, and only to white-collar workers. The natural state of employment is boredom, and non-bored workers are a rounding error.
Luckily, people find entertainment and intellectual satisfaction outside of work — if they have the time and energy left over at the end of the day!
Disconnecting the accumulation of life’s necessities from paid labor opens up time and effort for possible intellectual engagement. The alternative is what David Graeber terms “bullshit jobs” — 40 hours a week of boring work that doesn’t need to be done, invented for the sake of backwards compatibility with rules grandfathered in from an era where work was necessary.
By Rococo Modem Basilisk on December 1, 2017.
Announcing the 2018 No-Budget Film Contest
Announcing the 2018 No-Budget Film Contest
We live in paradoxical times: the equipment necessary to produce films is in all of our pockets, yet professional and indie films alike have ballooning budgets and increasingly conservative aims. When I first ran the No-Budget Film Contest in 2010, this was already the case; when I revived it eight years later , the landscape had not improved.
Luckily, the easiest way to convince people that filmmaking is accessible to the masses is also the most effective: demonstration.
Here’s the incentive: the person who best demonstrates that an entertaining, engaging film can be made with *no* budget gets a tiny cash prize, a picture of a no-longer-circulating coin, and a very dubious title: Best No-Budget Film of 2018.
The submission deadline is July 4th, and I will announce the winners by September 1st.
Rules:
1. Your film must be yours. I don’t accept mashups of stock footage or fan edits of existing media. (I don’t mind sweded films, but I hold them to a higher standard than totally original works.) 2. You must not buy or rent anything specifically for this film (with an exception for a camera, if you don’t own one). You also cannot have any paid actors. Props may not be bought for the purpose of the film, although you can use props that you obtained for some other purpose. No budget means no budget — not one red cent. 3. Submissions start April 20th and end July 4th. If you send your submission in outside of this time slot, it might not be considered. You may submit a film created prior to the submission period, if it was made with no budget and has not yet been entered into this contest. (I also don’t care if it was entered into other contests.)
I will announce the winners by September 1st. First prize will be $5 transferred via paypal. The top three submissions (judged any way I like) will be posted on this blog, and will get the titles of Best No-Budget Film of 2018, Second Best No-Budget Film of 2018, and Third Best No-Budget Film of 2018. They will also get a picture of a golden coin trophy.
How to submit: Upload your finished films to youtube or vimeo (or some other streaming video site), and post the link in the comment thread of this post. The films may be of any length, and you can enter as many times as you like. (If you cannot post comments on Medium for some reason but still want to enter, contact me on twitter or mastodon)
About payment: If you win, I will contact you by replying to your comment on this thread, unless you’ve specified some other way. I’ll need to get the email address associated with your paypal account, so that I can send you $5. Alternately, you can provide me with a paypal.me link.
Some guidelines: I would like to be surprised by these films. They won’t be judged on the same merits as professional productions — special effects are still pretty expensive — but using interesting-yet-cheap experimental techniques or having an impressive script or conceit will get my attention. I will prefer original works over spoofs and swedes. I will rank films much higher if they strike me as genuinely good, rather than merely impressive due to budget constraints.
By Rococo Modem Basilisk on December 11, 2017.
My hypermedia history
So, I actually have a long history of (mostly failed or unsatisfying) hypermedia-adjacent or Alan Kay-ish projects, starting way back in the sourceforge days.
First big one was called Sakura Hypermedia Desktop, and was a TCL/TK thing that you’d configure as your root window. You could rewrite parts of the code from a popup command line and persist those changes, and I had widgets. Ultimately, lack of proper multithreading and an inability to overlap widgets killed my plans for this.
(I could redo it today and make it way better (even sticking to TK) just by writing it in python & using shit I know how to do now. I was much less of a competent programmer back then.
Also TCL is almost worse than Perl in terms of how easy it is to write tiny powerful & totally unmaintainable code, and I made full use of that back then, because I had never worked on a big project before.
The site is still up: http://sakura-os.sourceforge.net/
All the code probably still works as well as it ever did. But, I was writing this in high school.
(The Sakura package contains just the desktop thing. Pak is the hypermedia archive tool. Papyrus is a never-functional text editor and Enki is a never-functional Pak navigator based on a 3d space of overlapping Pak-format cards.)
(Sakura & Pak are probably the only things worth actually looking at here.)
At one time I had all sorts of experimental integration with (shitty) speech synthesis and speech recognition tools in this too, though they were never actually usable & I don’t think that code ever got checked in anywhere or showed up in tarballs.
At the time I was obsessed with hypermedia demos like Knowledge Navigator & Starfire, and with descriptions by Alan Kay & Ted Nelson, but hadn’t seen this stuff because I had dialup and was using DOS & a c. 1998 release of Linux.)
The second big thing was a storage format called PAK, which stored bytestreams and metadata, and which had as part of the metadata links between parts of these bytestreams. Text was best-supported, but I had facilities to represent parts of videos (in time and space) as links, & wanted to create something like DVD-style menus. Lack of compatibility with third party tools & heavy use of python’s pickle format limited this but it fundamentally worked, and reliably, unlike the rest of Sakura.
Later, I discovered actual Xanadu documentation and wrote a whole bunch of attempted implementations of Xanadu-inspired stuff.
The first one was Project NAMCUB, which was an attempt to implement xu88/ udanax-green style enfilades in Prolog. It was later ported to Erlang, which also didn’t work.
The second was Project XANA, an OS in D that had a zigzag-inspired UI and an xu88-inspired filesystem. It worked but was horribly slow, & is mostly notable for being the first or second OS written in D.
Later, I took some code from Project XANA to implement iX, an OS in C that had a UI much closer to original ZigZag & features much closer as well. This was usable and basically stable, but I had misunderstood how certain ZigZag facilities were supposed to work and why.
iX is the project that got me noticed by Ted Nelson and from 2011 to 2016 all my hypertext work was for Ted and kept secret :c
But, between finishing iX & meeting Ted I worked on a couple other ZZ implementations.
The best of these was dimscape (which I helped with but only a little). Dimscape supports playing videos & displaying images as well as regular text-based cells. There’s a version of Dimscape that properly implements all ZZ keybindings and features, but it has Xanadu-owned code grafted on so I can’t actually post it, but the old Dimscape is great & worth using.
But, I also built a handheld thing with an arduino that ran iX-but-worse.
Now, this brings us basically to the present. I’ve got an independent implementation of a proper ZigZag backend, a nice gopher client, a not-yet-working hypermedia viewer & editor using ipfs, an abandoned attempt at a hypermedia viewer/editor with a web-based front-end (meant as an alternative to Medium, but also as a trojan horse to introduce people to real transclusion), and I think I might have a curses-based zigzag frontend half-implemented somewhere.
When I say that I’ve “achieved all my dreams and realized they’re crap” in part what I mean is that all the stuff I’m into now in terms of what I like to implement is stuff that I was into at age 12. Hypermedia, distributed computing, novel programming languages and UI systems, planner-based reasoning systems, and prose generation were all also my jam in middle school & high school, even though I didn’t have the skills to articulate that stuff.
(I’ve lost a couple interests since then. I no longer really obsess over robots, strong AI, or neural nets.)
And, I’ve implemented stuff that’s pretty close to what I wanted back then but found it less satisfying than I expected.
I guess that’s life.
So pro tip for all you young people:
Never have goals. If you achieve them you will realize they were crap all along, and if you don’t then you’ll be stressed out about failing.
I wrote a lot of stuff, but it was over a really enormous amount of time. Like, the last time I touched Sakura was apparently 2005, and I worked on that for years.
On the other hand, I’d like to believe that my experience demonstrates that the fruit is so low-hanging in this area that an incompetent kid can implement genuinely interesting things in his free time. Basically, the bar is low because very few people have actually worked on this seriously since the late 80s, and Alan Kay is 3 of them.
The biggest stumbling block I’ve run into is the difficulty of using third party libraries — particularly third-party UI libraries. Compatibility with existing standards is a problem for the same reason. There are a lot of assumptions that proper hypermedia violates.
The second biggest stumbling block is that trying to keep cross-platform & avoid large complicated dependencies tends to mean sticking with old, limited libraries. (For instance, TK is nice to use and provides most of what I want, but it doesn’t support alpha or overlapping widgets, even on canvases.)
I still believe pretty strongly that the wonderful hypermedia future shown in the Starfire & Knowledge Navigator demos — the future that people like Alan Kay and Ted Nelson are still fighting for, and that Jef Raskin and Doug Engelbart didn’t live to see — is possible. There’s no technical reason we can’t have those systems — they were implemented on a small scale over and over. There are some new faces working on this stuff too, but progress feels much slower than it did in the 90s.
I’m starting to think, however, that existing incentive structures discourage people from making the kind of novel and interesting UI descisions that make it possible to build a bridge from novice to expert.
At the very least, those of us interested in real hypertext and real hypermedia (as opposed to the web) will need to work together to create utility libraries that are flexible enough to support all our disparate visions of possible futures but specific enough that using them isn’t a turing tar-pit.
(Adapted from a thread on mastodon: https://niu.moe/@enkiv2/99270035409991776)
By Rococo Modem Basilisk on December 31, 2017.
transclusion is a diff: it shows us how to get from one version to another & which pieces were preserved
transclusion is compression: when a piece of text is repeated but has A origin, we only store it once and we only fetch it once
transclusion is attribution: it concretely shows where we got ideas, and acts in of itself as a means of crediting the original sources
transclusion is a doorway to related documents: documents that transclude from the same source are visible
(Originally posted here: https://niu.moe/@enkiv2/99269102534999527)
By Rococo Modem Basilisk on December 31, 2017.
Trajectories for the future of software
Trajectories for the future of software
Some decisions are sane. By this I mean: some decisions are things you would choose to do with full knowledge of all options and reasonable knowledge of potential ramifications, based on an analysis of how likely each possibility would be to increase the success of the general project it is a part of.
We don’t make sane decisions all the time, and we don’t need to. Sometimes, we make perverse decisions — we ignore sanity or (optimize for as little sanity as possible) in order to achieve some external goal that conflicts with the ostensible goal of the project. Perverse decisions are good and important, too.
When we choose the worst tool for the job with full knowledge of the toolbox, we make the job harder for ourselves. Sometimes, we do this in order to demonstrate our capability to ourselves or others — this is one of the historical meanings of “hack”, and it’s a big part of how the software development community self-organizes. Other times, we do it in order to force ourselves to think about a problem in a new way. Nevertheless, perverse decisions are (by definition) not sane — they take more effort for an equivalent result (or produce a worse result by equivalent effort).
When we make what we believe to be a perverse decision and then discover it to be sane — when we use the wrong tool for the job and then find the job easier than it would have been with the right tool — we have discovered a flaw in our understanding of our tools and their appropriate use. More often, however, we choose what we believe to be the appropriate tool and find the job much more difficult than we expected. Sometimes this is because we don’t know about the right tool; sometimes, the right tool doesn’t exist.
The biggest problem I see in software development today is threefold:
1. We teach people that certain tools are much more generally applicable than they really are 2. We prevent people from using appropriate tools and discourage people from knowing about them 3. We accept developer ignorance as reasonable and treat perverse tool ecosystems as sane
The clearest example is web applications.
HTML is a bad way of representing static documents — strictly worse than TeX for text documents meant to be viewed on paper, strictly worse than postscript for all documents meant to be viewed on paper, strictly worse than plain text or markdown for text documents meant to be read on a screen, and capable of only awkwardly and crudely representing a small handful of non-paper-oriented concepts (jump links and anchors, but not transclusions or links between spans). There is no situation in which the most appropriate representation for a static document is HTML.
HTTP is a bad way of serving static documents — far more complicated than gopher, yet not actually benefitting from any of its complications; it has none of the guarantees provided by IPFS (so, documents can change out from under you without warning, and if a host goes down so does the document). On top of this, no major HTTP server or web browser properly uses features of the standard that would slightly improve the problems baked into the rest of HTTP; instead, a whole parasitic ecosystem has spawned out of abusing improperly-handled parts of the spec. (According to the spec, of course, the same URL should always point to the same page, 404 should only ever be used for URLs that have never pointed to anything, and different codes should be used to represent pages that have moved or been accidentally deleted.) There is no situation in HTTP is the most technically appropriate way to serve a file.
For all the problems of using the web to host static HTML documents, things get far worse when it comes to interactivity. HTML was a mistake, but CGI is a crime, and using an embedded scripting language inside of a static document to force it to self-modify is a tragedy.
It’s the pinnacle of cleverness, of course. Had someone written a web app and presented it as a tongue-in-cheek demonstration of their own skill and perversity, I would applaud it.
After all, HTML is barely capable of representing minimally-formatted text in a sane way, and HTTP is a bloated stateless protocol for fetching files. Making a GUI work in a web browser is like porting DOOM to postscript and running it on your laser printer, one frame per page.
The problem is that the web invited in two generations of programmers who somehow believed that this perverse ecosystem was sane and that wasting their own time and the resources of the computers their stuff was running on was natural. It’s as though the video game industry decided that laser printers were the natural devices on which to write and play first person shooter games, and invested twenty years into making laser printers that printed faster, used thinner paper, and came with rumble pads.
Sometimes, the reason we don’t make sane decisions is structural. We know what the right decision is, but the person paying us is adamant that we make the wrong one. We may underestimate the cost of that perversity at the time — when a person who doesn’t understand how to make technical decisions makes them nevertheless, there is a tendency for demos, toys, and proof of concept projects to be reused as part of a theoretically-production-quality system. (The web is an example of this: it was a quickly-put-together simplified demo intended to teach suits at CERN basic hypertext concepts so that they could understand Tim Berners-Lee’s real hypertext project.)
Software engineers are in a unique position among white-collar professions — what other profession is in such high demand, with such inflated starting salaries, with so little formal education expected, getting paid for so little productive work? So, a software engineer is uniquely well-equipped to refuse non-sane solutions. Outside of places where the cost of living is inflated, a software engineer can (if they live frugally) afford to be fired for their unwillingness to actively make the world and their own lives worse.
There’s a related problem in pedagogy, and in UI.
There’s a sense of strict division between programmers and non-programmers today. There are campaigns to require students to “learn how to code” — mostly with the implication that this would guarantee them a high-paying job or leave them well-prepared to deal with an increasingly computerized ecosystem. Of course, any school that taught students to program in the way that most public schools teach students algebra or The Great Gatsby would not equip students to understand their computers any moreso than it equips them to understand Mochizuki’s proof of the ABC conjecture or Lacanian commentary on Finnegan’s Wake. The gap between novice programmer and minimally-competent programmer is far larger than the gap between someone who has never seen a computer before and a novice programmer, and it contains many more fundamental shifts in thinking.
The division between non-programmers and novice programmers is not a natural part of the learning curve, but is actually enforced by our tooling, which has steadily moved toward segregating users into “technical” and “non-technical”. The personal computer of the 80s, happily used by exceedingly non-technical people, which expected users to be able to type in a line or two of code from a manual in order to do much of anything, has been replaced by the modern PC, where getting the tools necessary to write any code at all involves seeking out somewhat-dubious-looking third-party websites. Beginning with certain management decisions made on the Macintosh project in 1982, the UI philosophy moved from “the simple should be easy and the difficult should be possible” to “anything not featured in the television advertisement should be impossible”. As such, intermediate states between non-programmer and novice (such as “power user”) are nearly extinct.
There is no fundamental technical barrier that keeps us from having “graphical user interfaces” that have the same kind of flexibility and composability as the unix command line. There is merely a social barrier: non-technical users are expected to obtain software produced for profit by corporations, each living in a walled garden, and they are expected to have no curiosity at all about how to change how these pieces of software work, while technical users are expected to run technical-user-oriented operating systems that are visually unpolished and to prefer text-based interfaces.
The Xerox Alto showed us how to make independently-developed graphical applications composable by non-technical or minimally-technical users back in the 70s — while they were running! It worked fast too, but then again they weren’t using Electron.js
Who benefits from separating programmers and non-programmers into distinct ghettos? Well, products that advertise (mostly falsely) that they will teach you how to go from non-programmer to programmer quickly and easily benefit quite a lot from such a division. (If everybody knew that it was possible to learn to code by gradually exploring more of an interface in which the division between programming and using existing programs was fuzzy then there would be a much-diminished demand for programming bootcamps.) Graphical applications can reframe lack of flexibility as “user-friendly”, even if they take more effort to learn to use than an equivalent (but scary-looking) command-line application; likewise, poorly-designed command-line applications can advertise themselves as “powerful” even if they do less than graphical applications designed for the same task & do it slower and worse, and technical users (particularly new converts) will buy into it. (For examples of the former, look at any Apple product since 1985; for examples of the latter, look at wannabe-hacker-oriented linux distros like Arch and Gentoo, which refuse to supply an installer and instead require users to retype the source code of an installer script from the manual.) But, outside of short-term status-seeking, the users do not benefit from this division: it’s not any harder to remain ignorant of programming on a system where programming is possible, more flexible systems have a consistent internal logic that can be understood and incorporated into the intuition of non-technical users, and flexible systems make it easier not just for beginning developers to progress but for intermediate and advanced developers to create useful and interesting programs.
When Peter Thiel said that “every company wants to be a monopoly”, he was correct. And, from the perspective of someone owning a substantial stake in such a company, that’s desirable. From the perspective of literally everyone else, it’s horrible. Well, segregating programmers from non-programmers and discouraging non-programmers from using or understanding flexible and composable systems allows the creation of little fiefdoms on their hard disks. In the popular imagination of both groups, the programmers, due to some imagined inborn talent, get to play freely in the green fields of unix; the non-programmers, who lack the divine calling, are constitutionally incapable of playing in those fields and should be grateful that they have been provided their leaky hovel and stale piece of bread. Of course, like many exploitative divisions that elevate one group over another, it’s totally artificial — engineered and maintained by a third party who benefits by making the lives of both groups worse but making sure one group has things much worse than the other.
These two situations are, of course, related. They feed into each other.
You don’t need to be employed by a corporation to write code. You don’t need to be part of a large group to write open source code. You can write things that make sense to you, and give them away as you see fit. You can write code for yourself, without considering what somebody else wants. Nobody can fire you for making good decisions in your personal projects.
Technical and non-technical users aren’t different groups. They both want to live their lives comfortably. They want to make sane decisions, or to make perverse decisions for the sake of showing off. Technical users don’t have a monopoly on decision-making facilities, and they aren’t even necessarily more knowledgable about what the most appropriate choice is, when it comes to a user’s particular situation.
Non-technical users are actually highly technical. They have elaborate intuitions about the behavior of the software they use. These intuitions may not be accurate, but they aren’t necessarily less accurate than the false beliefs many skilled programmers have about the tools they use.
Programming language design is part of user interface design. Not only that, but user interface design is part of programming language design. A user interface is a language with which a user explains their intent to the computer, and a user interface that makes decisions that would not be welcome in a programming language is broken, because a user interface is a programming language.
We all just want to be able to solve our problems. We all would like tools that make solving our problems easier. When tool-makers cockblock each other in such a way as to make their tools less useful, nobody benefits. When tool-makers tell one group of people that they are too dumb to use their tools and refuses to teach them, nobody benefits. The difference between a programmer and a non-programmer is fundamentally that the non-programmer was told that some tools were off-limits to them and they believed it.
It hasn’t always been like this, and it doesn’t need to be like this anymore. But, fixing the problem requires breaking compatibility with the old ways.
Using systems just because we are told to use them locks us into a spiral of progressively worse decisions. Using systems just because they are familiar locks us into a spiral of ignorance. Going out of our way to learn new techniques and refuse to implement or use bad solutions is not only a good idea — it’s a moral imperative.
If corporations want to create little walled gardens on our computers, then we should starve them out and eliminate them. Reject any system that makes it harder to craft solutions that work for you. Reject any system that artificially limits your ability to craft anything at all. Reject any system that wants to own your solutions or prevent them from working for you.
We’re all going to need to learn new tools and new languages. It’s okay, because the tools will be ours.
(This document is also hosted on Gopher at gopher://fuckup.solutions/0enkiv2/ trajectories.txt)
[1]
By Rococo Modem Basilisk on January 2, 2018.
Mycroft — A Predicate Logic Language — Overview / Post-mortem
Mycroft — A Predicate Logic Language — Overview / Post-mortem
A few years ago, I designed a programming language & wrote a reference implementation for it. This language is the end product of about five years of thinking about planners, distributed computing, and what it means to make logical inferences about real-world problems. It’s primarily inspired by Prolog, but has a couple ideas imported from Church and from DHT systems like Chord.
Being a university undergraduate through most of my time designing this system (and not being in mathematics), what I designed doesn’t take advantage of potentially powerful ideas that are common in automated theorem provers, simply because I lacked the background to integrate those ideas. Nevertheless, I feel like Mycroft presents a couple ideas that (while not totally novel) are underrepresented in other planner-based languages, and I have done my best to foreground those.
My implementation looks fairly polished, and is still usable; however, early design choices with regard to how parsing was done have led to major bugs that severely limit the utility of this implementation for the purposes I intended, and these bugs have proven very difficult to track down and solve. Of course, a language with exactly one implementation is of limited value, and I never intended this to be anything more than a proof of concept — I think the ideas presented are valuable enough to warrant a clear writeup, regardless of problems in an unrelated portion of the implementation. I would be interested in seeing some features I integrated into Mycroft appear in more popular languages like Coq, or in planner libraries for non-planer-based languages, even if no independent implementation of Mycroft is ever written.
Problems with Prolog
Most of the design decisions I made in Mycroft are direct reactions to problems I see in Prolog. Some of these problems are almost fatal in terms of potential real-world use. Others are purely conceptual — places where I felt that Prolog’s design subtly failed to reflect some important aspect of problems it could otherwise be applied to.
Prolog is famously slow. This is not to say that Prolog cannot be optimized. Instead, it’s more accurate to say that a big draw of Prolog is the ability to write programs in the form of an intuitively-understandable set of constraints, without considering execution flow, but that programs so structured will almost inevitably be much slower than similarly naive implementations in imperative languages. Writing efficient Prolog involves tracing execution flow and rearranging the order in which paths are attempted in order to “fail fast” — and while Prolog execution order is fairly easy to understand, highly-optimized Prolog code (replete with cuts and un-idiomatic structures) can be highly unintuitive.
Naive code in Prolog is compact, but its execution is highly repetitive. Most of this repetition would not be necessary, had the planner access to certain out-of-band information. (Of course, since Prolog is standardized, no planner can make unsafe inferences or other ‘intelligent’ optimizations, and the heavy use of planner-execution-order-based optimizations in real Prolog code makes experimenting with optimizations difficult, since messing with the planner is almost guaranteed to break existing programs.)
Prolog freely mixes determinate (i.e., “pure prolog”) predicates and non-determinate predicates (of the type that would be hidden behind a monad in a modern functional language), which means it’s up to the developer to keep these straight and structure things so debugging is straightforward. This failure to distinguish determinate predicates from those that contain externalities also has implications for potential optimizations.
Furthermore, Prolog’s truth values are wholly binary. A Yes/No value makes sense in a toy planner, but for an expert system (theoretically expected to grow to incorporate new heuristics and provide meaningful advice to a practitioner in a real-world situation) a Yes/No value is of limited utility. Consider the use case of diagnostic aid in medicine (something expert systems have been used in for quite some time): a system that provides an un-annotated list of potential diagnoses is almost useless, unless it has an extremely limited scope; a system that presents a list of potential diagnoses with associated likelihoods based on a combination of base likelihoods, symptoms, and symptom scarcity is much more useful, and one that can proceed to re-weigh the list based on severity and on the risks involved in certain tests and treatments is even more so. Getting such numbers out of Prolog is possible but involves threading probablistic truth and confidence values through the system (possibly many at a time) and implementing the calculation of probabilities yourself, while almost totally circumventing Prolog’s existing system for returning truth values!
Prolog is also limited in the extent to which its structure deals with the expectation of incremental expansion. In part because of the use of binary truth values, there’s no way to express that something is uncomputable as distinct from false (and in fact, the normal way to negate a predicate within another predicate definition is semantically equivalent to ‘has no true expansion’ rather than ‘is false’). So, we’re stuck with a ‘closed world’ assumption. Closed world assumptions have a couple benefits — we can expand the entire herbrand universe and try every possible input to try to satisfy a constraint — but the assumption that our program completely models everything about a problem is, at a fundamental level, never true outside of fairly trivial or purely synthetic situations. We can model all of mathematics starting from first principles, but modeling real-world situations (like disease diagnosis or finding subterranian oil) necessarily means making lossy assumptions and operating on potentially-faulty information. Expert systems are typically loose webs of heuristics, all slightly dubious, and a system based on such heuristics that presents its conclusions as a simple Yes or No value is lying. We should have a little more humility about our conclusions, and so should our machines.
Getting more LIPS (Logical Inferences Per Second)
Speed is one of the big reasons Prolog is so rarely used. Even when it was popular, it was common to prototype an expert system in Prolog and port it by hand to C++ when it was debugged. Keeping compatibility with Prolog (as noted above) severely limits the set of possible optimizations.
Because Prolog is optimized based on an understanding of traversal order, we can’t really do what Miranda and Erlang do and evaluate multiple paths simultaneously. Anyhow, those languages perform this operation using guards — and the conditions in those guards are essentially predicates themselves. Most of the time, if a guard makes sense and is cheap enough to compute, it becomes the first term in a pred’s definition.
If a single easily-computed condition isn’t enough to determine the success of the entire operation, then don’t we want to execute all branches? Furthermore, if we’re trying to find out how likely something is to be true and using fuzzy truth values of any type, don’t we want to combine the truth values of all branches?
Today, multitasking is a lot easier than it was in the 70s — our CPUs have multiple cores and we often have access to whole clusters of machines. So, if we have multiple paths to choose from, it makes sense to choose all of them. To this end, Mycroft has a mechanism built into the planner itself which distributes jobs to peers. In the normal case, a predicate with several branches will have each branch sent to a peer — though we implement a cut operation specifically in order to make more traditional guards possible (and this cut operation will force operations to be run in a particular order).
We also have a lot more ram than we used to, and access to good hashing algorithms. Since determinate predicates will always have the same output with the same input, we can memoize those results and return them without recomputing. Since hostnames and pieces of code will hash to essentially similar formats, we can save memory by controlling our redudancy in a manner likely to produce a pretty even distribution of cached results: we store the result not only in the node that computed it but also in the node whose hostname’s hash is most similar to the hash of the piece of code we’ve run. (We can store it in the N machines with the closest hash, too, in case a machine goes down unexpectedly.)
The result is that a complicated predicate with N distinct branches can run on N machines within the time it takes to run its most difficult branch, and that difficult branches become easier as more of their solution space is memoized.
(Now, internally, I decompose big predicates into pairs of trees — one tree containing zero, one, or two predicates ANDed together, and the other ORed together. These synthetic predicates are callable and can be memoized as long as their children are determinate. So, in practice this means that rather than memoizing every top-level input to a big function, we are memoizing every intermediate operation. We store a lot of values really quickly, but for each one we have replaced a subtree of unknown complexity with a constant-time lookup. In other words, over sufficient iterations, all fully-determinate predicates approach O(1), and this happens much faster than it would if we were just memoizing the top of the tree.)
Non-determinate predicates, because they might write something or otherwise modify state, cannot be skipped unless a cut is present. So, best practice is to limit your use of them. Any determinate predicate cannot run a non-determinate predicate.
Opening up the universe
As I mentioned in ‘Problems with Prolog’, a closed universe is implied by two-valued logic, and neither two-value logic nor a closed universe is accurate when it comes to modeling the real world. (Two-valued logic isn’t even technically sufficient to represent the domain of proofs in pure math, as Godel pointed out. But, we aren’t representing this either, except incidentally.)
In order to address this, I use a composite truth value (CTV). This is a concept I have taken from Probablistic Logic Networks, but the CTVs I use operate differently from theirs. (If you want to use CTVs in a real system & you have a proper math background, I recommend reading Probablistic Logic Networks and using their system, rather than modeling your implementation on mine.) I have simplified the math as far as I can while keeping this more useful than pure fuzzy values for real decision-making purposes, but anybody familiar with statistics or probability will immediately see room for improvement.
I represent CTVs (in an ode to bra-ket notation) as an ordered pair of numbers encased in angle brackets. Each number is between 0 and 1 inclusive. The first number represents ‘truth’, and the second ‘confidence’.
There are a couple ‘special’ CTVs:
<1, 1> is YES (100% true, with 100% confidence)
<0, 1> is NO (100% false, with 100% confidence)
<*, 0> is NC (“No Confidence”, or “No Clue” — any value with a zero confidence value is NC)
Identities:
For any X, YES AND X => X NO OR X => X NC OR X => X
YES OR X => YES NO AND X => NO NC AND X => NC (These follow from the definitions and the math, but they are also used to optimize execution, letting us skip steps when they contain only determinate predicates.)
Otherwise:
<A,B> AND <C,D> => <A*C,B*D> <A,B> OR <C,D> => <A*B+C*D-A*C,min(B,D)>
NC essentially means that there isn’t enough information (or not enough confidence) to make a determination. Any undefined variable is NC; any undefined predicate returns NC; a predicate, when it is asked to evaluate something that doesn’t make sense, should return NC rather than NO.
We can lossily ‘flatten’ a CTV into a fuzzy truth value by multiplying the two components.
By supporting NC, we have performed half of the work of integrating the open-world assumption into Mycroft. The other half is our handling of unification — which is to say, not handling it.
Rather than unification (which involves keeping a variable’s value in limbo until all the constraints that affect it are solved, then allowing it to collapse into the appropriate value), we simply make all variables immutable unless unassigned and set the semantics of predicatess such that they return True if their constraint is met on all parameters or if currently-assigned parameters can have a value set that will meet the constraints.
This is different from Prolog since no variable can remain in limbo for very long — if two predicates are in an AND relationship and the first value that the first predicate assigns causes the second predicate to fail, we do not automatically backtrack and try a second possible value — the whole conjunction fails (even if some value that would cause it to succeed exists).
If we want this backtracking behavior, we instead use a function that takes the set of valid inputs for each parameter and iterates over all permutations of all valid inputs, and unifies some parameter to either the first set of values to return a non-NO non-NC value or a list of all sets of values that do so.
Implementation details
I’ve spent most of this article describing features and attributes of Mycroft that I think would be valuable in planner-based languages generally, and I have tried to keep the discussion mostly relevant to such languages, which are probably implemented very differently from mine. However, I have some pride in parts of Mycroft’s design that are not so portable, so I’d like to briefly mention them here.
Mycroft implements a REPL, and the REPL has a built-in help system. The help system is really just an associative array between predicate names and short descriptions. This isn’t a hard thing to implement, but I consider it very important — every REPL should have a built-in help system containing information about all built-in functions and any documentation imported from modules, since being able to look at documentation during interactive development is extremely useful. Not only are all of my built-in functions documented here, but all of the language’s documentation is embedded here too, including information about internals and a language tutorial. The man page for the interpreter is actually generated using the internal help system.
The mechanism for work distribution is round-robin-based, and proceeds to send to the next item in the list if the predicate isn’t answered within a timeout period or the host gets NC as a reply. There’s a similar distribution model for predicate definitions, and for memoized results (which are stored as facts — predicate definitions that, because they have particular parameter values associated with them, have exactly one known return value). Upon getting a shutdown request, a node will use this method to give its internally-stored definitions and memoized results to its peers. When ‘directed mode” is enabled, we perform a sort by hash similarity on this host list before the round-robin mechanism for requests is used, so that no node is overloaded and a result can generally be expected from the first node. The number of nodes to try for a request before giving up is essentially equivalent to the replication factor because of this shared code.
Mycroft internals: networking and message passing
Mycroft polyfills a table called mycnet with the following settings:
mycnet.port=1960 mycnet.directedMode=true mycnet.backlog=512
The following values:
mycnet.peers={} -- each element is an array consisting of a hostname followed by a port mycnet.peerHashed={} -- an associative array of the elements of the peers table keyed by the sha256 hash of the serialized elements mycnet.mailbox={} -- a set of requests we have recieved from our peers mycnet.pptr=1 -- the index into the peers table pointing to the 'current' selected peer mycnet.forwardedLines={} -- a list of facts we've already forwarded, to prevent infinite loops
And, the following methods:
mycnet.getPeers(world) -- get a list of peers mycnet.getCurrentPeer(world)  mycnet.getNextPeer(world) -- increment mycnet.pptr and return getCurrentPeer mycnet.forwardRequest(world, c) -- send a line of code to next peer mycnet.forwardFact(world, c) -- send a line of code to all peers, if it has not already been sent mycnet.checkMailbox(world) -- get a list of requests from peers mycnet.yield(world) -- process a request from the mailbox, if one exists mycnet.restartServer() mycnet.hashPeers(world, force) -- hash the list of peers. If 'force' is set, then rehash all peers; otherwise, hash only if the number of peers has changed
If directedMode is set to false, a request will be sent to the next peer in the peers table, in round-robin fashion.
If directedMode is set to true, then for any given request, the order in which it is sent will be defined by the comparison of some consistent hash (in this case sha256) of the signature of the first predicate with the hash of the entire serialized representation of the peer — we send to the peer with the most similar hash first, and send to all others in order of more or less decreasing similarity. This method of routing is similar to Chord’s DHT routing.
The rationale behind directedMode is that routing will (in the normal case) require many fewer hops and much less replication will be necessary: even though the routing is arbitrary, it represents a consistent ordering and is more resistant to split-brain problems in a non-fully-connected network than round robin routing in a similarly connected network outside of pathological cases. However, this comes at the cost of making sybil attacks on particular known requests easier: an attacker with the ability to set his own node name can guarantee control over that predicate for as long as he is on the network by ensuring he is always in control of the node name with the closest hash to the predicate (so if the attacker can precompute hash collisions — which may not be so difficult since for performance reasons on embedded hardware we aren’t using a cryptographic hash — he can avoid needing to control most of the network).
Because of the request ordering in directedMode, even if the node that already has the solution for a particular determinate query is not (or no longer) the best candidate in terms of hash similarity, it will as a result of responding to the request distribute the response to be memozied by the most-matching node in its peer list first (meaning that over time the routing improves).
Memoized results should eventually be put into a priority queue, and a redistribution protocol of memoized results is planned as follows: results for which the current node is the closest hash match should never be discarded by the gc even if they are very stale, and results at the bottom of the queue should be forwarded to the best-matching node that responds within timeout period before being discarded by the gc. A node that is going offline should send its entire ‘world’ to every node, in the order defined by directedMode. (Currently there is no distinction between memoized results and user-defined facts that have been distributed by a peer; however, since only facts are memoized, and facts are small, this may be sufficient.)
On NodeMCU (i.e., on an ESP8622), a device should determine whether or not an AP exists with some special name and connect to it if it does, but switch into AP mode and create it if it does not. This is not yet impemented.
Depending upon whether or not a node is in daemon mode (along with other circumstances), timeouts are changed in order to ensure reliability. Currently, the default timeout for daemon mode for server listen in 300 seconds and the current default timeout outside of daemon mode is one tenth of a second. Timeouts on connections to other peers during request forwarding are set to one tenth of a second. This is hard-coded but subject to future tuning.
Mycroft has a fairly conventional error reporting system (similar to that found in Python or Java) for runtime errors, and user code can throw and catch arbitrary errors. This is missing from Prolog, where NO often is overloaded to mean error.
Mycroft gets rid of Prolog’s awkward syntax for lists in favor of using parentheses and commas, and uses a couple built-in list manipulation predicates to slice and dice lists into other lists, append lists, or get values. (Of course, lists are also immutable, and these list manipulation functions double as conditionals).
Mycroft has a ‘paranoid’ and ‘non-paranoid’ mode. In non-paranoid mode, nodes can be added to or removed from the cluster, filesystem operations are possible, modules can be imported, the state of the world can be exported, functions written in Lua can be added or removed, and the interactive interpreter can be spawned. The paranoid mode is essentially sandboxed. One can start a cluster and then immediately set paranoid mode on all nodes other than the one running the interactive interpreter, if one wants to be safe. (Or, you can set paranoid mode on the interactive interpreter too, though that makes it difficult to perform some operations.)
I’m proud of Mycroft’s syntax for preserving everything I like about Prolog’s syntax and getting rid of everything I dislike:
Mycroft syntax
Mycroft has a prolog-like syntax, consisting of predicate definitions and queries.
A predicate definition is of the form:
det predicateName(Arg1, Arg2...) :- predicateBody.
where predicateBody consists of a list of references to other predicates and truth values.
A predicate that is marked ‘det’ is determinate — if it ever evaluates to a different value than it has cached, an error is thrown. An indeterminate predicate can be marked ‘nondet’ instead, meaning that its return value can change and so results from it will not be memoized.
A predicate body is of the form:
item1, item2... ; itemA, itemB...
where each item is either a reference to a predicate or a truth value. Sections separated by ‘;’ are ORed together, while items separated by ‘,’ are ANDed together.
A truth value (or composite truth value, or CTV) is of the form
<Truth, Confidence>
where both Truth and Confidence are floating point values between 0 and 1. <X| is syntactic sugar for <X, 1.0>; |X> is syntactic sugar for <1.0, X>; YES is syntactic sugar for <1.0, 1.0>; NO is syntactic sugar for <0.0, 1.0>; and, NC is syntactic sugar for < X, 0.0> regardless of the value of X.
A query is of the form:
?- predicateBody.
The result of a query will be printed to standard output.
Comments begin with a hash mark:
# This is a "comment".
Variables begin with a capital letter, and are immutable:
?- set(X, "Hello"), print(X). # prints "Hello" ?- set(X, "Hello"), set(X, "Goodbye"). # fails ?- set(x, "Hello"). # also fails
Strings are surrounded by double quotes, however, words containing only letters, numbers, and underscores that do not begin with an uppercase letter will be evaluated as strings. Unbound variables will also evaluate as the string value of their names:
?- equal(hello, "hello"). # true ?- equal(X, "X"). # also true
For some simple examples, see the test suite.
By Rococo Modem Basilisk on January 5, 2018.
Orality, Literacy, Hyper-Literacy
Walter Ong calls only spoken word impacted by literate culture ‘secondary orality’ (things like audio recordings, broadcasts, memorized speeches, and habits of thought formed from literacy applied to speech such as speaking in paragraphs). The secondary folk-meaning of ‘secondary orality’ — extemporaneous, informal, and expressionistic text — he calls ‘secondary literacy’.
If you consider a spectrum between orality and literacy with books on one end and homeric poetry on the other, this makes sense.
The secondary folk-meaning implies a totally different way of approaching the subject, and one that I think opens new possibilities. Specifically: in Ong’s view, ‘secondary’ just means incorporating elements of the opposite end to create a hybrid; however, if we consider the orality-literacy spectrum as extending out on the literate end indefinitely, we can call both ‘secondary orality’ because both are oral-literate hybrids. (I suspect the spectrum extends further in both directions than Ong believed. Toki pona is a more oral language than could exist in a pre-literate society, for example.)
When you go more-literate-than-literate you get hypertext, and that is therefore an appropriate name. Hyper-literacy differs from literacy in precisely the same ways as literacy differs from orality. Everything is forever; rather than linear, space is multidimensional and yet the multidimensionality of this space is still clearly defined, as opposed to the fuzzy spacial nature of orality. In rudimentary hypertext systems like the web & gopher, we explode the essay into a tangle of footnotes, but each footnote is capable of being followed. In more advanced systems we do the same with pull quotes and revisions.
We have a priest model of hyper-literacy, though: many can read it but it is written mostly by professionals. Hopefully this will break apart as tools for manipulating the more nuanced features of hypertext become more common. The web, by being ‘good enough’ and most people’s first experience of hypertext, has set us back at least 20 years here.
The only large group that writes and edits hypertext with these features at a scale that might set them up for hyper-literacy is Wikipedia editors so we should look at them for possible examples of how hyper-literacy will affect society as a whole. Wikipedia editors are demographically and psychographically skewed at least as much as medieval scholastic scholars ever were, and we can expect them to represent our fully hyper-literate descendants about as much as Duns Scotus represents the average victorian reader of penny-dreadfuls or the average 1930s pulp author.
(Adapted from a thread: https://niu.moe/@enkiv2/99316232536600118)
By Rococo Modem Basilisk on January 8, 2018.
Quick OmniStage history
Time for another ‘Enki talks about his failed projects’ storytime corner!
Around late 2006, I had this idea of how to ‘solve the IoT privacy problem’ & do ubiquitous computing with cheap consumer hardware.
Basically, the idea was that you’d have a hardware dongle that was your input device and contained your settings (or information about how to retrieve your settings/preferences from elsewhere). This dongle would be totally passive until you enabled it.
The idea is, you can identify a space where you can connect to a ubiquitous computing environment, and then you need to make a conscious decision to trust that space — one that corresponds to a contract with that space. (Basically — within reason, I trust this space to not fuck me over, and allow it in turn to access and modify my settings on my behalf.) This space is literally a building. Like, with a bunch of projectors in it. I wanted to replace cybercafes with collaborative public computing spaces.
This project was stuck in a conceptual phase until early 2007, when 3rd party clones of the wiimote became cheap and widespread, and open source wiimote interface libraries proliferated.
Then I made prototypes using the wiimote as input device.
What I was going for was something like what Alan Kay & Brett Victor are doing with dynamaland: you’d have a big communal space with big wall-sized screens & people could let each other collaborate by sharing write access to what they were working on. So it was going to be geared toward creative workplaces of the type they show in ads. (I didn’t realize those places were mostly imaginary.)
I actually gave a lightning talk at CFP 2008 about this. (I spoke right after moot.)
Killer: multi-pointer X didn’t exist yet, and also the wiimote wasn’t totally passive when not linked. (The wiimote also didn’t have enough space on it to store much, so I was going to have it store an ID & a keypair for accessing encrypted data on Amazon S3. I no longer think this is as good an idea as I did a decade ago.)
In the end, it’s one more project I half-did in high school that I still think is conceptually cool but will never happen.
By Rococo Modem Basilisk on January 9, 2018.
Some tentative guidelines for GUI composability
Introduction
You know how on TV every piece of software has a UI that is centered around doing something extremely specific and visualizing it in an intuitive and clear way? We all make fun of it, saying “have these people ever used a computer?” Sometimes it’s bullshit and sometimes what’s being visualized is something we couldn’t reasonably expect to have accurate information about but other times it’s like — why AREN’T our apps like this?
When I’m trying to solve a problem with a CLI, I write a tiny shell script whose UI is extremely specific to the problem I’m trying to solve, and I do it in a year or less. I do it incrementally. My command line environment really does operate like a hollywood UI.
If we had good graphical composability and the ability to do the kinds of mashups in a graphical environment that are normal in a unix command line, any ‘power user’ (not just developers) could have a hollywood UI — not because some developer had used existing shitty UI toolkits to painstakingly develop something for their incredibly specific case, but because you can whip something up out of components quite quickly that will do what you want in a graphical way.
I think we could at least manage general composable interfaces like those shown in Apple’s Knowledge Navigator film, if not Starfire, if we tried. The catch is that commercial software (and indeed no software that is meant to be run by more than one person) can do this.
If the average GUI app was like the average shell function (i.e., made in years by someone who isn’t an expert, slowly honed over years, and never intended to be able to be used by anyone else), we’d basically have that future. The catch is to make doing that kind of stuff as easy as the unix command line is. We only don’t have it because GUI toolkits are so bad that it takes an expert a long time to write a shitty app — leading to a situation where GUI apps are required to be general-purpose, stable, and unexpressive, in order to make up for the labor that went into them.
A composable graphical system is possible, if we do it from the ground up. The Alto & Squeak environments get close, and so does Tcl/Tk’s ‘wish’ interpreter, but we’re not there yet.
Here are some guidelines for a potential composable graphical UI system.
Guidelines
1. Widgets aren’t magic. You should be able to turn a widget of one type into a widget of another type by modifying some internal values & have it behave in a sensible way. 2. Widgets aren’t magic. If you make a widget that combines two or more existing widgets, it’s of the same type as a more ‘primitive’ widget and works the same way. 3. The window system knows about the environment and the user can view and live-edit the source of anything visible, with changes taking place on the next draw. 4. The live editor is integrated with the environment and written with these same widgets. You can select and edit the code of any widget you can see. 5. Widgets are prototypes but also part of a hierarchy. Edit a particular widget and it can be a template for new widgets. Edit its parent and changes apply to the whole class. 6. Versioning is integrated with the environment and with the live editor. You can see all changes, revert to any, and navigate through the tree (even to previously-undone branches) or branch off part of a version tree as a new widget type. 7. All widgets accept all messages and all events. Some widgets handle and respond to them. The default response is to ignore an unknown message, or a message targetted at a different widget. 8. The normal way to produce an app is to create a widget by forking existing widgets and connecting their messages together, writing message handlers if necessary. Messages and default message handlers should be designed so that writing new message handlers is usually not necessary (or is trivial) for simple applications — useful apps can be created simply by directing messages between existing widgets, the same way useful shell scripts can be created simply by piping between existing commands. 9. An app is composed of widgets, and is itself a widget. Other apps can cannibalize and fork parts of your app. 10. Every object has a visual representation. Every visual representation corresponds to an object.
(Adapted from two threads: https://niu.moe/@enkiv2/99366610716102498 and https: //niu.moe/@enkiv2/99366404418510312)
This post is also available on gopher: gopher://fuckup.solutions/0enkiv2/ composability.txt
By Rococo Modem Basilisk on January 17, 2018.
I don’t know where the number 2501 comes from.
I don’t know where the number 2501 comes from. (There might be a meaning in chinese numerology I’m not getting — that’s not really my forte.)
The explanation that 2501 is an insider trading system is on page 269 of my copy of the manga (Dark Horse’s first 1995 English edition). It might occur on a different page number in other editions. 2501 explains it himself when Kusanagi first dives in.
By Rococo Modem Basilisk on January 27, 2018.
I’d like to defend the idea of putting plop art specifically in places that the pieces aesthetically clash with. After all, when there is no clash, the pieces blend in and are easily ignored. (Just consider the nigh-omnipresence of modern art in the background of high-budget hardcore porn. Or consider the way you never noticed it was there until now, because fancy modernist houses go along pretty naturally with abstract geometric sculptures.)
Part of the value of art is in its capacity as an attack on complacency: a criticism of the arbitrary division and categorization of spaces, and a reminder that the world is fractally strange in ways that are neither easily summarized nor easily comprehended.
Creepy Lucy does not successfully capture a realistic representation of people’s internal idea of Lucille Ball, but it provides another way of looking at Lucille Ball that’s worthwhile, and in that way it’s more effective than a more conventional sculpture. Having shock value (whether that shock comes initially or only later) is necessary, though not sufficient, for a piece of art to be worthwhile as art rather than as advertising or design, and whether or not the deeper meaning unlocked by a piece is intentional (or an accident of fate caused by lack of skill, poor materials, or bureaucratic infighting) is irrelevant.
By Rococo Modem Basilisk on February 1, 2018.
I’m happy that somebody’s finally starting to address some of the web’s big design oversights.
I’m happy that somebody’s finally starting to address some of the web’s big design oversights. (Span-to-span links were a feature of most pre-web hypertext systems, and certainly are and remain an important part of Xanadu.) Nevertheless, I worry about trying to tie it to a single site or platform, instead of trying to push this fix through w3c standardization.
In fact, there *is* a w3c standard for this type of link, and the HTTP protocol theoretically supports fetching specific byte-spans. Unfortunately, neither are widely implemented (by browsers in the former case, and browsers and servers in the latter), even though these standard are twenty years old. The reason is probably that such features depend upon the assumption that URLs are permanent addresses for unchanging content, and (starting with CGI but proceeding along to web apps and domain squatting) the web as a whole has totally abandoned this restriction. So, it makes more sense to use span-to-span links on IPFS or some other content-addressed protocol, wherein this restriction is actually enforced.
Just having span-to-span links doesn’t, itself, add transclusions, transpointing windows, versioning, or any of the other fundamental hypertext features that the web decided not to bother with. And, implementing span to span links in a walled garden isn’t hard — it’s an afternoon project for a beginner programmer. Hopefully, more beginner programmers will write hypertext systems, and we’ll dig ourselves out of this mess.
By Rococo Modem Basilisk on February 1, 2018.
I always maintain that a good, thoughtful, well-constructed review of a very flawed work is more valuable than a less flawed work by itself. The review is a piece of art in of itself, and should be able to stand on its own outside of the context of the work it responds to, while simultaneously adding extra dimension and context to that work. After reading a good review of a bad book, reading that book should become rewarding.
By Rococo Modem Basilisk on February 2, 2018.
Guidelines for future hypertext systems
Since 1992, the web has been the only hypertext system most of us have known (outside of occasional hypertext systems built on top of the web, such as mediawiki). However, the web is merely the most popular of hundreds of earlier systems, most of which were technically superior.
For historical and political reasons (detailed in Tim Berners-Lee’s autobiography, Weaving the Web), this implementation dropped nearly all of the features that define hypertext in favor of a single one (jump links). For historical and political reasons, this implementation relied upon existing ill-fitting technologies (the SGML-derivative HTML, the filesystem- and host-oriented HTTP). For historical and political reasons (CERN’s clout, attempts to cash in on Gopher trademarks, early pushes to open source Apache and Mosaic, the proprietary nature and lack of portability and interoperability of many earlier hypertext systems), the web won.
Nevertheless, the web will not be the last hypertext system. The features that the web dropped are useful, and for the most part adding them back in on top of the web’s existing structure is fated to result in slow, complex, and unreliable hacks.
The situations that made a web-like system easier to design and build than a Xanadu-style system are, for the most part, gone. Over the past thirty years, making a web browser has graduated from a weekend project to such a major undertaking that only three or four modern browsers exist (webkit, mozilla, internet explorer, and possibly blink); at the same time, systems like IPFS have made permanent host-agnostic addressing easy. Furthermore, the web is no longer mostly used as crippled hypertext system — it is primarily an application sandbox. As a result, new hypertext systems are not even in competition with the web.
As someone who has worked on hypertext systems (both independently and under the aegis of Project Xanadu), and as a part of a community around the design and implementation of post-web hypertext systems, I would like to provide some guidelines for the structure of future hypertext systems. Tech has a short memory lately, and I would like future implementors to learn not only the lessons of the web but the lessons of pre-web hypertext systems (which often solved problems that the web has yet to address).
Guidelines:
1. A hypertext system should make no distinction between client and server. All such systems should be fully peer to peer. Any application that has downloaded a piece of content serves that content to peers. 2. A hypertext system should not distinguish between authors and commenters. While every piece of text is associated with its author, all text is first-order and can stand on its own. 3. Links live outside of text, potentially created by third parties, and are loaded as overlays or views. 4. There is no embedded markup. Links provide hints about how to format bytes, in addition to providing hints about connections between sequences of bytes. 5. A link has one or more targets, represented by a permanent address combined with an optional start offset and length (in bytes). 6. A document is not a sequence of bytes that render as the target text. A document is sequence of bytes interpreted as a list of pointers to sequences of bytes that, when concatenated, render as the target text. These sequences are downloaded (or retrieved from local cache) and assembled at render time. 7. While links can apply to a particular document, they can also apply to particular sequences of bytes independent of document. 8. A link can connect rendered documents to sequences of bytes (possibly included in many different documents). A link can connect the unrendered representations of documents with their rendered representations. 9. A user can individually enable or disable any links, and create their own. 10. Permanent addresses refer to unchanging byte sequences. New versions have new addresses. All content is stored at permanent addresses. 11. A second addressing scheme may be used to refer to the newest version of some collection of documents and links — a pointer to a permanent address containing a list of permanent addresses to documents and links, as well as the permanent address of the previous version of that list. 12. A hypertext UI contains facilities for both viewing and editing (i.e., creating a new version). Anything that is viewable can be edited, and that new version can be republished. 13. A new version is a sequence of references to the previous version combined with a sequence of references to any new content typed or uploaded. Multiple sources (even from different authors) may be combined (even by a third). 14. All quotation or content reuse embeds within itself a record of its provenance, being a reference to some portion of its proximate origin. This information may be flattened for local storage and fast lookup, but is always present. 15. A hypertext UI contains facilities for viewing both sides of a link simultaneously, side by side. It also contains facilities for fetching multiple versions of a document side by side, and facilities for finding other contexts for the same piece of content. 16. All byte spans are available to any user with a proper address. However, they may be encrypted, and access control can be performed via the distribution of keys for decrypting the content at particular permanent addresses. 17. No facility exists for removing content. However, sharable blacklists can be used to prevent particular hashes from being stored locally or served to peers. Takedowns and safe harbor provisions apply not to the service (which has no servers) but to individual users, who (if they choose not to apply those blacklists) are personally liable for whatever information they host. (To clarify, since some people have asked, I don’t recommend a centralized blocklist facility. Instead, users can subscribe to multiple independently-maintained blocklists, as addresses for versioned lists of hashes. This is similar to how sharable blocklists operate in ad-blocking software, and it is also similar to a proposal for fediverse instance blocklists & another proposal internal to IPFS.) 18. The system will not support dynamic content or automatically-running scripts, although users may download and manually run any code they find on it. While formatting links may apply complex styling to content, such styling is not turing-complete. 19. A user may create content privately, on their own machine, and refuse to publish it. This means that while it is stored in the private cache, it is not made available when it is requested by address. A user may then later publish it, in part or in full, encrypted or plaintext. 20. A user may keep a killfile of nodes or authors they would like to avoid automatically downloading content from. This content is replaced with a placeholder, indicating the source.
This story is also available on gopher at gopher://fuckup.solutions/0enkiv2/ guidelines-for-future-hypertext-systems.txt
[1]
By Rococo Modem Basilisk on February 9, 2018.
I hope so!
I hope so! I’ve learned some potentially useful lessons from some of the autobiographical stuff you’ve already posted.
By Rococo Modem Basilisk on February 13, 2018.
News and Lies 2:
News and Lies 2:
On post-truth
[1]
“What happens when anyone can make it appear as if anything has happened, regardless of whether or not it did?” technologist Aviv Ovadya warns. — Charlie Wartzel
A recent article by Charlie Wartzel summarizing the perspective of Aviv Ovadya has become quite popular. He admits that to scare-mongering, but justifies this by claiming that the situation is bad enough that we all really should be scared. Certainly, some of the details are accurate, but — as par for the course for a popular take on a newly-relevant subject with a long history — there’s a great deal of context missing.
Let’s first add a little historical context to defuse a bit of the appeal-to-authority that propels this article. Ovadya is described as having “predicted the 2016 fake news crisis,” on the grounds that he made a presentation about it in October of 2016. This is a very low bar: the political ramifications of propaganda circulated within social-media-amplified cultural bubbles was a hot topic throughout 2016, the same way it was at the tail end of Obama’s first term, when the publication of The Filter Bubble coincided with concerns about right-wing conspiracy theories. The Filter Bubble didn’t invent these concerns, either — that book was a (perhaps independent) rehash of concerns about internet news expressed on various mailing lists as early as 1992.
The term of art back then for what we now call ‘filter bubbles’ was ‘the daily me’ — as in, a hypothetical newspaper that, based on personality profiling, shows the user only the news stories they want to see. We can probably trace the idea back even further; however, I first became aware of the ‘daily me’ concept back in 2008, in a lecture at the Computers, Freedom, and Privacy conference — and I was one of the few attending who wasn’t already familiar with the concept, meaning that in the community at the intersection of tech and social justice, the political ramifications of fake news on social media was old news ten years ago. Ovadya’s epithet could be applied to anyone who was reading political coverage in mainstream news outlets in 2016 just as well as it could be applied to him, so his authority is not, by that metric, meaningful.
In the absence of the authority of someone who “predicted the fake news crisis”, we can critically re-examine the claims being made.
The narrative Ovadya & Wartzel paints is one where a fixed, stable, universally accepted common ontology is being eroded by tech that manipulates flows of information while simultaneously making forgery easier. This perspective is shortsighted — it doesn’t match history, and depends upon some dubious assumptions about the homogeneity of culture.
Humans don’t live in reality, and we never have. We live in networks of personal mythology, occasionally shaped and guided by reality’s physical limits, on the rare occasions when those physical limits interfere with our ability to maintain false beliefs in ways that are not easily ignored. Our personal mythologies are a bricolage of (potentially internally conflicting) heuristics and factoids, collected from all the people and media we interact with.
For a few hundred years, due to the standardization of educational systems around canons of works deemed important, large groups of powerful people (the rich, the intelligentia, royalty, etc.) had substantially similar personal mythologies. A literate westerner in the late eighteenth century could be reasonably expected to be deeply familiar with the bible, classical mythology, the works of Plato and Aristotle, and a handful of other works, along with being able to read and write in Latin (and probably French), regardless of their homeland. A literate westerner a few hundred years earlier would have probably been a monk who had studied the Trivium and the Quadrivium. From what I understand, similar highly standardized educational systems existed in China, established far earlier, and this system was geared toward state bureaucracy rather than religious institutions — however, I am not familiar enough with this system to describe it in detail.
The most important aspect of this situation is that it was not extended to most people — the poor and illiterate had their own private oral mythologies, influenced but not fully controlled by religious and state institutions. Improved printing technologies — cheap enough to use for mass entertainment and education but expensive enough to require a surrounding institution — universal mandatory education, and various attempts at education standardization exposed a greater number of people to particular memeplexes favored by particular groups of powerful people.
It’s important to note that, in the United States, this process was part of a then-politically-radical democratization made possible by access to low-cost printing technologies by dangerous terrorist subversives like Benjamin Franklin. When the revolutionary war was won, printers switched gears from propaganda pamphlets and broadsheets to general material for the education of a population who needed to be brought up to the minimum standards people like Thomas Jefferson thought were necessary to keep a democratic system from falling into tyranny. While today we generally see this process as a good idea, we should recognize that in the eighteenth century in europe and the americas, popular vote was seen as two steps short of anarchy and mass education was not seen as a universal good: from an outside perspective, we’re talking about dangerous political radicals determining the canon of an education system.
Of course, books were still quite expensive until the 20th century, with the introduction of paperback ‘pocket books’. The 20th century also corresponded to the development of film, radio, and television — popular formats that, like book and newspaper printing, depended upon expensive technology and institutions, and therefore were broadcast. This is the first point at which we can say that people’s personal mythologies began to mostly converge: the point at which a handful of national TV channels, a handful of nationally-syndicated radio networks, a handful of large movie studios with strict control over theatre chains, a handful of big newspaper companies and book publishers, a standardized education system, and a very active censorship bureau controlled much of media. This period could be bookended on one side by the beginning of the Great Depression (when movie theatres became cheap mass entertainment) and on the other by the late 1960s, when new limitations on post office censorship and widespread access to Xerox and Mimeograph machines made a mass non-broadcast culture feasible — what we call, variously, faxlore or zine culture.
The development of online communities starting in the 80s can be seen as an extension of this anti-broadcast trend that I trace to the late 60s. There’s a big overlap between early online culture and faxlore, ham, and CB radio culture, after all. This development has never been apolitical: as soon as scalable alternatives to broadcast culture appeared, people began to take advantage of it to create and distribute their own personal mythologies, and these mythologies have often had a political element — as with the development of discordianism starting in 1958, the radical political zines and newsletters on the left right and radically unseen sides through the 60s, punk zines in the 70s and 80s, and the faxlore origins of the proto-alt-right in the early 90s with anti-Clinton xeroxed ‘factsheets’. These strains made the jump first to Usenet and BBS, then later to the web.
All of this is to say that, rather than a sudden assault on the edifice of consensus truth, we are looking at the tail end of a sixty-year return to equilibrium — the conclusion of an anomalous century of relatively-homogeneous consensus reality.
Usenet and the web did something that BBSes (outside of store and forward networks like fidonet) and zines largely could not — they deterritorialized or delocalized exposure to alternate realities. People like John Perry Barlow and McLuhanist media theorists put this in utopian terms, and the culture jamming movement put it in functional, operative terms. After all, the broadcast reality is often wrong, and sometimes intentionally so: fraud, being expensive, was the domain of the powerful, and the democratization of the means of fraud (or, if you prefer, the democratization of disinformation construction and distribution mechanisms), was seen as a net positive. Culture jammers hoped that the good lies and the bad lies will cancel each other out in open forum.
When we talk about filter bubbles, the problem is not that such alternate realities exist. Instead, geographically-dispersed clusters of alternative cultures remain isolated from each other as a side effect of ranking algorithms. These cultures, which until the 80s corresponded to regions, now can cross state boundaries in difficult-to-trace ways. Because representative government is based on geography rather than psychography, this disrupts attempts to consolidate political power: it’s very difficult to gerrymander around a primarily internet-centric culture in such a way that a guaranteed win is possible.
Filter bubbles produce very real problems. The human capacity to ignore physical reality is impressive — only rarely does even mortal danger shake us, or else no veteran of active combat duty would consider themselves religious or patriotic, except perhaps in fairly warped ways, such as belief in a sadistic or blind-idiot god, or faith that no alternative exists to a zero-sum politics of spherical annihilation. Nevertheless, in less extreme situations, periodic challenges to our umwelt can indeed cause gradual change, and heavy exposure to diverse and conflicting alien myths can cause us to critically reconsider our own mythologies. Lack of exposure to alien myths means that the alternate realities produced by filter bubbles are just as stable as those previously produced by geography.
When we are familiar with the perspective of the ‘other side’, we can accurately distinguish between likely and unlikely stories — we can identify disinformation, even if that doesn’t impact our willingness to spread it. But, constant and consistent exposure to the same material eats away at our critical response. Furthermore, simply pointing out that some stories are false has unintended consequences.
So, it’s vitally important that we retain that exposure. However, at the same time, we should not assume that such exposure will avenge some mythic edifice of consensus truth. An (often justified) contrarian strain acts against the consensus, and the centralized power necessary for building the illusion of consensus can reasonably be expected to use that false consensus to bolster its own continued power.
Wartzel’s article highlights DeepFace, AudioToObama, and similar technology as mechanisms for supporting widespread fraud in the near future. I have a couple problems with these specific examples (in part because the technology is far from convincing, and in part because the limited scope of these projects and the existence of other related technologies means that they don’t substantially lower the cost of believable fraud), but on a fundamental level, fraud is always a possibility and our sense of what constitutes reliable evidence depends on a folk-understanding of fraud technologies. Nobody could use these technologies right now to convince a layman, let alone an expert, but the hype and scaremongering around them means that in the near future video will have the same status as photography in terms of perceived reliability — in other words, considered easily-faked.
The ultimate result of this — since video fraud is still approximately as expensive as it was 20 years ago — is that more skepticism will be expressed about ‘video evidence’, and that skepticism will be expressed earlier. This will ultimately probably mostly impact the people who have the resources and motivations to actually fake video evidence. It’s possible in the short term for people to take advantage of existing cultural bubbles to manipulate this skepticism toward political ends, but in the longer term we’re merely adapting to the state of the reliability of video evidence in the past several decades.
The time in which we live is unprecedented not for the unreliability of evidence but for its reliability. Again, we slowly return to equilibrium as political and technical competition takes advantage of short-term differences between the perceived and real reliability of certain kinds of evidence. There are real dangers associated with this manipulation, but they are not dangers to capital-T Truth but mundane ones — everyday grifting, political spin, and propaganda. Our best tool against this particular variety of manipulation is to maintain the accuracy of our folk-ideas about evidence reliability, not to demonize toys as existential risks.
Framing is very important. Where I agree with Wartzel and Ovadya is that there are serious systematic problems with the way we route information between people — problems that cause political schismatism, failures of empathy, and in some cases direct physical danger. However, Ovadya and Wartzel’s framing of this problem as one of attacks on consensus reality implies a solution with unfortunate authoritarian tinges: the reconsolidation of power over information.
Instead, I suggest framing the problem as systematic bias in exposure to information, in ways that limit the effectiveness of our normal intellectual growth. Rather than rebuilding the tower, we should be breaking down the walls.
Update: February 20th 2018: Wendy Grossman on the RISKS mailing list brought to my attention that the term ‘Daily Me’ was credited to Nicholas Negroponte, related to his work on early electronic newspaper technology at the MIT Media Lab. In 1995, Fred Hapgood, writing for Wired, claimed that Negroponte had floated the idea as early as the 1970s, and points out a quote from a 1968 book considering a related idea.
See also:
News and Lies 1: In Defense of (Some) Propaganda
We Can Weaponize Fiction, But How Do We Monetize Truth?
BladeRunner and the Synthetic Panopticon
By Rococo Modem Basilisk on February 13, 2018.
This essay feels like two essays combined: the first about the importance of erring on the side of caution, and the second about the importance of lore in tech. Attending an engineering school managed to instill in me the former; the latter feels more and more controversial.
I was getting into programming in 1999, when this essay was first being written. At the same time, I was getting into hacker culture. My worldview was impacted heavily by lore (as filtered through Eric S. Raymond, Steven Levy, Karen Jennings, and Steven Johnson — filters I later found were slightly yet systematically warping the specifics of some of these stories, but important figures nonetheless in that they wrote these stories in a form that a kid with a library card who knows no other programmers and has unreliable internet access can absorb), and at the time many of my sources emphasized the importance of lore. I stand by that importance — I agree that our mythology teaches important lessons, and I would go further, saying that (much like bibliomancy) good mythology continues to teach lessons that it was not constructed to teach, being general enough to act as a stand-in for wide swaths of truth, with meaning unfolding as one’s experience increases.
However, I get a lot of push-back when defending the importance of this lore, from people who see it primarily as elitist gatekeeping. I can understand this take, too: particularly as someone who, were it not for a series of happy accidents, would never have stumbled upon most of this lore.
On one level, the lore is functional. Referring to common lore is important as a communication tool. Even the gatekeeping element of lore has a function: lore teaches lessons (which unfold over time), and we would like to be surrounded by people who make good decisions, so previous familiarity with a piece of lore is a good proxy for having a nuanced understanding of some of the lessons that piece of lore teaches (although there are, as in all uses of shibboleth, false negatives: people who learned the same lessons through different means, who will be unreasonably excluded).
I see a lot of the bad ideas I see in Silicon Valley’s tech sector as related to unfamiliarity with pieces of lore that people who began learning to code a few years earlier almost inevitably would have absorbed. So, I feel like pretending lore is valueless is not just stupid but dangerous, too. It’s not merely in terms of cautionary tales, but in terms of stories that poetically illustrate the ramifications of particular design decisions (such as the apocryphal story of Marvin Minsky & the checkers-playing neural net, which at first blush looks like a blanket criticism of random wiring but can be taken as an illustration of the circumstances when avoiding programmer bias makes sense), or slang that makes particular elements of a technical mechanism viscerally clear (like the use of vomit metaphors for debug prints).
I suppose the difference is one of how learning the lore is framed. I always saw it as a joyful thing: sharing stories and jokes, and exploring history — something we do as part of our own self-development but also as bonding with friends. It’s easy to see it as a set of obscure tests intended to trip up outsiders, if you already feel like you don’t belong among programmers.
I’m not sure how to fix this. Popularizing the stories (the same way ESR did) helps, but there’s a tendency to eliminate important but unmarketable items (the same way that, in the realm of urban legends, the Vanishing Hitchhiker has had endless film and TV adaptations while the less-marketable spiders-in-the-beehive-hairdo myth from the same era and area has had nearly none). For instance, SV culture buys in 100% to the (mostly nonsense) hero-myth of garage entrepreneurs but appears to forget anti-authoritarian trickster myths like The Story of Mel.
By Rococo Modem Basilisk on February 13, 2018.
There’s another reason why bad films are particularly beloved by cinephiles: they illustrate (by negation) the importance of filmic techniques — particularly, the parts of cinematic language that are so basic and widespread that they lack terminology and are learned by experience. For instance, otherwise-competent scenes intended to be dramatic become comic because of slightly incorrect timing of cuts, showing the incredible emotional manipulation that cut timing by itself can achieve when used carefully.
Understanding what went wrong is a film nerd’s game, since it requires, rewards, and deepens understanding of the mechanics of the medium and conventions; as a result, the best bad movies are those that could have been great but fail in unusual or interesting ways. Most bad movies fail by being boring — doing conventional things in conventional ways and merely being slightly off the mark in creating the right combination to make it passably entertaining — but Manos: The Hands of Fate fails at being boring on the grounds that its filler scenes are out of place, combines some genuinely creepy cinematography with terrible sound design and incomprehensible lines, and keep flubbed takes in. Most bad movies fail by hewing too closely to the genre conventions and becoming predictable, but The Room fails to hew closely enough to a genre whose conventions the author misunderstood, and the result is an interestingly warped reflection with its own built-in hooks for criticism.
By Rococo Modem Basilisk on February 15, 2018.
Un-Jobsifying software development
Steve Jobs justified his distain of user studies by claiming that “users don’t know what they want”. This is true, as far as it goes — recent events have demonstrated the dangers of giving users what they think they want at scale in ways that even major media outlets can’t ignore. The second, unstated part of this, however, was more powerful and more dangerous: the idea that Steve Jobs knows what users want. In the case of Jobs, this concretely meant the uncritical adoption of design ideas intended for standalone appliances.
Unix philosophy is often summarized as “do one thing and do it well”. When stated this way, it’s pretty similar to Braun’s design rules. The third part, often left unstated but monumentally important, is “play nice with others”: what distinguishes the Unix command line from a Chumby or a Macintosh Plus is that the components are composable. Ignoring this third rule transforms a computer from a construction kit (capable of solving any problem with a little ingenuity) to a box of ready-made appliances (each not quite doing what you need, with no customer-servicable parts inside).
These twin misunderstandings define Steve Jobs’ legacy. (I say Jobs’ legacy rather than Apple’s legacy because the other half of Apple’s DNA comes from Wozniak, whose approach was much more in line with the Unix hacker’s emphasis on tools-to-make-tools and tools-for-engineering.) Apple’s late popularity (after twenty years on the edge of bankruptcy, mind) has cemented Jobs as some kind of design visionary and encouraged the uncritical adoption of the ideas he uncritically adopted: that designers know better than users about users’ own needs, and that walled gardens are more desirable than ecosystems. I’ve written at length about the second elsewhere, so I will focus on the first (which I have only touched on).
So long as programming remains a niche skill, the developer-user relationship is an asymmetrical power relationship, like that between teacher and student or doctor and patient. Like doctors, we attend to the needs of vulnerable people who don’t have the background to distinguish good advice from bad advice. Like teachers, we control institutional access (locking people out of accounts or letting people in, determining whether whole classes of people get to use our software based on our implementation of accessibility standards and our color scheme and font size decisions, and in extreme cases even controlling whether or not people go to jail). Like lawyers, everything we do becomes precedent for future situations — and tools for the less competent or scrupulous.
Unlike lawyers, no judge exists with domain knowledge and the power to shoot down ideas. Unlike teachers, no institution exists to discipline us for favoritism and no tenure or emeritus system exists to encourage the retention of experience. Unlike doctors, professional societies and standardized ethical codes have no control over our employability and people who make unforgivable mistakes cannot have their license rescinded.
As a result, if we care about the world, we need to be very careful about how we use our substantial power. A weekend project can serve more people than any building does over its entire life; we have an incredible responsibility to those people. Hacks get uncritically copied, trendy technologies get uncritically adopted, and users get caught between the poorly-fitting gears. We put liability waivers on our tools, but while we may no longer be legally responsible for how other people use our tools, we still bear enough residual causal responsibility to justify being extremely careful. The first step toward such care is getting rid of the illusion that we are more knowledgable and capable than the people we serve outside of a tiny corner of specialization.
The ideal interaction style between user and computer is not appliance-use but collaboration. In this way, the user and computer can develop each other together, each providing things the other could not alone. In order to make this possible, developers need to act collaboratively with their users, too: understand the core loop in their creative process, and provide tools that allow the user-computer-amalgam to be more creative and more discerning and tools to make those tools more effective. Tools-to-make-tools are not the sole domain of engineering: a serious musician invents tunings or whole new instruments, and a serious painter experiments with and ultimately invents new personal techniques; the only thing standing in the way of opening up the computer to such experimentation by “non-technical” users is communication.
Ultimately, by bridging these gaps, the distinction between “non-technical” and “technical” will begin to dissolve, the same way that public schools dissolved the scribal class.
[1]
By Rococo Modem Basilisk on February 16, 2018.
So much of debugging is about playing the game of lists (and so much debugging is still necessary with modern design tools) that I wonder if people who are abnormally bad at the list game are filtered out of software engineering at the very beginning stages — finding debugging extremely difficult and not realizing that it’s a very particular type of memory / imagination to blame.
By Rococo Modem Basilisk on February 19, 2018.
I’m with you here, Gutboom.
I’m with you here, Gutboom. The reverse-chronological feed is a *necessity* for anybody who spends a lot of time reading on this site (and indeed, on any site). Medium is the last in line to get rid of it (outside of mastodon) in what feels like an attempt to cater to a more casual audience — but this conflicts with their push toward membership (who would pay $5 a month other than the type of person who wants a reverse-chronological feed?) and their historical focus on catering to writers (what with all that jazz about typography). It’s in line with a series of changes that make the site feel like it wants to be more like a conventional social network and less like a communal blogging platform.
As somebody who likes Big Blocks Of Text, the sudden shift in content a few years ago toward short, shallow, image-heavy pieces felt like, if not a betrayal, at least a shocking reveal: I thought Medium was My People and watching them get in bed with low-quality content turned my stomach. But, I figured that was simply related to ad revenue: short pieces make sense when all you need is an impression.
I thought the open paywall model would lead to an increase in the proportion of material that was seriously (or comically, but nevertheless deeply) engaging — that Medium would incentivize good quality content by weighing your claps by the length and read percentage of pieces, and featured stories would be dominated by fantastic articles with estimated four hour read times and 99% average completion rates.
The average paywalled story, unfortunately, is not distinguishable from the average non-paywalled story: the norm is short, shallow, and sleazily self-promotional, dominated by reaction images, clickbait titles, and tired cliches presented as though they are life-changing revelations.
(This is not to say that Medium doesn’t have great stuff. Medium has some of the best articles I’ve ever read. I don’t think any of those are paywalled though.)
Medium managed to attract a community of talented essayists who care about writing. This thread’s comments are full of them! But, we’re the old guard now, and Medium has been spending years catering to and attracting people with very different needs (people who might be spending less than an hour a day reading essays on this site! For shame!). It’s risking losing us. I don’t mean to make a value judgement here, although I clearly prefer the kind of stuff I like to read; what’s objectively clear, though, is that Medium catered to us when nobody else did, and they now risk seriously alienating the one group that they have never had any competition for.
By Rococo Modem Basilisk on February 20, 2018.
It’s easy for automatic habits to take over even when doing things that seem like ‘original thinking’ (like writing an essay about original thinking). This manifests often as logical gaps obvious to attentive readers but totally invisible to the author. (Peterson is a good example of somebody who has a lot of these gaps & seems unable to think about them directly, even when they are pointed out.) One way to make sure that you’re really thinking when making arguments is to construct your argument as a response to the strongest possible counterargument to it — in other words, to debate yourself, and be as aggressive against your own argument as your strongest and most clever enemy could be. If you are tempted to go easy on your own position, that means that your argument is flawed and your position needs to be seriously reconsidered.
By Rococo Modem Basilisk on February 20, 2018.
Do you find that there’s a difference in effectiveness between films with a supernatural focus & films that are more grounded?
I tend to find grounded films (like, true-crime-adjacent ones about serial killers or other totally-human figures) hit “too close to home” & are as likely to worsen my anxiety as to help it — particularly when the conflict leans toward people yelling at each other (which bothers me a lot more than stylized gore). However, I’ve heard a lot of people claim that supernatural-centric films are too hard to engage with, and I’ve heard some people claim that true crime is specifically calming for women on the grounds that it deals with more realistic fears.
By Rococo Modem Basilisk on February 21, 2018.
Invisible Architecture II
The Maintenance of Haunted Spaces
[1]
When a space has a history known to those who walk it, a virtual element is added to its psychogeography — objects and structures become iconic, in the sense that they stand for particular ideas and events. This association is accessible only to those who know the context. Unlike the explicitly didactic stained-glass and frescoes of a cathedral, constructed to be legible to the illiterate masses, the configuration of stairs in a famous murder house or the wear on the testicles of the bronze bull statue on Wall Street require outside knowledge to become meaningful. Such spaces are haunted, in the sense that cultural memory has imbued it with meaning that changes how people interact with it without affecting the space physically.
Spaces are not, generally speaking, engineered to be haunted. The exception may be explicitly sacred spaces like churches or temples, where the layout is somewhat standardized and certain rote behaviors are associated with that layout without being generated by the physical space itself. A catholic and a buddhist visiting each others’ temples would not move in the same way as they would in their own. Most haunted spaces become so because of an event (sometimes imaginary) that recontextualizes the geography — a gruesome death on the stairway makes a dark splotch on the banister horrifying; the publication and acclaim of Ulysses elevates a particular path through Dublin to an odyssey; the apocryphal tale of a bunny-suited axe-wielder or a wronged woman’s suicide drives traffic to and away from a bridge at night, sorting people by their courage or foolhardiness.
Memorials are a unique kind of haunted space, one with institutions dedicated to keeping them haunted. So long as a space like Auschwitz remains haunted by the memory of what happened there, we can point to it as an example of something to avoid. Control is placed on a memorial’s psychogeography. We place explicitly iconic items in place of the things they signify, provide context to visitors as they enter, and perform careful environment design to ensure that for the most part the private experience of visitors is in line with the story we are haunted by.
Private experiences cannot be fully controlled — psychogeography has a margin of error. However, well-engineered spaces can be set up so that, at scale, independent people have a particular experience, and the differences cancel out. There are problems when groups of independent individuals are replaced with crowds: tour groups and field trip groups have different interests and different dynamics. People who know each other clump together or drift apart based on their affinity, which, in large enough groups, is a kind of complex astrodynamics of interacting orbits. Museums and memorials are designed to handle this, too.
When Pokemon Go came out, the alternate psychogeography it laid atop Auschwitz was immediately identified as problematic. We do not need to reference sacredness, offensiveness, or respect to understand why. The function of a memorial is to be haunted in a specific way, overlaying a conflicting set of values on that geography, accessible only to some, disrupts the ability of environment design to maintain that haunting. Memorials are functional, in that they specifically remind us of mankind’s biggest mistakes, and warn us against them. Detouring a memorial weakens that warning.
Read the first part of this series here.
By Rococo Modem Basilisk on February 22, 2018.
The best description of it is probably still in Licklider’s Man-Computer Symbiosis & Englebart’s Augmenting Human Intellect. However, this kind of symbiotic relationship is familiar to any developer who frequently does iterative development in a REPL — which is to say, almost everybody who develops in a scripting language on a unix.
I’ve written some specific recommendations here, with regard to bringing the expressive power & flexibility of command line interfaces to the GUI realm:
Some tentative guidelines for GUI composability Introductionhackernoon.com
As for forces that oppose this, I’d have to just point at lack of political will, short-termism, a focus on marketability and backward compatibility, and a general lack of interest in empowering people when it doesn’t add to the bottom line.
(In other words, capitalism.)
By Rococo Modem Basilisk on February 25, 2018.
There’s another problem with ‘trending’ — it gets the value-distribution relationship of information almost exactly wrong. People go on the internet to be told something they don’t already know; ‘trending’ presumes that people go on the internet to be told whatever their peers have already read (which they will inevitably hear second-hand from those selfsame peers if they haven’t already read it). If there’s any valuable popularity-based ranking, it’s boosting the visibility of things that are high-quality but little-known (which might not specifically require curators: upvote/downvote buttons let us identify how a story is ranked, and we could easily boost stories with high average rankings but low popularity). Curators focusing on the strange & little-known also are more useful than curators who focus on summarizing what’s already popular.
By Rococo Modem Basilisk on February 26, 2018.
Baudrillard reified the spectacular nature of Desert Storm’s media coverage with his famous phrase “The Gulf War never happened”. (Baudrillard was, of course, being provocative: what he meant was that there is only a very loose association between the actual gulf war & what the media depicted it as.)
And, of course, the Emergency Broadcast Network made subversive mashups of Gulf War propaganda footage a cornerstone of its video installations and performances.
By Rococo Modem Basilisk on March 2, 2018.
A design for a hypothetical undergraduate CS curriculum
CS100: Introduction to Programming in Lua This is the introductory programming course. In it, students are expected to learn conditionals, control structures, and the use of arrays and hash tables, as well as solve simple problems involving regular expressions. Will also cover the use of version control systems. Textbook: Programming in Lua by Roberto Ierusalimschy.
CS101: Introduction to Programming II In this course, students will learn to write the same imperative control structures they learned in CS101 in C, as well as how to interface C code to Lua and how to write makefiles and use standard Unix build tools.  Textbooks: Programming in Lua by Roberto Ierusalimschy, The C Programming Language by Kerninghan & Richie. Prerequisite: CS100
CS110: Discrete Math for Computer Science A course covering introductory number theory, introductory graph theory, symbolic and first order logic, and boolean algebra. It should also cover the concept of computability and universal turing machines.
CS200: Language Survey I Each class will represent an introduction to a new programming language, with assignments for small but non-trivial projects in that language due by the following class. Languages covered: Brainfuck, Forth, Befunge, Scheme, APL, Replace, Unlambda, and Hyper Set Language. Cannot be waived. Prerequisite: CS110
CS201: Language Survey II This course follows the curriculum laid out in Seven Languages in Seven Weeks. Textbook: Seven Languages in Seven Weeks, by Bruce A. Tate Prerequisite: CS200
CS202: Language Survey III This course follows the curriculum laid out in Seven More Languages in Seven Weeks. Textbook: Seven More Languages in Seven Weeks, by Tate et. al. Prerequisite: CS201
CS210: Databases This course covers database design and SQL.
CS220: Topics in Computing and Ethics This class is taught in cooperation with the philosophy department, and combines the study of historical ethical failures in computing & engineering with discussion of current events and material from moral philosophy. Must cover: the concept of virtue ethics, Kant’s moral imperative, utilitarianism and hedonism, the trolley problem, Simone de Bouvoire’s ethics of personal freedom, THERAC-25, the Challenger explosion, the ESA martian probe, the Tacoma Narrows Bridge, 5EYES, the pentagon papers leak, the christmas tree virus, the clipper chip, and the sesame credit.
CS230: Computing History Students will learn the history of computing starting with Ramon Llul. At the same time, they will learn basic hardware concepts like the FDE cycle, the parts of a CPU, and the layout of a vonneumman machine. Projects will include designing a microcoded 4-bit CPU capable of performing simple arithmetic, building delay line and relay based multiplexed memory devices. Textbooks: The Creeping Fungus by Karla Jennings, The Information by James Gleick, The New Hacker’s Dictionary ed. Eric S. Raymond, Turing’s Cathedral by George Dyson
CS240: Topics in generative art Students will generate poetry and prose using generative grammars, produce interactive graphics with L-systems and cellular automata, become familiar with the use of corpus statistics tools like Word2Vec for classifying and modifying texts, and learn about the history of generative art techniques (including exquisite corpse, cutups, dissociated press, markov chains, TALESPIN, SAGA II, Racter, the demoscene, puzzle canons, and oblique strategies).
CS300: Topics in Computer Security An overview covering symmetric vs asymmetric encryption, common vulnerabilities (such as buffer overflows and code injection), weird machines, social engineering, denial of service attacks, van eck phreaking, and fuzzing. Prerequisites: CS202, CS210, CS220
CS310: Technical Writing for Software Engineers A course on writing clear documentation. Will also cover public speaking.
CS320: Topics in Programming Languages An overview of lexer and parser technologies. Final project is to write an interpreter or compiler for a student-designed language, including full documentation. Prerequisites: CS202, CS310
CS330: Number Theory for Computer Science A course on number theory, centered on writing proofs. Also covers automated theorem provers. Textbook: A Friendly Introduction to Number Theory by Joseph H. Silverman Prerequisites: CS202, CS110
CS400: Unix shell tools and system administration Students will learn basic UNIX shell tools not used for development, and learn to write scripts of varying complexity. They will be expected to install and maintain a UNIX-like operating system.
CS410: Operating systems concepts Students will learn about memory models, privilege levels, paging, task switching, filesystem layout, hard and soft interrupts, monolithic versus microkernel architecture, and executable headers. Final project is to write a simple multitasking operating system from scratch with managed memory. Prerequisites: CS310, CS230, CS300
CS420: Social Topics in Computing Students will discuss the social impact of computers from the perspective of writings by non-engineers. Final project is a twenty-page paper on one way in which a piece of software has indirectly impacted a group of people that does not use it without their knowledge. Textbooks: Computer Lib/Dream Machines by Theodor Nelson, Interface Culture by Steven Johnson, Writing Spaaaaaace by Jay David Bolter, Track Changes by Matthew Kirschenbaum, The Gutenberg Galaxy by Marshall McLuhan
CS430: Thesis Project A student’s thesis project must not be written wholly or primarily in a language covered during their curriculum.
By Rococo Modem Basilisk on April 1, 2018.
Ad-tech and fatigue
Ad-tech and fatigue
The world is not getting crazier or more outrageous. The world has always been deeply strange, and it has always been full of injustice, horror, and loss. Communications technology has made it easier to remain aware of this, but more importantly, ad-tech has incentivized it.
Causing panic for personal gain is not a new technique. However, it used to work in two ways:
1. A state of panic lowers people’s guard, making them willing to accept less rigorous arguments, and thus making them easier to manipulate in particular ways. Arguments only need to sound convincing. 2. You can manipulate people by manufacturing a threat and then presenting your product as a solution.
Ad-tech changed this.
Panic (or, more precisely, autonomic arousal) creates a state of shallow vigilance: we are constantly scanning our environment for evidence of threats, but energy is diverted from the neocortex to the skeletal muscles to prepare for fight or flight. Shallow vigilance means increased engagement metrics: we refresh the page in order to check if the danger has passed, or if we need to do something other than freeze in place; we refresh the page again because our starved brain has already forgotten what we read ten seconds ago.
Ad-tech-driven renvenue models incentivize maximization of impressions (which are easily measured), not effectiveness (which has never been reliably measured), and so they optimize for engagement. This is not completely irrational: the mere exposure effect produces some benefit for advertisers, and does not depend upon reason, autobiographical or procedural memory, or any of the other facilities that are heavily impacted by panic.
Panic for engagement’s sake doesn’t need to be packaged with hope — we aren’t selling the remedy, after all. Panic for engagement’s sake doesn’t need to be temporary — we aren’t trying to foment an ill-considered riot and then turn former rioters into careful statesmen. It’s a new situation, because somebody with the systematic power to keep everybody panicing all the time also has the incentive to do so — and no incentive to suggest a way out.
Of course, constant low-grade panic has a body count. Arousal doesn’t merely redirect resources away from the neocortex — resources are redirected away from the immune system (so we get sick easier and remain sick longer) and the digestive system (so we are less efficient at extracting nutrients from food, experience intestinal discomfort, abuse our anuses with periodic torrents of diarhhea and our gullets with periodic torrents of vomit). Arousal forces the heart to work harder (increasing the likelihood of heart attacks & strokes), optimizes motor control for sudden forceful movements (making us jittery and unable to execute fine motor control), tenses up the muscles (increasing jitteriness and producing muscle fatigue), and generally wastes resources. Normal endocrine regulation of mood is put on the back burner — since this is an emergency, damn it — and so our normal protections against depression, mania, neurosis, and obsession are hamstrung.
On the other end, ad-tech acts as a broker between parties gambling on the value of ad impressions — both of whom, acting on models of advertising based on television and newspapers, assume that their advertising is operating on something less weak than mere exposure and therefore systematically overvalue it.
Mere exposure is important at scale: if you are Coca Cola, Pepsi, or McDonalds, your name is said so often that merely saying your name creates cravings; however, only such big players can afford to enter that game. If you aren’t McDonalds, you can’t afford to depend upon mere exposure: you need to reason with your customer, and for that, you need a customer capable of reasoning.
Even with both parties systematically overvaluing ad impressions, our impressions are still valued at a fraction of one one-hundredth of a cent. The broker can make plenty of money at this, by brokering billions of such deals per second. The host makes next to nothing, and with the price of impressions in free fall, must host more and more ads. The buyer of the ad spot gets next to no benefit from each ad impression in terms of actual sales, and wastes money buying more and more ad spots.
(Into this landscape walks the ad-blocker, which provides some tough love. The user has been screwed on this — footing the bill on cycles, electricity use, time, and network use, to a tune of several times what anybody else has made per ad impression — so he blocks the ads totally, saving himself money and pain while throwing the entire ad-tech ecosystem out of the casino.)
Of course, the news sites and social media systems are still optimized for ad-tech. They still optimize for engagement — even when they host no ads — because of a cargo cult approach to design that doesn’t consider their real goals. So the user has saved a couple cents on his electric bill, but he’s still getting sick.
None of this is “technology”. It’s not inherent to computers.
We made a mistake by accepting advertising as a revenue model on the internet. We made another mistake by forgetting that other models existed, once prices started crashing. Now, the people who are making the decisions are too sick and tired to see a way out because they are hamstrung by the system they created, and the people who need out the most can’t afford to take a break.
Computers and networked communication can be wonderful for the world, if we keep the world in mind when designing how we interact with them. If we keep our paychecks in mind, we lose both our paychecks and the world.
By Rococo Modem Basilisk on April 1, 2018.
Against trendism: how to defang the social media disinformation complex
There’s an essential mistake that almost every social media platform makes — one inherited from marketing (where it makes some sense), and one that is mostly unexamined and unaccounted-for even in otherwise fairly socially-conscious projects like Mastodon and Diaspora. In almost every one of these systems, incentives exist that confuse popularity with value.
I call this ‘trendism’ — the belief that an already-trending topic deserves to be promoted.
In marketing, because the piece of information being spread is intended to sell a product, the spread of that information is, in fact, theoretically proportional to its value. In social media, the information being spread is not a piece of advertising, and while most of these systems have revenue models based on advertising, that advertising is generated on the fly based on the viewer’s browsing history and has nothing to do with the content of the piece of information being spread.
The thing is, ideas travel in packs. When we encounter one idea, we tend to see its nearest neighbours also. When we find out something new, our friends hear about it too. So, trending posts are rarely surprising: by the very nature of their popularity, they are already familiar in their essence to most of the people who are directed toward them.
The information content of a message, in Claude Shannon’s formulation, is proportional to its deviation from expectation — information is surprise. Kolgorov’s formulation is similar: information content proportional to the smallest possible message that could say the same thing (which, of course, includes references to earlier messages or prior knowledge as a possible tactic).
In other words, from an information-theoretic perspective, a post that only tells you things you already know is worthless. Yet, trending content is almost always composed solely of things the viewer has already seen.
There’s one piece of information that a copy of a viral post actually has — the association between the content of the post and the person posting it. We share posts we’ve already seen as a way of expressing our identity, both personally and within a group. That is the only form of information valued by trending-oriented systems: tribal affiliation.
If we want to force our social media platforms into information-rich environments and lower the amount of tribal rivalry we are exposed to, there are a couple general-purpose solutions, and they all come down to kneecapping the machinery of trendism.
1. Rather than block political content (only one kind of tribalist content, and one that is at least theoretically grounded in genuine philosophical differences about the ideal shape of the world, rather than geography or social groups), we should block all shared content. Remove retweets and shares from your feed entirely. Most of them are things you have already seen, and most of the rest don’t contain meaningful or useful information. 2. Emotionally-manipulative posts get the most engagement, and are therefore ranked higher in feeds. (I don’t want to be emotionally manipulated. Do you?)* To defeat this ranking, force your feed to reverse-chronological order. To filter out emotionally-manipulative posts, filter out anything with more than a set number of interactions. 3. Avoid being part of the problem. Before sharing, determine: is the information true? Is it new? Is it playing mostly on my emotions? If possible, delay your sharing for a long period of time — read an article, and then wait a few hours, or even a few days, before deciding whether or not it is of sufficient quality to actually re-post. 4. Identify when you are being drawn into heated arguments, and ignore them. In the heat of the moment, you’re not actually making good points anyhow, and you’re more likely to misunderstand or misrepresent your opponent. The suggestions from #3 apply here too for comments — make sure your comments are accurate, informative, and cool, even if that means waiting several days to respond. Never let the system rush you. 5. Visible metrics gamify trendism. Remove them.
Most social media platforms don’t make it easy to follow this advice. Mastodon is closest — it hides metrics from the timeline by default, supports only reverse-chronological post ordering, and allows you to filter all boosts from your timeline. For everything else, you will need to use browser extensions.
Facebook Demetricator and Twitter Demetricator hide metrics like number of notifications, retweets, and likes. F. B. Purity has a variety of options for hiding post types, and allows you to permanently force your timeline order to reverse-chronological. New XKit does the same for Tumblr. I wrote a greasemonkey script to grey out Tumblr posts with more than 1000 notes.
If you’re looking to simply spend less time on social media, there are several configurable blocking extensions, as well as Intent, which merely records and displays how long you’ve been using particular sites.
*Emotion is important, but good communication comes after emotional reactions are understood and contextualized.
By Rococo Modem Basilisk on April 1, 2018.
Against UI standardization
A user interface is a translation layer that sits between the user’s mental model of a problem and the developer’s mental model of that same problem. UI design is, therefore, trying to solve the same problem as programming language design: how do we represent all of the details relevant to the problem domain in an unambiguous way while avoiding the need for awkward manipulations on the part of the user? Language design makes certain assumptions about the type, breadth, and depth of the user’s knowledge on certain subjects: notably, that the user is willing to read some documentation and expend some effort learning the representation in addition to the effort put into solving their problem. So, we can say that UI design is about creating languages whose full expressive power is revealed gradually — languages with a shallow initial learning curve.
UI design, as practiced, is not in line with this language-centric perspective, nor is it in line with the philosophies or practices of the much-cited mid-century thinkers who designed physical devices. A big part of this is involved with widget and UI pattern standardization.
Google, Apple, NeXT, and many others have published style design guides, while every UI toolkit has semi-consistent native styles (which are often difficult to change). Even on the web, where creating a wild experimental UI is almost exactly as difficult as creating a conventional one due to the document-centric structure of embedded markup based display, convention reigns. Why?
One answer is marketing. The ‘look and feel’ of a UI is considered an important part of branding. Of course, if you are not a first-party developer, remaining consistent with the look and feel of your host OS or UI toolkit is not a good branding decision — you are increasing the mindshare of someone else’s brand!
Another answer is poor tooling: UI toolkits often don’t have good support for theming or for creating genuinely new kinds of widgets, and often have system-wide restrictions based on the needs of built-in widgets that limit one’s ability to create behaviors the original developers didn’t consider. (Consider the difficulty in displaying large quantities of editable text on a 3d surface in OpenGL, or the difficulty in making two widgets overlap or intersect window boundaries in any mainstream UI toolkit.) However, this doesn’t explain why web UIs, which have no built-in widgets and need to use CSS hacks to rearrange text and images to implement any structure other than minimally-formatted wrapped text, are so conventional in appearance.
Perhaps developers fear that users will be confused about how to use a novel-looking UI? Of course, users are also confused about conventionally-structured UIs, which (by their nature) cannot match the natural way a non-technical user thinks about their problem.
Ultimately, I think the main culprit is a lack of imagination, and a lack of willingness to think deeply about the appropriate way to structure a UI for a very particular task.
Much as the developers of UI toolkits have failed to imagine that people might want to use their tools in particular ways, designers have failed to imagine that users neither know nor care about the stock widgets, templates, and patterns. They have needed to learn how to use the applications they use; they will learn to use new applications faster when those applications more closely match their workflow and conceptual models.
The user is the domain expert: they know their own business better than we do. The job of a developer or a designer is to cater to them and recommend possibilities, not to dictate their environment.
[1]
By Rococo Modem Basilisk on April 1, 2018.
Big and small computing
I’m a utopian, in that I don’t believe that computers are a mistake. I have big criticisms about particular technical decisions, but I don’t think those decisions were inevitable. An alternate computer universe, as projected from trends thirty years ago and earlier, was possible; with care and effort, it still is.
The biggest structural problem I see is a failure to distinguish between two different kinds of computing that have fundamentally different needs.
Big computing is computing at-scale. It’s the kind of thing anybody in the software industry is used to, and anyone not in the software industry is accustomed to complaining about. Big computing is client-server. Big computing processes big data. Big computing has millions of users. Big computing hides ‘advanced settings’ behind a checkbox or a button so ‘regular people’ don’t get intimidated. Big computing has maintainers, bug trackers, and devops on call. Big computing is worried about accidentally committing experimental code to prod. Big computing writes tests, cares about strong typing, and writes things in java because it’s easier for HR to find qualified candidates that way. Big computing is worried about job security. Big computing has a project manager and stock options. Big computing ships.
Small computing never died, but you wouldn’t know it from reading Hacker News. Small computing has an average user count of one. Small computing is peer to peer, and human scale. Small computing does exactly what the end user wants, because the end user is the developer. Small computing doesn’t distinguish between programmer and non-programmer. Small computing doesn’t care about marketing. Small computing is open source because there’s no point in using a restrictive license, not because anyone will ever submit a pull request. Small computing is as unique as a geocities page. Small computing plays.
If you are being paid, you should be doing big computing. Big computing means scale, and scale means that your decisions have technical, social, and ethical ramifications that you have a responsibility to seriously consider. This means asking for permission. It means facing reality, caring about security, avoiding intellectual laziness with regard to tool choice, and maintaining familiarity with the lore. Major technical problems often can be traced back to the application of small-computing mantras (“move fast and break things”, “yagni”, “it’s better to ask for forgiveness than permission”) to big-computing situations. Big computing should be extremely conservative, and because of its centralized and hierarchical nature, we should be making decisions based on the categorical imperative: make a technical decision only if you think it would easily and unproblematically scale to every machine in the planet forever.
On the other hand, I consider small computing much more important than big computing. Big computing, because it is big money, gets all the attention; however, big computing is one-size-fits-all and therefore doesn’t quite fit anyone. Every programmer began in the context of small computing, and every programmer, in his or her off-time, operates in that context. Systems geared toward small computing (like REPLs, notebook interfaces, smalltalk VMs, and the UNIX command line) are incredibly powerful. Unfortunately, small-computing systems are not made accessible to non-programmers, even though they absolutely could.
Almost all user-facing interfaces should be small-computing. Big computing should only exist as a fallback when we, as developers, have failed to make small-computing-oriented systems sufficiently unintimidating. Users should be able to gradually learn to program, without reading manuals, simply by interacting naturally with their computer’s UI and performing the kinds of casual customizations we all do to optimize for our use cases. The system of even a non-technical user should be composed of 75–80% code written by that user, within a few months.
On the other hand, big computing, because it is professional, should be subject to licensing. Licenses are not a guarantee of competence, but they are a mechanism that filters out those unwilling to make minimal effort, and they also present a mechanism for ethical lapses to be effectively punished. (“Why don’t I have a license? Oh, Uber asked me to implement a fake surge pricing mechanism and I said yes. Oh, I lost my license because I collaborated with an NSA wiretapping request. I lost my license because I exposed a credit card database to an unvalidated input field. I lost my license because I didn’t implement buffer overflow checks. I lost my license for using unsalted SHA1 for password hashes.”) Big computing can ruin people’s lives, so professional developers and their employers should be legally liable for their decisions.
Here are other essays I’ve written on related topics:
By Rococo Modem Basilisk on April 1, 2018.
How to fail at culture jamming
How to fail at culture jamming
The story of the twenty-first century media landscape thus far is the story of the left’s own tools being used against it, more effectively than they had previously been used. ‘Fake news’ is the currently-popular term, but we’re really talking about disinformation (from the Russian Дисинформатся), a particular variant of propaganda. More specifically, we are talking about culture jamming — a set of techniques intended to disrupt, subvert, or complicate existing unconsidered ideological positions using non-rational or pre-rational mechanisms.
Culture jamming has its origins in the Situationist movement’s idea of detournment and in the Discordian religion’s idea of Operation Mindfuck. Detournment is the remixing and recontextualization of some existing sign, subverting its meaning and demonstrating its underlying absurdity: a common version of this among Situationists is to substitute the dialogue bubbles in comics or subtitles in foreign films with surreal slogans that contradict or poke fun at the original work (and such classic detournment is a stable of modern internet culture). The Situationists thought that scrambling the message of centralized corporate-capitalist media was necessarily good — that only in this way could the progress of consumerism (the “Spectacle”, an attitude wherein things are only important to the extent that they participate in an economy of conspicuous consumption) be slowed. Among Discordians, all strongly-held beliefs are considered suspect, and until recently it’s been mostly held as obvious that the creation of any kind of doubt (particularly doubt with an irrational element) is ultimately a good thing.
Culture jamming, as codified by Kalle Lasn, is a combination of these sets of techniques, but is put toward the kind of centrist liberalism that Lasn’s Adbusters magazine supports: it is neither the scattershot ontological terrorism of Operation Mindfuck nor the focused radical anti-capitalist surrealism of Situationist works, but merely a marketable and centralization-friendly application of a mix of counterculture-oriented & traditional marketing techniques geared toward a vague greenwashing-friendly folk-environmentalism and capitalism-friendly anti-consumerism. By growing the political tent to include centrists of all stripes, Lasn did for memetic subversion what Eric S. Raymond did for free software: he divorced techniques from the moral and structural criticisms they were originally meant to fight against, and opened up the door for even fairly uncreative people to use those techniques to uphold the status quo.
Of course, Lasn does culture jamming wrong — which is to say, he performs the techniques ineffectively. The mistakes of Lasn (and Adbusters in general) are basically similar to the ones being made by the center-left, the center-right, and almost everybody using the techniques today (with the exception of some Discordians and certain parts of Russian intelligence).
Most of these mistakes are 101-level propaganda failures, warned against in Paul Linebarger’s text Information Warfare — literally the US Army’s psyops/ propaganda manual. The biggest one is a failure to properly identify and cater to your target audience.
Almost all pieces of ostensibly-subversive media are dual-use, and fail to perform either task effectively because of it. Signalling one’s own position is important — it’s how group norms are communicated and maintained — but it’s poison if you’re trying to communicate with the outgroup. Generally speaking, your ingroup’s idea of the motivations, behaviors, and beliefs of the outgroup does not accurately represent how the outgroup thinks of themselves, and acts as an ingroup signifier for your group. Any media based on how some group’s ‘enemies’ think of that group, rather than how that group thinks of themselves, will not merely fail but be instantly identified. For instance, the right characterizes the left as wanting to be babied by designated superiors, while the left characterizes themselves as expecting a basic level of empathy between equals. A right-wing meme that targets leftists will fail if it is built on the model of leftism that assumes a hierarchical structure of care. Likewise, the left characterizes the right as callous, while the right characterizes itself as self-sufficient. Any left-wing meme that targets the right will fail if it assumes its target thinks of itself as callous! Such messages are good for defining and communicating the values of the ingroup, but worse than useless as propaganda.
[1] [1] Ineffective, because they are based on outgroup models
The second biggest problem is the direct attack. Even if you have correctly identified the opposing viewpoint and have correctly identified a logical flaw in it, pointing out that logical flaw directly will be ineffective. Positions are not generally held for purely rational reasons — there is an irrational core to every belief, made necessary by the absence of full information. (If we were all perfect logical-positivists and refused to believe anything without solid evidence for it, we would all be solipsists, since occam’s razor would tell us that it’s easier to believe all our senses are totally unreliable than to believe that a world as strange as ours exists at all.) We build up rationalizations around a core logical leap, and we build our group and personal identity around those rationalizations. A direct attack on a logical inconsistency that keeps us glued to our family, friends, and favorite TV shows is rightly considered an attack on our entire way of life. So, while logic is necessary for formulating a piece of propaganda, it must be obscured in the piece itself.
Consider two theoretical pieces of propaganda, both intended to convince a right-wing person to support universal medical care. One shows a scowling Clint Eastwood, with the text “Don’t be such a hard-ass; support universal health care.” The other shows John Wayne, in character as a cowboy, taking care of his horse, with the text “Sometimes, even a cowboy needs a support from a loyal friend; support universal health care for all the cowboys in your life.” The second will probably be a lot more effective.
[1] [1] Somehow, I couldn’t find a picture of John Wayne grooming or feeding a horse. So, here’s a filtered version of an unnamed man in a cowboy hat, from a horse-grooming how-to site.
The first caters to an outgroup assumption about the target audience (callousness) and contradicts it while failing to argue against it. The second uses imagery that represents your target audience to themselves (cowboys, seen as strong and independent), brings up an obvious flaw in that attitude (“nobody’s actually fully independent”) obliquely so it doesn’t feel like an attack, and then reverses the normal thrust of both left-wing and right-wing rhetoric on the topic (suggesting that, contrary to the idea that universal health care makes people less independent, much like a cowboy’s horse, it makes people more independent by being like a “reliable friend”). It reframes some right-wing ideas around the topic without naming them (health care is a friend rather than a parent, and — like a horse — a servant rather than a master), and is quite explicitly gentle around a hot-button topic.
The third problem Lasn makes — one that most current practitioners of culture-jamming are incapable of making — is a dependence upon normal centralized distribution mechanisms & economies of scale. Lasn published a book through a conventional publisher and then started a magazine. This requires a lot of people and a lot of money. As a result, the content can’t be too radical — investors can’t be turned off by it — and so-called ‘grey or black propaganda’ (i.e., situations where the group the material came from is misrepresented or unknown) is off the table. Before the internet became most people’s primary media intake channel, grey and black propaganda either required a lot of independent resources (for instance, ostensibly-native German-run pro-Nazi pirate radio stations in WWII Britain & the American-run pro-American equivalent in WWII Japan) or some cleverness (like Joey Skaggs, who independently created fake organizations and shell companies and manipulated the use of unverified press releases and media packets by lazy journalists). Today, propaganda generally comes without clear lineage. We see shares of shares of screencaps of reposts, and unless the bias of the originator is very poorly hidden, we can’t tell that the originator’s group is at odds with the target’s group.
The propaganda outlet RT is a great example of this done well: it caters primarily to a group of comorbid suspicions, all of which are pretty reasonable (yes, the United States *does* perform morally-dubious acts and hide them from its citizens; yes, you are absolutely justified in being worried about that), and uses its selection of mostly-accurate facts and mostly-believable fabrications to subtly push its agenda. One would not necessarily know, from the content RT puts out, that its parent organization is simultaneously making both right- and left-wing Facebook groups, running bot swarms to retweet both white-separatist and black-separatist messages, and organizing protests and counterprotests of their own events in several countries. Instead, RT basically looks and feels like a center-right version of The Intercept with more UFO-related content.
When we talk about the distribution of information, we often use epidemiology metaphors. These metaphors are more apt than one might think: not only does information spread like a virus, but even notions of immunity are shared. Actual viruses have a protective covering around their DNA called a capsid, and this capsid allows them to attack particular kinds of cells by binding to particular kinds of receptors. Memes have capsids too: every meme has not simply a juicy inner statement, but formal elements like font, language, and imagery choices that indicate which audience it’s for. We decide whether or not to read the text of an image macro based on whether or not these more obvious signifiers indicate that it’s meant for us. Once we read it, we are able to be affected by it. The biggest barrier is a failure to properly target messages meant for people not like you.
Perhaps even larger than targeting is the open question: do we really want our propaganda to be effective? If we know how to target the other side but we still fail to put ourselves in their shoes, it means that we have decided that the masturbatory self-gratification of demonstrating our ingroup solidarity is more valuable than achieving our ostensible aims — or that the outgroup is so awful that we would rather our project fail than empathize with them. Perhaps this is true: maybe, sometimes, we face an enemy so inhuman that communication is pointless and empathy only corrupts us. However, I refuse to believe in inherent evil; I think our best bet is still communication, even if our communication must be evasive in order to reach home.
By Rococo Modem Basilisk on April 1, 2018.
Meditations on morality
Meditations on morality
1. The universe doesn’t optimize for morality. The moral arc of the universe doesn’t bend toward justice unless we bend it. 2. Even ‘good’ people don’t necessarily optimize for morality, unless they are primed by their environment to look at things through a moral lens. We tend to optimize for whatever we’re focused on. That’s not usually morality — often it’s survival, or something we have been told is equivalent to survival (such as money, miliary dominance, or the relative power of our ethnic or political group). 3. Nevertheless, improving the world (and encouraging others to do so) is worthwhile. The world can’t be fixed but it can be improved. Some improvements are even low-hanging fruit — never performed, because distractions from moral imperatives are so effective. 4. Optimizing for morality is just like optimizing for anything else: if you don’t keep your model updated with new information, you will end up maximizing something else entirely — something that isn’t quite your goal, and that (at the extremes) conflicts with it. 5. Morality is hard to quantify, but ethical systems are not. Each ethical system is an attempt at codifying what constitutes moral behavior. 6. Ethical systems conflict on the margins and in pathological or corner cases. Our familiar moral thought experiments tend to highlight these conflicts, because they are designed to differentiate between systems, as a test of which system is more effective. 7. Nevertheless, ethical systems tend to agree outside of pathological cases — because they are attempts to approximate the behavior of our collectively-evolved internal compass, which basically does generally agree on what is right. 8. Moral lapses tend to occur in a domain in which ethical systems are in agreement about the appropriate behavior. They tend to be caused by optimizing for some goal other than morality (or even adherence with some ethical system). 9. By studying the common features of ethical frameworks, we can determine something about the function of the moral compass. Specifically, every ethical system appears to be a heuristic or set of heuristics about how to scale society beyond the individual or family unit while minimizing damage [1]. So, we can conceive of morality as social scalability. 10. Knowing what we are trying to do — in other words, optimizing for social scalability, rather than trying to minimize a sense of disquiet that itself slowly evolved as a metric for whether or not our behavior scales to a 150-person group, differs between individuals, and cannot be easily quantified — allows us to more easily determine when particular ethical systems are appropriate tools. 11. Even without such explicit optimization, our moral compass is effective when it is used. Normalizing looking at things through a moral lens is one way to make use of the moral compass more widespread. 12. Organized religion once served part of this function: a community would get together, talk about morality, and enforce morality by shunning or criticizing people who performed acts deemed immoral. 13. However, organized religion has vulnerabilities even in this role: despite the flattening created by the protestant reformations, these structures tend to be hierarchical and authoritarian, focusing on codified virtue ethics, and are isolated to a separate conceptual domain of life. 14. Because of the hierarchy, a congregation has a single point of ethical failure: since a single individual controls what is and is not considered a moral lapse, that individual exerts substantial amounts of power over the norms of the group. With multi-level hierarchies (such as in catholicism), a single person in a position of power can effect even greater damage to norms. There are various mechanisms intended as checks on this power (which I attribute, both on the protestant and catholic sides, to the criticisms that led to the reformation), but they are often ineffective. The worst case scenario, when the hierarchy goes wrong, is something like Jonestown or Heaven’s Gate. 15. Because of the emphasis on codified virtue ethics, norms cannot quickly adapt to changes in circumstances. In many cases, the original reasons behind particular rules are forgotten and the rules are applied outside of their intended scope. (For instance, anti-castration rules in the New Testament were intended to distinguish christianity from other, more extreme cults that arose within judaism, many of which required adherents to castrate themselves in order to guarantee celibacy. Such rules have been deployed against transgender people.) It’s easy for virtue ethics to be coopted as political weapons. 16. The conceptual isolation of moral thinking to a particular corner of life makes it possible for people to perform immoral deeds without thinking about the moral implications, despite having a venue in which they are required to employ a moral lens. Such “sunday christians” do not think of themselves as bad people — they simply don’t consider morals outside of church, and therefore are unable to evaluate their own righteousness. 17. The very nature of religious framing produces a vulnerability in the form of various reversals. Religious framing claims that what is good for the society will inevitably be rewarded in the individual. This is not necessarily true. When we contract the time-frame — saying that good deeds are rewarded during life and bad ones punished — we end up with something like the prosperity gospel, which claims that behaviors that materially benefit the individual in the short term are necessarily righteous while those who are suffering are necessarily evil. Since the function of ethical systems is to discourage people from acting in their own short-term self-interest when that interest creates greater damage to society as a whole than the benefits accrued to the individual, a prosperity gospel framing actively inverts this and justifies immorality a priori. 18. Rather than an isolated moral domain, we should encourage people to discuss and enforce moral behavior in their daily lives. When making business decisions, we should ask “is it right” before asking “will it make money”. We may be wrong on both counts, but by asking, we will at least consider the moral dimension — eliminating the low-hanging fruits of obviously-immoral behaviors we engage in because we were focusing too intently on something else. 19. By talking about morality, we remind people to think about things through a moral lens and we give them the mental tools to consider moral issues — tools that necessarily go beyond virtue ethics, and should include utilitarianism, De Bouvoir’s existentialist ethics, and the moral imperative. People who care about doing the right thing will be more effective at it. 20. By holding people accountable for their ethics collectively, we force people who don’t care about doing the right thing to nevertheless behave in ethical ways (or spend effort hiding their ethical lapses). Ultimately, it makes life harder for the people who make life harder for the rest of us. It becomes in even sociopaths’ self-interest to contribute to the greater good.
[1] The following ethical systems can be thought of in terms of scalability of different aspects of life:
Virtue ethics: this set of behaviors tends to be found in healthy and well-functioning societies, so it becomes part of a set of “best practices” for individuals. (Different virtue ethics have different rules, and often mix in hygene-related rules or rules that depend heavily on the structure of the society. Aristotle’s virtue ethics has material that only makes sense in a greek-style society with citizen-soldiers rather than a standing professional army, for instance.)
Kant’s moral imperative: can this behavior scale to an entire population? In other words, could what I am doing now reasonably become a part of a future virtue ethics?
Hedonism: is this action producing any good at all?
Utilitarianism: is this action producing a net gain in the happiness of the society?
De Bouvoir’s existentialist ethics: is this action making the world freer? (In other words: am I ensuring that the expressive potential for other people’s decisions grows?)
By Rococo Modem Basilisk on April 1, 2018.
On tackiness
Tackiness is a moral issue, in the sense that it identifies free-riders. This is why it’s punished so harshly.
Tackiness is a quality we attribute to any action that produces cultural capital without actually benefitting the community as a whole. We’re wired to punish tackiness the same way we punish other kinds of ‘cheating’.
(The special case of social class in tackiness is something I won’t get into too much. Suffice to say that conspicuous consumption has weaponized anti-tackiness reflexes in order to destroy social mobility.)
Social groups (and media) are characterized, to outsiders, by their tackiest elements. The reason is that people who optimize for social capital to the exclusion of all else tend to get it (if only temporarily). Whoever in your group has the greatest self-promotion to group-benefit ratio (whoever is tackiest) will end up being the public face of your group, because everybody else is content to communicate among themselves and follow social rules. Only the tacky actually break through the edges of the group while embodying (a distorion of) group norms, as a side effect of trying to saturate the consciousness of the whole group.
Case in point: I joined Twitter in 2006. Hashtag use was never ‘normal’ among regular users. Using it felt tacky if not slimy. However, since hashtags are an amplifier, people who did use them became loudest. They came to characterize the platform to outsiders. As the platform grew, outsiders became new users, and they acted the way that they had been told twitter users acted (which is to say, they thought normal users were expected to act like marketroids and spambots). Thus, the new users got louder, and shouted over reasonable regular conversation.
How do you prevent that? One way is to ensure that tacky behavior is not merely socially punished but actually technically only marginally effective.
I think the fediverse (the federated social network formed by Gnu Social, Mastodon, and others using the ActivityPub and OpenSocial standards) does this relatively effectively: behavior is isolated to particular groups, and it takes more effort to hop between groups. Because of this, the ability for a message to get ‘outside’ depends fairly heavily on catering to the needs of the regular users, who feel a sense of ownership and protectiveness around their communities. Attention-seeking messages don’t have the same kind of edge on community-serving messages in terms of expected attention that they would on Twitter. Furthermore, because metrics are typically hidden, it’s slightly harder to carefully tune messages for virality.
By Rococo Modem Basilisk on April 1, 2018.
Software utopianism in a flawed universe
Software utopianism in a flawed universe
“I believe in the power of good I believe in the state of love I will fight for the right to be right I will kill for the good of the fight for the right to be right” - David Bowie, The Cygnet Committee
Whenever I post about how I think software should be designed, I get called naive. Sometimes I’m told that what I propose isn’t technically feasible (even when what I describe has already been successfully implemented), but more often, I am told that existing social structures prevent my goals from being adopted at scale.
Sometimes this is true.
I sympathise with people who write bad code with bad technologies for work, and have no say in important technical decisions. (I am, of course, also often in this boat.) I don’t think that negates the importance of clearly imagining an ideal condition and working toward it.
We live in a world that constantly asks us to lower our standards. We build industries off toy technologies while ignoring theoretically-solid ones. We get told “worse is better” often enough that we almost believe it.
Nevertheless, our ability to implement better systems is limited primarily by our willingness to imagine them, and as our technical tools have gotten increasingly more expressive, imagination has become even more of a bottleneck. In 1992, it was reasonable to claim that the web was a reasonable concession to hypertext given the deadline; today, it is far easier to make a real hypertext system than a standards-compliant web browser (let alone a usable one).
Better systems aren’t necessarily more widely adopted. This might matter in the context of a business, but business has no place in utopianism: the pursuit of money is not fully compatible with making the world a better place, and when the two conflict, the pursuit of money should lose. With business concerns out of the way, there’s no reason for adoption to be maximized.
Out of all of the utopian rhetoric circulated about the internet in the early 90s, one piece remains true: communities form based on shared goals, interests, and norms, regardless of geography. Build a better world in a pocket of cyberspace, and build it with your friends. Start a revolution in the style of bolo’bolo — by building an alternative to the monoculture and letting people visit. Taking over the world is neither necessary nor desirable: anyone who likes your system can adopt it on their own.
Every alternative to the mainstream, no matter how small its community, demonstrates that another world is possible. Every alternative demonstrates new possibilities, which can be adopted piece-meal by others. Small scale isn’t a liability, here: the stakes are low, and so experimentation is possible.
As a practical utopian, I recommend a three-pronged approach to saving the world:
1. Whenever possible, make good decisions and avoid bad ones. 2. Whenever possible, explore alternatives, both conceptually and experimentally, alone and in groups. 3. Whenever possible, remind people that “the way things are” is not inevitable, and that drastic improvements are still possible.
Attempts to improve the world are worthwhile even when they fail. The alternative — accepting what we have — is untenable.
By Rococo Modem Basilisk on April 1, 2018.
The manufacture of steam-engine time
Technological determinism presents a tempting narrative. We see society shaped by technology and technology shaped in turn by society, and we feel justified, by that tight coupling, in saying that technologies that failed did so for a good reason and that our current lineup, because it survived in the hyper-speed pseudo-darwinism of the global market, is fitter in some general, eternal sense. Path dependence, because it’s so difficult to predict ahead of time, gives us the illusion of destiny. The phenomenon of simultaneous discovery bolsters this feeling — “it steam engines come steam-engine time” because until that point the prerequisites for steam engines were not in place, and come “telephone time” six people file patents simultaneously.
It is, however, an illusion. Technology (as the class of all human-made items) doesn’t “want” anything, although specific technologies have particular prerequisites and particular side effects that make them fit better in particular environments. Unlike the pseudo-darwinism of global markets, this is a real darwinism: the only metric for success is existence, and fitness is always contingent upon the environment. A technology that is fit for 1998 is not fit for 2018, but likewise, a technology that is fit for 2018 is not fit for 1998.
Path dependence is real, but is also mere convenience writ large. We use the materials that are available, and the ideas that are available, and cater to available incentives. Path dependence locks us into a route, and we value that route because of what we have seen at the end of it, despite being totally ignorant of what lies along diverging routes. As our technological path influences our social path, we move toward a future based on present convenience, with no reason to believe that it’s the best possible future (or even any reason not to believe it is the worst one).
Simultaneous discovery is symptom of a similar kind of mental imprisonment. We produce ideas via a random walk through our inner semantic landscape — we move between similar ideas. Occasionally, we will make a connection that is novel enough to be patentable or protected by some other IP law — in other words, the state, not knowing how to increase the number of novel ideas, merely incentivizes ideas that rise above a particular threshold of novelty. If a connection is novel enough to be patentable, that really just means that the person filing the patent has a mental landscape different enough from the patent examiner that different connections seem obvious, but similar enough that these new connections can be communicated. (The relative nature of novelty is highlighted by the domain of software patents, wherein basic concepts or forty year old practices get approved by patent clerks whose background is generally in mechanical engineering.)
Ultimately, simultaneous discovery means that the people working in a field are part of a monoculture — enough people have similar enough backgrounds that they produce the same thoughts at the same time. Having a monoculture like this is a waste of effort: these people could be producing different ideas of equivalent value, if only they had been diversifying their reading material!
Simultaneous discovery doesn’t mean a global monoculture — more often, it means that there’s a specialist monoculture. Everybody with interest in a particular field & access to the means of production has similar life experiences & is reading the same canon of books — the kind of isolated culture initially made possible by broadcast media and currently maintained by the amplification of broadcast-seeded microcultures by social media stovepiping. As we’ve seen in Silicon Valley, this kind of homogeneity produces inventions that are a poor fit for everyone outside the bubble. (And, of course, the now-narrowed path is narrowed further by market forces, which eliminate possible technological lineages based on their ability to make money, often by impressing an even smaller and more isolated group of investors. The association between this and utility in the wider world is approximately zero.)
If we want our technological paths to work for us, we will need to tame them — to perform technological husbandry.
The first step is to lower the forces of path dependence by going out of our way to reward seemingly-pointless, seemingly-impractical work for its novelty — in other words, to cause each current path to bloom into thousands of branches. The second step is to cull paths based not on market forces or appeal to a group of rich eccentrics but based on the possibility of long-term value to society. (We need to cull these paths late — let them develop unguided for years — in order to avoid making short-sighted decisions about their value.) The third step is to make sure all of the work is documented and available to the world. This produces a proliferation of paths, and provides the material with which to impress upon people the variety of possibilities.
Once we have created a cabinet of sufficiently distinct possible futures, we should work to disrupt forces that encourage monoculture. This is not a matter of individual improvement: systemic problems require systematic solutions. One way to discourage monoculture is to discourage both broadcast communication and power law orientation: direct people toward projects that are understaffed, rather than ones that are currently succeeding.
This kind of project does not require government integration. In fact, I would discourage it as a part of any large institution. Taking as a model small writing awards and chindogu competitions, I recommend the formation of many fully-independent small annual competitions for the seeds of novel ideas, each providing a small number of bigger prizes for former entries that have been developed in interesting directions five to ten years later.
By Rococo Modem Basilisk on April 1, 2018.
I worry about the tendency of large institutions to coalesce power & apply that power toward their own biases. Since we’re trying to maximize variety, it makes more sense to lower the necessary resources to the point where many small institutions can perform their own versions.
Consider something like the Long Now Foundation — they’re a fairly strange group, with unusual aims, but for various reasons (mostly related to the personal connections of individual members) they have a huge amount of clout, and therefore they can’t really be as experimental as they need to be to achieve their aims. They have too much influence to be able to take small risks in public.
A better model is writing contests. For a little while, I followed a publication that did a monthly roundup of open writing contests, and I was surprised to find that each month there were hundreds, mostly funded by small universities or foundations, with prizes between $25 and $1000 and restrictions ranging from “entries must be below 100 words” to “authors must be native american lesbians from rural Wyoming over the age of 65”. Individual contests have very specific biases, but each one is small enough that the total bias across all contests is kept small.
By Rococo Modem Basilisk on April 3, 2018.
I’d argue that the BEAM needs a prolog-y language too — erlang is prolog-like in syntax only (lacking unification, backtracking, cuts, and other really fundamental constructions). This really screwed me up when I was first working with it — I expected functions to be reversible, expand unbound variables to the herbrand universe, etc.
(I don’t think erlang should become more prolog-like. I just think a more prolog-like language would benefit from the kind of message-passing & implicit multithreading that BEAM makes straightforward.)
By Rococo Modem Basilisk on April 10, 2018.
Kaukatcr: an experiment in language design for multi-dimensional spaces
One of the various projects associated with Project Xanadu™[1] was ZigZag™, a kind of organizer system or mind-mapping tool built around twisted multi-dimensional spaces called ZZStructures.
From the beginning, we wanted to make this system scriptable. Some existing internal implementations supported scripting in conventional languages, and Ted wanted a spreadsheet-formula-like language (since he thought of a ZZStructure as a kind of spreadsheet whose rows and columns were set free from their grid and tangled up in arbitrarily expressive ways).
When I was there, Jon Kopetz and I came up with the concept of a language that took more full advantage of the structures available, and I wrote a proof-of-concept implementation. It was not persued further — the language wasn’t really accessible to non-programmers the way a formula system might be, and we had other priorities — but I consider some of the ideas valuable, since, for all its limitations, it sits at the intersection of literate programming, stack programming, and visual programming.
Quick legal note
Project Xanadu produces a lot of code internally — a lot more than ever gets released — and a lot of internal documents and discussions. The code I’m linking to was written independently, from memory, based on things that I wrote for Xanadu. However, it is not officially blessed by the project, so the various trademarks (Project Xanadu™, ZigZag™, the flaming X logo, and probably others) don’t apply. As far as I am aware, this material does not violate any trade secrets — all trade-secrets related to ZigZag™ that I was privy to have been made public. Also, while patents related to this tech were filed, to my knowledge no applicable patents are currently in force.
From now on, I will refer to ZigZag™-like systems as ZZ. These ideas do not necessarily rely upon all of the features of ZigZag™, but may be applied to any system based on a ZZStructure. Also, I hate typing trademark slugs.
A crash course in ZZStructures and ZZ interface conventions
A ZZStructure is a collection of ‘cells’ — objects containing a value and a string-keyed associative array of pairs of pointers to other cells.[2]
A programmer might conceptualize a cell as the intersection of arbitrarily-many doubly-linked lists, each with a name. Alternately, they might see a cell as a point in a tangled multi-dimensional space (hence why the names given to these lists are called “dimensions”).
[1] A very twisted portion of a ZZStructure, with two visible dimensions, as seen in the gzz implementation of ZigZag
A full ZZStructure implementation can be written in 200 lines of python. For the sake of explanation, here is an even-more-simplified version:
cells=[]
class ZZCell: def __init__(self, value=None): global cells self.cid=len(cells) self.value=value self.connections={} self.connections[True]={} self.connections[False]={} cells.append(self) # core operations def getValue(self): """ get cell's value. Use this, rather than the value attribute, unless you specifically don't want to handle clones """ return self.cloneHead().value def getNext(self, dim, pos=True): if dim in self.connections[pos]: return self.connections[pos] return None def insert(self, dim, val, pos=True): """ like setNext, except it will repair exactly one connection """ if(dim in self.connections[pos]): temp=self.connections[pos][dim] self.setNext(dim, val, pos) val.setNext(temp, val, pos) else: self.setNext(dim, val, pos) def breakConnection(self, dim, pos=True): if self.getNext(dim, pos): temp=self.connections[pos][dim] temp.connections[not pos][dim]=None self.connections[pos][dim]=None def clone(self): """ create a clone """ c=ZZCell() self.rankHead("d.clone", True).setNext("d.clone", c) return c def cloneHead(self): return self.rankHead("d.clone") # underlying operations (usually not exposed through the UI) def setNext(self, dim, val, pos=True): self.connections[pos][dim]=val val.connections[not pos][dim]=self def rankHead(self, dim, pos=False): """ Get the head (or tail) of a rank """ curr=self n=curr.getNext(dim, pos) while(n): if(n==self): break # handle ringranks curr=n n=curr.getNext(dim, pos) return curr def getDims(self): return list(set(self.connections[True].keys() + self.connections[False].keys()))
Generally speaking, a conventional ZZ interface (a “view”) will consist of two panes, each with a particular cell selected and with two or three dimensions mapped to the spacial dimensions of the pane. Users navigate both panes simultaneously, or use each pane to view different aspects of the same cell (in the form of connections along different dimensions).
This article will be illustrated with hand-drawn images of a simplified ZZ interface of this type. ZZ keybindings and interactions are beyond the scope of this article, although I encourage those interested to read the official documentation in the ‘starter kit’ and watch the demonstration video.
A demonstration of the use of ZigZag for organic chemistryTed Nelson explains ZigZagAnother demonstration of ZigZag, this time at the Internet Archive
Kaukatcr
Kaukatcr (pronounced “cowcatcher”[3]) is a stack-based language modeled loosely on Forth. It avoids tokenization by treating cell boundaries as word boundaries. Like Forth, any word that is neither a built-in nor found in the dictionary of defined functions will be treated as data and pushed onto the stack.
Kaukatcr has an ‘interpreter head’ — a cell that corresponds to the beginning of the program. The stack hangs off the interpreter head along the dimension ‘d.stack’ ; the names of defined functions hang off it along ‘d.funcs’; the call stack hangs off it along ‘d.call’; the interpreter iterates along ‘d.exec’.
Function definitions hang off their names along ‘d.exec’. ‘d.branch’ points to the targets of conditional jumps.
By convention, we hang comments off of our code along ‘d.comment’.
For the sake of readability for non-Forth-programmers, a few Forth keywords have been renamed: ‘:’ becomes ‘def’ and ‘;;’ becomes ‘end’.[4] Function definition, nevertheless, works approximately the same way:
: 2dup swap dup rot dup ;;
becomes:
[1]
The ‘ret’ is required because we do not automatically return from calls when they exit — without the ‘ret’, the program will terminate when it walks off the end of a rank. This makes it a little easier to exit a program early, and so supports interactive editing — one can observe the stack during execution of an unfinished program, perhaps edit it, and then resume at an exit point.
Once the code defining that function has been read, our interpreterHead will look like this:
[1]
Finally, here’s an example with both functions and conditional branches:
[1]
Differences from Forth
Despite borrowing some syntax and conventions from Forth, there are fundamental differences (even beyond tokenization, which is not configurable in kaukatcr).
One difference is that builtins are not overridden by defined words with the same name. In the reference implementation, user defined function lookup is so much slower than builtins lookup that performing it on every step would be unreasonable for large programs. Furthermore, while redefining builtins is very powerful, it makes the most sense in a system that can easily have its state reset (to revert all possible breaking changes), while ZZ implementations theoretically should be image-based systems (like smalltalk VMs) where every change is automatically saved to persistent state. (Few-to-none of the actual implementations do this, but all of them are supposed to, according to design specs.)
Another difference is that ‘if’, being non-linear, doesn’t come with ‘else’ and ‘endif’ and friends — instead, it’s essentially a conditional branch. It operates equivalently to the ‘jnz’ instruction in x86 assembly. As a result, ‘if’ can be modified to act as an arbitrary ‘goto’ — just push 1 onto the stack before an if, and you can jump unconditionally to anywhere in the code.
In the current implementation, because we jump to the linked cell instead of the clonehead[5] of the linked cell, each cell can be the entrance point for at most one direct branch — however, I see no reason this couldn’t be an implementation-dependent or dialect-dependent decision. Limiting jump points in this way makes analyzing certain structures easier since they collapse into well-defined loops, but circumventing this in order to jump to a particular point from many places involves injecting a bunch of no-op instructions, which should be avoided.[6]
[1] [1] In this contrived example, we show how following clones to their origin (shown via dotted line) might simplify structure. Assume that we have already run the code “def nop ret end”
[1] A procedure for getting the item at a certain index in a 2d array. On the stack at call time: y-coordinate, x-coordinate, and the cell-id of the head of the matrix.
Footnotes
[1] Project Xanadu™ is the original hypertext project, formed in 1960 and still going. I worked there, in a volunteer capacity, from 2011 to 2016. It’s the brainchild of Theodor “Ted” Nelson. I recommend looking at the official website for more details.
[2] To be more accurate, a collection of cells is called a ‘slice’, and a ZZStructure may be composed of one or more slices. The module, as defined, only supports one slice (the ‘cells’ array at module scope). In other implementations, there is a slice class, which handles cell creation, garbage collection, serialization, and the allocation of cell IDs; however, I thought this might be too confusing for people unfamiliar with the concepts I’m trying to introduce. Anyhow, supporting multiple slices is mostly interesting because of the ability to link across slice boundaries — the mechanisms for which were at one time under trade secret. While I believe those trade secret protections are now void, I’m erring on the side of caution by avoiding detailed descriptions or implementations of them.
[3] The cowcatcher is the wedge-shaped construction found at the front of steam trains. It was invented by Charles Babbage, who also invented the stored-program mechanical computer. We chose this name because of this, and because the movement of the instruction pointer in two dimensions reminded us of a train following tracks. ‘Kaukatcr’ is the phonetic representation of ‘cowcatcher’ in the most popular lojban orthography.
[4] Some of my examples used ‘:’ by accident. So, I believe, did the internal prototype I initially developed. I have modified the implementation to support Forth-style word definitions. Writing and modifying this kind of code is very easy to do with a real zigzag interface, but quite awkward in GIMP!
[5] The ‘clonehead’ is the original from which some cell is cloned, if the cell in question is a clone. Mechanically, cloning works in ZZ by creating a rank of blank cells hanging off the original along d.clone — so, the clonehead is the head of the rank along d.clone. When we get the content/value of a cell, we automatically forward that content request to the clonehead, so from a UI perspective (and the perspective of anything that ‘reads’ the content of a cell) a clone contains the same content as the original.
[6] As you can see from the example, dereferencing clones makes certain kinds of code easier to write, but it has ramifications both for how function definition works (since currently it just clones the original function source) and for the regular use of the system. In particular, a user of a ZZ system might expect to be able to clone pieces of data from other programs or from non-code portions of his slice into his code in order to manipulate it, and dereferencing will break this behavior in potentially confusing ways.
By Rococo Modem Basilisk on April 12, 2018.
Do you listen to the Desert Oracle Radio podcast?
Do you listen to the Desert Oracle Radio podcast? I find it quite compelling, and I’ve never even been on that half of the continent.
By Rococo Modem Basilisk on April 13, 2018.
I always wonder a little bit why dependency-tracking systems that don’t strictly require ordering don’t perform pooling.
It’s one thing if we’re talking about a package manager that builds dependencies from source as they are needed, and therefore must run a particular set of commands in order to make other resources available to a future build. But, if all injected dependencies are available to all their children in the dependency tree, we’re essentially talking about flattening a walk through a graph into a list, right? In other words, if what Guice is doing is creating a list of strings corresponding to classpath elements, and that classpath is made available to every VM we spawn, then circular dependencies are a non-issue because we can check for cycles by searching the already-stored list of dependencies for each new one we find.
(It’s a different matter if we need to make it possible for different versions of a jar with the same canonical name to be used by different classes. I think that could be managed with some reflection-based name mangling.)
This is not a complaint about your solution, of course. This is more of a complaint directed at the authors of these systems, who don’t seem to have realized that they are not operating under the same memory restrictions as Dijkstra & are already storing the information necessary to identify & break cycles in what functionally isn’t really a dependency tree.
(Python imports have a similar kind of solution: a module enters a dictionary of loaded modules before its own imports are handled, and module loading is idempotent, so a circular import is a non-issue.)
By Rococo Modem Basilisk on April 13, 2018.
We may be talking about different things.
If we can expect that `HeartbeatActor.class` is defined to be the same thing anywhere in the dependency hierarchy, though, then the criticism still stands — we have, somewhere, a pool of associations between abstract types and the concrete types we prefer to use for them, and if that pool of associations is readable when we are constructing part of it, then we can avoid looping dependencies just by checking it.
However, if we’re talking about a situation where a constructor *must* be called, and that constructor *may* inject a dependency (whose constructor likewise *must* be called), it’s not so easily avoidable.
(In the python example, module loading can happen anywhere in code & even conditionally, so it’s a purely runtime concern. Simply making import idempotent by having it check the pool is sufficient to avoid dependency loops.)
By Rococo Modem Basilisk on April 16, 2018.
What people get wrong about Xanadu
What people get wrong about Xanadu
Links are separate from documents. They aren’t IN documents. The author of a document doesn’t have any greater authority over links than anybody else — they can create a bunch, and users can choose to apply them or not. The host that contains a document does not necessarily have any special association with the author, nor does it need to perform any coordination with or acknowledgement of links.
Instead, it’s closer to the ‘Rap Genius’ model — arbitrary users make arbitrary connections, annotations, rearrangements, and re-skins that are applied to the already-downloaded ‘original content’, and the user gets to choose which he or she would like to apply.
Hypertext is not about an author carefully controlling their brand by creating associations. It’s about a bunch of nerds sharing their own connections and footnotes and doing mashups & remixes of existing text.
You know how Thomas Jefferson took a knife to a bible & cut out all the supernatural stuff, then published his truncated version? Part of the point of Xanadu is that you could do that with the newest Hunger Games book, or literally combine the text with Interview with a Vampire, or whatever, and nobody could complain because the authors were getting paid for the passages that were kept from the originals.
It’s social footnoting. That’s the whole point. It’s for fandoms.
Even though I’m not a fan of copyright or micropayments in general, this is why I think they make sense in the Xanadu context.
Rifftrax could exist in a Xanadu type system in a way that encourages movie producers to get riffed, for instance.
People couldn’t use DMCA takedowns to shut down critique, because there’s no grounds, because all of the quotes are getting paid for & the original context is connected to commentary *automatically*. On top of that, any remixing or rearrangment is being done on the client — an argument can be made that derivative works aren’t being distributed so much as instructions on how to modify someone’s personal copy of a piece of media.
The web, by not really putting much thought into how to do data storage or addressing, and by foolishly embedding links into documents, essentially cemented a hierarchical system in which the rich get richer — because control over domain names and access to expensive hardware was directly connected to ability to serve and control data.
It’s stupid to address a document based on a machine, which is why email got rid of routes & fidonet never had them.
On the web, if the little guy’s information gets too popular, he’s punished — his site gets slashdotted and goes down. So, the little guy decides he doesn’t want to run his own web server anymore — now he pays an already-big company to handle it, at a much higher price. If he doesn’t, his information goes away forever (unless somebody else takes it and gives it to the big company first). This problem was created by sticking hostnames in addresses & sticking links in documents.
From the beginning, even in models like xu88 and xu92 where the network was centralized, which machine a document happened to reside on was never part of addressing on Xanadu.
Versions currently in development have lost that, using HTTP for file storage, though big public caches and leaning heavily on archive.org are encouraged. (And there’s no reason they *can’t* be patched to support IPFS. I had experimental IPFS support on one I worked on.)
But, at least links and formatting are totally separate from documents. And, at least, you’ve got first-order mechanisms for rearrangement (EDLs, which are just lists of transclusions — assembly instructions for frankensteining new documents from disparate sources.)
None of these things are *technically* difficult. A full Xanadu-style hypertext system is easier to write than just the HTML parsing component of a modern web browser alone.
Note: I don’t speak for Project Xanadu in any official capacity. This piece is reflective of my understanding of intended use, guiding philosophy, and version history based on about six years working on Xanadu and ten previous years researching it.
By Rococo Modem Basilisk on April 16, 2018.
Yup, this is exactly what I meant!
The b.bar() in your gist, since it’s at the top level, is essentially an import-time operation.
I would naively expect dependency injection in java to have no import-time operations, since it would be assigning some concrete class to fulfill some interface (i.e., deferring initialization until all such interfaces have their concrete equivalents determined *should* be possible, if the dependency injection is done using reflection instead of having the actual constructors request dependencies be fulfilled).
From your example, it doesn’t look like it’s done this way — it looks like you’re running some code to inject dependencies from the class that needs them, rather than having a static/class-level member that lists members, their abstract types, and an unbound variable of class Class for each, then having a separate dependency manager class handle injection before something is used (if necessary, called from the constructor of any class when those values aren’t filled in, filling them for the entire hierarchy).
I think, with clever use of partial deferral, the problem of cycles in runtime injection could probably be eliminated entirely (from a practical standpoint) just by ensuring that at no point does code that depends upon dependency injection run before the entire dependency hierarchy is fulfilled. (Of course, with a large existing codebase, converting all dependency injection to this form is tough — or even all dependency hierarchies beyond some problematic point! One might be motivated to make this change but not have the authority, as well.)
By Rococo Modem Basilisk on April 17, 2018.
I find subscribing to medium worthwhile if only because it gives me the equivalent of a subscription to those major (also-paywalled) news outlets. A subscription to each of them (or even, in some cases, only one) would be more expensive.
That said, paywalled posts aren’t generally of a higher quality than non-paywalled posts. The only real benefit of the paywalled posts is that, by TOS, they aren’t allowed to be about how to maximize your medium followers or shit like that (or, for that matter, to contain any ads).
By Rococo Modem Basilisk on April 20, 2018.
There was never an ‘open web’
There was never an ‘open web’
There was never an ‘open web’ because at a fundamental level the web enforced hierarchy via how URLs were defined to work. Instead, the ‘loss of the open web’ is an eternal september situation: most contributors to the content of the web were no longer people who had the time & money to buy their own domains & host off their own machines.
It’s still a problem, and the problem does not lay at the feet of the influx of new users. Everybody in 1990 should have been able to see that by 2000 lots of people would be online and the people who were up for buying a domain name would be the same ones who had one in 1990.
Instead of making it easy for people to host things on their own machines & optimizing for inconsistent connections in the dialup era, we took the easy way out & relied on ISPs to host user content. That seemed OK because we were doing the same thing with usenet back then. We never really fixed this as the dailup era ended.
Of course, we’re addressing this a bit now with IPFS and DAT and similar things. But, it might be too late. Already, hosting content is “something that nerds do” and “something that big companies do”, never “something that I can do” from the perspective of non-technical users.
Everybody has a browser cache & so it’s pretty stupid that we don’t have cache sharing along bittorrent or something built into every major browser and turned on by default. Like, we could have done that in 1995 (not with bittorrent but just with full-file hashing or something, among users with the same ISP, with the ISP coordinating). Federated peer-to-peer cache contents sharing is a lot easier to gradually transform into a hostless fully distributed storage thing. Instead, when ATT WorldNet stopped hosting our home pages we turned to geocities, and when geocities stopped we went to MySpace.
By Rococo Modem Basilisk on April 24, 2018.
The uncanny practicality of impracticality
I think one of the big cultural disconnects between me & current mainstream dev is the idea that it’s a burden to know something.
It’s not totally alien: it’s a burden to need to know something, and it’s a burden to need to know it on a deadline. So, I understand wanting a minimum understanding when it comes to doing professional work. But, I have never considered professional work to be the most meaningful part of software development.
I got into this field because I consider it a joy to learn about the ugly twisty obscure corners of the domain. So, the “real work” is the stuff I do in my free time out of interest, while the stuff I do on a deadline is just leveraging my existing skills to make a couple bucks. I’m up for learning anything, as long as it happens off company time and happens because I feel like learning it.
Knowledge’s association with effectiveness is complicated. I suspect that you literally cannot benefit fully from knowledge-gathering if your goal is to benefit from knowledge-gathering.
The most valuable bits of knowledge, in terms of productive work, are the ones that nobody could have reasonably expected to apply, because anything that can be expected to apply will be learned by everybody and become common sense. We have an intellectual monoculture in tech driven by capitalism.
If you learn whatever seems marginally interesting, each individual thing is unlikely to be relevant but the number of things means that the likelihood that one of them will be a real breakthrough is pretty high.
This is sort of a tangential response to the recent Signal v. Noise post called “Conceptual compression means developers no longer need to know SQL”.
My take is basically: it’s great to not need to know SQL, pointers, garbage collection, and whatever else, but if someone doesn’t want to know those things in the abstract, it’s a problem. The benefits of knowing those things goes far beyond the circumstances wherein knowing them is strictly necessary.
To say “C means developers no longer need to learn assembly” rather than “C means developers no longer need to write assembly” is upsetting — it considers knowledge by itself burdensome, rather than the work it takes to apply that knowledge and the awkwardness involved in that application. From my perspective, learning is only something to minimize when your time is someone else’s, and even then, only when they don’t value learning appropriately.
This is the main reason I don’t trust developers who don’t write code in their free time — it’s a good proxy for determining whether or not somebody is curious enough to have learned anything beyond what was required when they first started, because it’s very easy in this industry to stop learning long enough to become completely useless without HR noticing. However, it applies to any kind of knowledge work: someone who isn’t intellectually curious can easily stagnate, as whatever knowledge they do have becomes devalued either through sheer redundancy or through a sudden and complete loss of relevance. If you stand upon blocks of knowledge then it’s easy for your tower to fall as small bits of it lose relevance, but if you float along a stipple of knowledge cross the whole conceptual landscape, any piece losing relevance is pretty likely to be accompanied with another piece gaining relevance.
This is not strictly a defense of intrinsic motivation (or a defense strictly of intrinsic motivation, or a defense of strictly intrinsic motivation). Feeling effective or accomplished is another form of intrinsic motivation. I think the key is curiosity and generalism.
I also think that an important component of generalism is not having a strong emotional connection to your sense of identity. It makes us dismiss interesting things on the (often absolutely accurate) grounds that they’re terrible — which is dumb, because terrible things are great. Terrible things teach us lessons in entertaining ways, and when they do something right, it’s surprising enough to be memorable. In a mostly-open possibility space, knowing one thing you shouldn’t do is more valuable than knowing one thing it’s desirable to do — we have made many novel varieties of turing tarpits but we haven’t even gotten close to figuring out the ideal programming language, for instance, so knowing not to make PHP is valuable.
This is also not a defense of poor documentation strategies or purity tests. Making things easier to learn helps everybody. Specifically, optimizing documentation for usage makes it easier for people to learn by doing, whether or not they want to continue learning beyond that material. A lot of projects (think java-style object orientation), systems (think Mac OS), and documentation sets (think the ElasticSearch documentation or any other form of purely-tutorial-oriented documentation) are structured so that certain ideas are easy to learn and everything else is hidden, because the distinction between “doesn’t need to” and “can’t” is lost — so we end up with systems where use cases not considered by the original authors are on the spectrum from hacky to literally impossible, because a particular attitude has been imposed upon all users.
I would also be wary of the idea that incidental complexity is inherently burdensome while only essential complexity is interesting. Knowing about incidental complexity makes things that are made difficult by incidental complexity easy, and makes things that are made very difficult by incidental complexity possible.
We live in a world where the apparent complexity of almost all tasks is absolutely dominated by incidental complexity, and removing that incidental complexity requires understanding it.
I find many forms of incidental complexity fascinating, and understanding it is just as useful.
There’s also the problem that what constitutes essential vs incidental complexity is a matter of the problem statement & domain.
If I’m told to do something in three hours, then the essential and incidental elements are inverted from what they would be if I was given three months for the problem, because the time constraint means that a naive solution based on a shallow understanding is the only possibility.
If I’m told to do it with a particular tool, I can’t ignore the tool’s sharp edges just because another tool could make the problem easy to solve. What is easy in TCL is difficult in Prolog and vice versa, and these constraints are not platonically incidental or essential — all that matters is whether or not the choice to do something with the appropriate tools is open. Today, in software development, tool choice is rarely fully open and the selection of acceptable tooling rarely includes anything appropriate for solving unusual problems. This largely comes down to the expectation that all tools should already be familiar to all developers — in other words, being able to treat employees as disposable is more valuable than having efficient solutions and giving employees a little time to understand them. So, understanding incidental complexity is extremely important, and the more of it you learn, the easier a time you’ll have.
Every piece of essential complexity is surrounded by a thick shell of incidental complexity. That shell is nigh impenetrable without a grasp of whatever tools. However, as a generalist, you will probably find that the shell is hollow: essential complexity comes in a handful of forms and while the names change the same basic structures play different roles across different domains. Things that seem distinct only do so because of their context — nobody had noticed that two fundamental ideas are really exactly the same thing because nobody has gotten to the same level of understanding in those two specific fields. At this point, the generalist looks like a genius — but of course, all that has happened is that he has saved time and effort by being sufficiently eclectic. His aimless wandering has prepared him to encounter familiar ideas in alien costumes, and his impressive ability to identify new ways of manipulating some piece of essential complexity is really just rudimentary memory of how the same structure was manipulated in a seemingly unrelated field.
The value of this kind of cross-disciplinary connection is in its unpredictability, and so the only way to guarantee it is to avoid having exactly the same set of interests as anybody else. The easiest way to go about doing that is to pursue whatever sticks out to you about whatever domain you find yourself in, without concern for relevance, appropriateness, or order.
By Rococo Modem Basilisk on April 25, 2018.
Yeah, I think that if a hiring manager or human resources person is grading people on whether or not somebody codes in their free time, that’s extremely problematic. (It also ruins the heuristic when this is done, because people start to think that doing side-projects in their free time is a career builder.)
As a regular person who writes code, when I decide how seriously to take somebody’s ideas about a technical domain I don’t know much about, determining whether or not the person is sufficiently curious is very important. Having whatever proxies are used to determine this never be directly rewarded by businesses is therefore extremely important. Whatever behavior is rewarded by businesses in an over-compensated field like ours becomes almost immediately worthless for both the business and everything else.
(If doing side-projects in order to attract HR attention becomes widespread enough that you can’t identify the career-building intent from the number of buzzwords in the description, I’ll need to determine how seriously to take someone based on how many side projects they have that would actually lower their likelihood of getting hired.)
By Rococo Modem Basilisk on April 26, 2018.
The problem with Electron
The problem with Electron
The problem with Electron is not Javascript.
Javascript is just a basically-ordinary scripting language, and isn’t inordinately terrible (or even Perl-levels of terrible). It’s not great, but it’s OK, and that’s good enough for lots of people.
The problem with Electron is that browsers are terrible at literally everything.
Treating the browser as the logical sandboxing tech for javascript & treating DOM manipulation as the logical GUI toolkit for it dooms JS.
Essentially, the issue comes down to the web stack being a poor fit for general-purpose GUI tasks.
I maintain that browsers shouldn’t really be used for anything other than displaying HTML 1.0. No CSS, no javascript. Get rid of them. Browsers aren’t even great at displaying HTML 1.0, and HTML 1.0 isn’t even great at being HTML 1.0, but of all the things browsers do, it’s the only thing that’s marginally acceptable.
Cascading styles are potentially interesting. They might even be useful, if we applied the concept to something that wasn’t embedded markup. CSS is like Make — an ad-hoc implementation of three quarters of a broken Prolog-style constraint solver. Considering the kinds of amazing feats people are able to achieve with CSS, imagine what they could do if a CSS-like system was designed to be usable.
CSS has a high initial learning curve because of a large number of rules that lack consistent patterns & thus must be individually memorized. Every piece of web tech has that problem (and plenty of non-web-tech like C++ and Java), but it’s still something that’s absolutely unnecessary & rightly criticized. It comes out of standards being written by competing implementations trying to screw each other while maintaining maximum compat with what they already wrote.
There is no technical reason why CSS couldn’t have been written in such a way that the time it takes to get to current professional-level competence from zero experience is two hours. (This is a lie. There is exactly one technical reason: embedded markup makes everything difficult. But, CSS could have been made much easier to learn and much easier to use, had it not grown primarily organically.)
I don’t think these criticisms are particularly radical. I get a lot of push-back every time I make them, though. We’ve got a couple different bones of contention here:
1) how easy is “easy enough” 2) what is the appropriate domain for this tech
If you’re a professional in some area and you don’t spend much time teaching absolute novices, it’s easy to forget how steep the learning curve is for things. If you’re a professional with one stack and barely touch other stacks, you can mistake incidental complexity for essential complexity.
With regard to the first problem, I maintain that the learning curve could be made nearly flat, on the grounds that Tk’s learning curve is nearly flat compared to CSS and most other GUI toolkits have similarly nearly-flat learning curves for complex style and layout. However, CSS is limited in that it acts on the DOM, and the DOM is awkward and overly complex in ways that other representations of rich text documents aren’t. Thus, if we do not determine that web stacks are inappropriate for general GUI use, we are stuck with compatibility concerns that force every layer in stack to be more difficult to use than it would naturally be.
Of course, if you ditch the web stack, you can certainly still use Javascript. There’s literally no reason you can’t write code in JavaScript that uses Tk or GTK or QT or XCB or AWT or whatever the hell you want as a GUI toolkit. WINGs, even. At this point, you’re writing a desktop application. Why bother with trying to retain compatibility with something gross?
It’s not really a matter of platform-specific vs web — this is a popular false dichotomy. There are a lot of cross-platform GUI toolkits, with neither the learning curve nor the performance overhead of DOM manipulation.
I can understand why someone who made a web service might decide to repackage it as an Electron app as a placeholder for an eventual real native app. I don’t really understand why somebody would write something from scratch that way, other than basically unfounded fear that the learning curve for non-web stacks is as steep as the one for web stacks (something basically not found outside of esolangs).
It’s easier to write a simple GUI app with GTK when you don’t know GTK than it is to write the same thing as a web app when you’re already a pro at writing web apps. This difference in the difficulty curve becomes more extreme the further you get from simple forms (which web tech can do without significantly more trouble than other platforms) and closer to more abstract interactive spaces (which are trivial on most platforms but an exercise in futility on web stacks).
By Rococo Modem Basilisk on May 1, 2018.
The language around free speech in civil-libertarian circles is flawed
The language around free speech in civil-libertarian circles is flawed
There’s something that bothers me about the way we talk about speech. By ‘we’, I mean not just self-defined anarchists but the broader scope of civil-libertarians, from the EFF to the ACLU.
There are terms like “chilling effects” that, in denotation, are really well-defined, but in connotation are almost dog whistles. We cause a chilling effect when we create an environment where saying certain things has a social cost, and this is basically always framed as bad. But, it’s also a useful tool — when we create a code of conduct, set ground rules for our own spaces, encourage etiquette, or give someone a dirty look when they behave boorishly, we are using chilling effects to discourage certain kinds of mostly-communicative behaviors.
If you’re using soft power to shape the way people communicate in order to make a society work better, that’s an improvement over using hard power to police speech, from the perspective of anybody who might violate it in good faith, right? Having soft-power-oriented structures in place lets people choose to violate them when they feel like they must, & lets them change if it’s decided they’re counterproductive. It ultimately has the flexibility that hard power lacks: the upper limit on punishment is solitude, and each person chooses the extent to which they agree with enforcement.
So, the bigger deal is identifying soft power and understanding it *as* power. And, we do that pretty well in many cases (though newbie anarchists often don’t).
The freeze peach crowd does it well too, but can’t distinguish soft power used toward positive ends from soft power used toward negative ends (or sometimes can’t distinguish soft power from hard power). It makes sense: after all, when it’s used in the context of an existing hierarchical dynamic based on hard power, and acts in conjunction with the threat of heavily imbalanced hard power, it acts as a threat toward those who go against the system. Nobody wants to get a chilling effect produced by the FBI or FSB, because ignoring it might mean a death sentence, and organizations who are used to looking at environments where a few powerful actors dominate media (such as, well, every environment prior to around 2005) will get strange results if they apply their normal tools of analysis to equitable groups with power ratios hovering around 1.
If a group decides, collectively, to write a code of conduct that discourages certain kinds of speech, this is fundamentally different from a government writing a law that forbids certain kinds of speech. For one, the consequences are not comparable: the most the group can do is refuse to let you in (and for every constraint in some group’s code of conduct, you can find another group without that constraint or with that constraint reversed), while a government can imprison you or kill you. For another, forgiveness is collective in the case of the group: if as little as a third of the members of an equitable group decide that what you did was OK, the letter of the law ceases to matter, because you can avoid coming into contact with the rest; in the case of a government, forgiveness has a single point of failure & a structure designed to punish anyone who acts upon forgiveness at lower levels. For instance, Edward Snowden could not return to the United States even if nearly everyone decided he was in the right, unless he got a pardon from the president; if he tried, anybody who helped him would be liable to be imprisoned.
Getting rid of the state (or even tuning its presence in our lives down to a dull roar) means needing to understand how to wield soft power to cultivate the health of your community, and knowing if and when hard power becomes necessary. A big part of that is going to be getting rid of the idea that soft power is inherently cowardly or dishonest.
I don’t think soft power is even particularly subtle — and I’m autistic, for fuck’s sake. It becomes obvious if you look for it.
When it comes down to it, soft power means human flexibility over rules-lawyering, and that means that bans are replaced with chilling effects. That’s what progress toward humane consensus-based self-governance looks like.
By Rococo Modem Basilisk on May 2, 2018.
Ultimately, when you’re in a business context, you need to please the people above you in the hierarchy. That often means making technical decisions you know are bad for historical or political reasons. For instance, if everybody working under you only knows about the web stack, you’re going to need to convince a non-technical manager at some point that broadening their horizons & shifting focus is justifiable in terms of short term return on investment (a hard sell) or you’re going to need to put up with it. We don’t always have the resources necessary to stand firm on this stuff, and we have to pick our battles.
I’d like it if everybody who works on Electron against their will (or uses stuff like XML or Java against their will, or is pushed into using Hadoop to do what could be done faster on a single machine in a single thread) stood up and said “I’d rather be fired than make our customers and coworkers pay for this obvious mistake”, but the truth is a lot of them would get fired if they did that (or at least get shortlisted for layoffs or get a couple black marks on their record) and while software engineers are more capable of absorbing that risk than many other occupations, I understand why people would be nervous.
(Software engineers aren’t really unionized, and even if we were, unions rarely strike over treatment of customers.)
By Rococo Modem Basilisk on May 2, 2018.
1850, but faster
1850, but faster
With apologies to Charlie Stross
The singularity already happened. It happened before any of us were born.
We are not the dominant life form on earth. The dominant life form is a giant, of toddler-level intelligence, toddler-level emotional intelligence, and superhuman powers. These giants can literally move mountains, but they move slowly. Their component parts are people. We donate to them a third of our lives in exchange for room and board. Those who refuse to sacrifice their lives to the giants die.
The giants come in two breeds. One, the limited corporation, is a race of cannibal warriors: they stumble around the landscape, fighting each other, and their existence is justified on the grounds of their incredible troop supply chains. The other, the herbivorous non-profit/governmental organization, has a symbiotic relationship with the limited corporation, acting to maintain their health by redistributing supplies between them, because they are incapable of cooperating outside of the context of pack hunting.
This structure is an old one, but not an ancient one. There have been a handful of changes over the past century and a half (like the end of an already-dying race of giants called empire around 1914, whose niche the others have already begun to fill), but for the most part, 2018 looks like 1850 but faster: the key players are the same kinds of creatures, and progress primarily comes in the form of these giants moving faster and requiring fewer resources.
The worst case scenario, with regard to superhuman AI, is not that it will change the structure of the world but that it will merely accelerate the exact trends we have seen from 1850 to today — in other words, the giants will continue to have owners but cease to have human component parts, but they will continue to rule the earth. Even without superhuman AI, this remains the trajectory — we would get there eventually, if we didn’t form our own giants with fundamentally different programming.
By Rococo Modem Basilisk on May 3, 2018.
I’m being a little more prescriptive than descriptive, & that’s probably why my categorization is simplistic. But, it’s not totally accurate that I’ve broken the situation into only two categories. I see four:
1. technical (i.e., electron is actually the best tool for the job) 2. tactical (i.e., we have an existing web app & we need to ship a native version on monday) 3. political (i.e., the boss likes electron) 4. information-flow (i.e., half our team is only familiar with web stack, or nobody on our team knows that there’s a better tool for the job)
I’ve made some attempts to argue that #1 is quite rare. I’m not convinced that a situation where electron is really the best technical fit exists, though I’d love to be proved wrong on this — it would bring it, in my mind, from the category of technologies that are equally bad at everything to the much more interesting category of technologies that can do everything but are much better at one particular domain than all others. In practice, all four tend to interact to some extent, so you get qualified successes like “this is the best tech (#1) that our team is comfortable with (#2 & #4), aside from this other thing that corporate won’t let us use (#3)”.
Small-computing situations (like personal projects) are a different beast entirely. In small computing, concerns like curiosity, masochism, competitiveness, playfulness, perversity, and showing off can easily individually override technical and tactical concerns, and in fact many technical and tactical concerns can be ignored or inverted. But, once you have customers, the kinds of intrinsic motivators that drive personal projects should never override the responsibility to the customer. In practice they do all the time, because we don’t do a very good job of separating hobby programming habits done in our free time from professional habits that make sense when there are higher stakes.
By Rococo Modem Basilisk on May 3, 2018.
How to contribute to open source without stress
How to contribute to open source without stress
Step 1: write some code
Step 2: throw an OSI-approved license on your code
Step 3: put it online
Step 4: ignore patches and pull requests, or reply with “I am no longer actively maintaining this project, but feel free to fork it”
Step 5: there is no step 5
By Rococo Modem Basilisk on May 4, 2018.
The WTF Awards / sketching a map to an escape from steam engine time
The WTF Awards / sketching a map to an escape from steam engine time
The myopic utility- and profit-oriented nature of markets funnels us into a world where monoculture & path dependence turns technological development into endless deja-vu. In my previous article on this subject, I recommended counteracting these forces through a proliferation of small-scale independent contests designed to reward absence of apparent utility. In this one, I would like to present a sketch or prototype of what such a contest’s ruleset would look like.
This kind of contest should be made small-scale and as inexpensive as possible to run: I would like to see a federation of similar contests occurring simultaneously, run by different universities and other organizations.
If you are in the position of creating such a contest, feel free to crib & modify these rules.
The XXth Annual X University WTF Awards
Abstract Short-term thinking locks us into decisions before we have the opportunity to determine the best path forward. On the other hand, the results of undirected exploration can become useful after they have already matured. To this end, we would like to encourage undirected exploration through a celebration of the pointless, unclassifiable, and inexplicable: those things that push the boundaries we can’t see yet.
Rules 1. Applications begin April 1st, at the beginning of the awards ceremony for the previous year, and end at midnight of that night. Your application may not contain any information about your planned submission. If we recieve more applications than we can reasonably expect to judge, we will choose accepted applications by lottery.
2. Work done is to be presented on April 1st of the following year. Anyone who presents their work after one year is also eligable to present at the ceremonies three, five, and seven years later, so long as they have done new work on their project in the meantime.
3. Judges should be expected to grade on the criteria of: apparent uselessness, novelty with respect to previous work, novelty with respect to other entries, cross-disciplinarity, technical difficulty, classification difficulty, new ground covered, and (if this is a 3, 5, or 7 year project) dissimilarity to earlier versions of this project.
4. Those projects being presented must provide documentation of build processes, early research, rationales behind decisions made, and any discoveries made during the course of the project. This should be complete enough that a third party, following this documentation, could reproduce the project in its presented state. This document will be released into the public domain and stored in a public archive associated with the contest.
5. The winner of first prize in the X University WTF awards recieves $500, gives the closing speech for the ceremony, and their project is banned from being re-entered (although they may take their entry to other WTF Awards with whom we are federated, after 3 years). The winner of the second place prize gets $250, and the winner of the third place prize gets $200.
By Rococo Modem Basilisk on May 4, 2018.
Don’t say “technology”
“Technology” is one of those words where your bullshit detectors should perk up whenever you hear it.
It’s so broad but is often used to talk about something really specific (mobile internet, or social media, or W3C standards, or a particular small group of struggling small businesses in southern california, or taxi services, or labor law evasion), so it’s used to apply generalizations to large groups of things that have nothing in common except accidents of history.
I have never heard the word “technology” used in a meaningful way outside of social anthropology.
So, if somebody says something about “technology” that isn’t applicable to hand-axes, ask yourself: what did that person actually intend to talk about, and what unrelated things could easily be confused for it that the statement would be false for?
(For instance, “technology kills social skills” is probably intended to mean social media but not labor law evasion or tax services.)
Very often, when somebody uses a broad word like that unthinkingly, what they’re saying isn’t actually meaningfully true in any possible specific reading! They said it that way because they haven’t actually thought clearly about it.
(For instance, “technology kills social skills” can only be interpreted as true in very limited way: i.e., different means of communication produce different norms of behavior, and behavioral norms irrelevant to someone’s life are forgotten or never learned.)
By Rococo Modem Basilisk on May 11, 2018.
Honest AWS ad copy
Honest AWS ad copy
“HEY, KIDS! Why do real work when you can spend 90% of every work day struggling with authentication systems nobody understands, complete with permissions you need to do your job but that nobody can tell you who to ask for! Now, you can, with Amazon Web Services!
That’s right: Amazon Web Services, your one stop shop for filling your life with pointless and confusing red tape!
Usernames and passwords are so 1990s! Instead, log in with your user name and password, and then we’ll prompt you for an eighty digit number, without which you can’t do anything at all!
What’s a role, you ask? It’s like a set of permissions, except it’s an eighty digit number that you need to memorize, once someone with a different role has given you a role that lets you access that role!
Tired of being able to see the machines you’re using listed in a consistent way, regardless of geography? Check out our new feature: randomly resetting your location from ‘Oregon’ to ‘Ohio’ and setting the font size so that only the ‘O’ is visible in the menu, so that you will be perpetually confused as to why your resources aren’t visible!
What, you have no machines in Ohio? No problem! We don’t either!
We’ve heard your feedback about how useful auto-scaling can be, and that’s why we’ve made it so whenever you change an auto-scaling related setting, we terminate all of the affected nodes, losing your data permanently!
That’s right: decrease max scale, or increase it, or change the minimum scale to anything at all, and we will immediately terminate all those machines, so you can wait half an hour for the load balancer to notice and bring new ones up! Magical!
Tired of your machines being able to talk to each other? Try our wonderful new feature: VPC-linked security groups! Now, even if you’re on a role that can see all the machines you need to use, they are nearly guaranteed to have each other blocked! And, as a bonus: modifying a security group or copying a security group from one VPC to another requires a role that nobody in your company has!
Installing applications is lame, right? That’s why we invented AMIs. That’s right, folks: AMIs are entire Linux installs that exist to run exactly one application. Just spend eight hours struggling with our out of date and extremely crippled snapshot of Red Hat’s repositories to install the dependencies for your application, create a snapshot, do it all over again because it’s on the wrong VPC, do it again because it’s on the wrong role, and watch as our molasses-slow console fails to bring up a machine running it and gives no hints as to why you can’t ssh in! (Hint: you’re probably on the wrong security group or VPC!)
And the best part is: we charge by the hour, and WE ROUND UP.”
By Rococo Modem Basilisk on May 11, 2018.
Small computing, artisanal computing
Programming reproduces a worldview, and the problem is that the worldview reproduced in most cases is too much of a monoculture, with barriers to representation in the form of implicit and explicit hurdles & gatekeeping. (This is part of the reason I think “small computing” is so important: users should have enough control to make the code they interact with representative of THEIR umwelt.)
The absolute best case for coding, on an ethical level, is that you have reproduced your worldview and you’re the only one who uses the software you wrote — while someone else can still take it and adapt it. Making it so everybody feels comfortable replacing the system’s perspective with their own is a UX concern (maximizing expressivity at every part of the learning curve, and smoothing the learning curve out)
The way I see it, UI standardization is a matter of supporting corporate capitalism, and doesn’t make sense outside that structure. This might mean that, so long as people have jobs, we can still have this distinction between a standardized “work machine” that multiple people use and an idiosyncratic and personal “home machine”.
A computer is both an expression of our worldview (i.e., an art-work) and an extension of our mental space (like a journal, mood-board, scrapbook, sketchpad, or face). When it works, it’s an auxiliary brain lobe with a slow connection. The degree to which it functions as an extension of our mental space is dependent on the degree to which it’s representative of our umwelt. Because, when our computer isn’t good as art (i.e., doesn’t actually map to how we think about things in a natural sub-cognitive way), we have to struggle with translating things so the computer understands this. Essentially every piece of standardized or commercial software is awkward to use until the user changes their mental space to match it, because it’s not an expression of the user’s worldview but of some designer or programmer’s.
By Rococo Modem Basilisk on May 11, 2018.
Perhaps a good model for a “left-wing game”, if we take seriously the idea that right/left division is a matter of the assumption of a harsh vs safe world (i.e., the first circuit in the 8 circuit model or the openness axis in OCEAN), might be something like Princess Maker 2, wherein the challenge is not in fighting enemies, winning against competitors, or navigating a treacherous terrain, but trying to help this figure (the titular princess) flower into a fully functioning adult by suggesting routines of self-development.
By Rococo Modem Basilisk on May 15, 2018.
I have my own version, in pure shell, using /usr/share/dict: https://github.com /enkiv2/xkcd-passwords
By Rococo Modem Basilisk on May 22, 2018.
> I took some time to look through what you said and checked around the internet.
> I took some time to look through what you said and checked around the internet. Interestingly, I found nothing that proves wrong the information I conveyed.
Look harder?
> Are you familiar with the difference with the words ‘tell’ and ‘make’ or ‘create’?
Indeed. Ontology-based joke generators are more complex than something that simply reposts existing jokes. They are still quite straightforward.
For instance, you can write a pun generator using an ontology like NELL’s by identifying a word with two senses, then producing a statement that begins by implying one sense but ends by verifying the other. There’s an implementation of this algorithm in lisp from 1995 floating around — should be within the first ten pages of google results for “computational humor” since it’s gotten frontpage on slashdot a few times. That kind of joke isn’t usually very funny, but it’s quite unambiguously created by the system.
> you should find out about its current state
I’m part of the generative fiction community. I write these sorts of things on a regular basis.
There are tutorials. If you have any interest in the subject, I recommend reading them.
By Rococo Modem Basilisk on May 26, 2018.
Joke generating AI has been an area of research since at least the 1970s — so, around the same time as some of the first planner-and-ontology-based narrative generation systems. There are plenty of academic implementations floating around, for people willing to do a few minutes of research, along with papers on the subject by the likes of Marvin Minsky. I’m also aware of a few dozen non-academic joke generators active on twitter. It’s not terribly difficult to implement one.
The difficult task is not creativity: a random number generator is more creative, strictly speaking, than any human can ever be, and we have lots of enormous ontologies to use as source material. The difficult task for these machines is to figure out how to avoid telling jokes that are too obvious or too obscure for their audience. (And none of these implementations, as far as I know, try to tackle this problem — they tell all the jokes, and then people latch on to the ones they like. This puts them in the same category as professional comedians.)
Before saying that making a joke-generating AI is impossible, I recommend spending ten minutes writing one. You will be successful.
By Rococo Modem Basilisk on May 26, 2018.
Trendism & cognitive stagnation
(This is a follow-up to Against Trendism)
Basing visibility on popularity is a uniquely awful version of ‘tyrrany of the majority’ because uncommon views become invisible, even if, were they to start on an even playing field, they would become popular.
In this way, it encourages mental stasis: since ranking is based on an immediate appraisal of how popular something already is, and visibility is based therefore on past shallow popularity, there’s no room for rumination.
This is NOT an attribute of ‘technology’ or ‘social media’, but an attribute of visibility systems based on immediate ranking. Visibility systems based on ranking delayed by, say, three days, or with the top 25% most popular posts elided, would be fine.
Our capacity to imagine new possibilities is based largely on our familiarity with the bounds of possibility space — we can only imagine views that are in the neighborhood of views we’ve heard expressed in the past. So, making the already-unpopular invisible limits imagination.
(There are hacks we can use to make it possible to imagine views nobody has ever held. We can make random juxtapositions, impose meaning on them, and then figure out a justification for them — like tarot reading. Or, we can merely iterate from some basic idea, getting more and more extreme, while internalizing the perspective of each iteration as something someone could possibly believe in good faith. The former — the bibliomancy approach — is common in experimental art, while the latter is typical of dystopian science fiction.
But, these hacks are pretty limited. We need a starting place. If we’ve only heard mainstream ideas, we’re going to have a hard time going off the beaten path with the dystopia approach, while we will struggle with the bibliomancy approach because most ideas can only be made to seem reasonable with the help of other ideas. Getting into uncharted territories with either of these approaches is difficult unless you’ve already filled out the middle of your possibility space with other ideas, because in their absence you would need to independently reinvent them.)
This is not a justification, in of itself, for banning metrics entirely. After all, this kind of exponential distribution happens with ideas even without the use of popularity signifiers: ideas spread, and popular ideas have more opportunities to spread. Trendism merely accelerates the process and widens the gap between the most popular ideas and everything else.
Sites like reddit use segmentation to prevent total ordering of popularity from dominating, although this ultimately means that popular subreddits have a disproportionate impact on this total ordering when it is seen. Similarly, we have seen piecemeal attempts to limit the effects of trendism for particular topics — the curation of trending topics at twitter and facebook, for instance, or ad-hoc ranking demerits for particular tags on lobste.rs.
However, we could be applying the measurements we already take to counteract trendism rather than accelerating it: making popularity count less the higher it gets, removing overly-popular content entirely, boosting the visibility of mostly-unseen content, using information about organic reach in sites like twitter to boost the synthetic reach of people who don’t have many followers (instead of boosting the synthetic reach of the rich), systematically demoting posts that comment on trending topics, spotlighting spotify tracks and youtube videos with zero views, and so on.
Where trendism devalues the function of recommendation systems as novelty aggregators, these tools could be modified to be anti-trendist, pro-novelty, and promote a cosmopolitanism that broadens our horizons in ways traditional word-of-mouth never could. This is a unique capacity of recommendation systems over curators: recommendation systems can recommend things nobody has ever seen, and can recommend them on the grounds that nobody has seen them.
By Rococo Modem Basilisk on May 26, 2018.
We should kick this off with a bang, by writing RP fanfiction in the form of a listicle of rhyming couplets, about Gutbloom’s medical conditions & foot preferences, as told from the perspective of his cat. Any takers?
By Rococo Modem Basilisk on May 27, 2018.
Software vs Capital
Software vs Capital
I have periodically said “software has nothing to do with business” as a criticism of tech journalism that focuses on firms and corporate politics over tech itself, or that considers all software to be products or potential products, or that considers success to be defined primarily in terms of money. I get a lot of pushback, including from intelligent tech journalists who I feel should understand my criticisms, so I’d like to expand on my quip.
When I say ‘luckily software has nothing to do with business’, I do not mean that business is irrelevant to software in practice. What I mean is that software is uniquely positioned, compared to all other engineering, to be free of economies of scale: it is cheap to develop and has near zero cost of reproduction. Business has the capacity to be irrelevant to software without any radical changes to how business is run or software is developed in practice.
In other words, software development can be done outside a capitalist context to a much greater extent than material forms of tinkering. Coding is not an expensive hobby: only the free-time component applies. Even a fairly weak form of UBI could give a big boost to free software, because most available software (still) is not developed by businesses or for business purposes but as a hobby by hobbyists who expect nothing in exchange.
I am not claiming that software engineering for the money is somehow illegitimate. I consider the entire ‘work to live’ arrangement to be illegitimate. Software engineering, like writing, benefits more from the abolition of that arrangement because of the combination of low resource costs & a lot of people willing to do it for fun.
The purchase of a physical computer for the sake of developing software is much like the purchase of a typewriter for the purpose of writing a novel: it is a large one-time cost, after which only small incidental costs are incurred (paper and ink, or power and internet), and it is a cost that most people have already paid. You can use the same computer for a decade or two. Other technical hobbies (like knitting or electronics) have much larger ongoing material costs — and even near-ubiquitous situations like car ownership are much more onerous in this respect.
Amusingly, the very attributes that make it possible for something to escape from capitalist domination also make it particularly desirable to Capital. The elevation of sign or Spectacle over reality is like Capital engaging its death drive, since value is wholly imaginary.
The logic of markets depends upon the (strictly incorrect but historically mostly good-enough) notion that there’s some natural value to things, inherent to them and stable, which can be determined by looking at what people are willing to pay. When use value is the primary factor, sure. When labor value is the primary factor, sure. When the primary factor is speculative (what other people are predicted to be willing to pay), cost becomes totally unstable. This instability is mostly unpredictable. Of course, anybody who bets on instability and wins is going to attribute that to skill or insight, and since losing money is harder than making it once you’re rich, they can remain unchallenged in this attribution. But, there is no major insight factor — merely survivorship bias. The winners win bigger and the losers lose more as we become untethered from the world and get stuck in never-never land, where the tinkerbell effect is the only law.
In other words, speculation is the end of those elements of markets that have ever been generally beneficial, but since they are overconfidence-generating machines, Capital is pulled into their gravity. I think we’re past the event horizon here already. IP is a lot like speculation: the value of social constructions is usually dominated by social constructions, as production & marginal costs approach zero.
“Software is eating the world” is maybe better phrased as “The Spectacle is eating the End of History”.
Why hasn’t the essential spookiness of software eaten capital yet? I think a big part of this is an accident of history: the ascent of the web as the highest-profile hypertext system at the same time the ban on commercial use of the internet was lifted. The use of host-oriented addressing in URLs, while non-problematic in smaller networks where the number of viewers has a low upper limit, produces problems when the number of viewers becomes large. Making even a static website scale to extremely large numbers of requests requires expensive hardware, because all the requests must be tricked into thinking they are connecting to a single machine with a single address, even in situations where a single machine of arbitrary power couldn’t possibly serve all of them. This opens up a convenient opportunity for rent-seeking: anyone with sufficient capital can provide the expensive hardware necessary for working around the web’s broken scaling, and so we get hosting services, data center services like AWS, and cache services like CloudFlare.
Even free/open source software, nominally isolated from the constraints of capital, typically makes use of host-oriented URL schemes and the host-oriented assumptions that come with them. Projects get hosted by github, or pay for domain names and get hosted in data centers. In this way, these projects shore up the finances of the merchants of scale, slumlords of the internet.
I expect that if named-data networking systems like IPFS, sneakernet-capable social network systems like Secure Scuttlebutt, & alternatives to for-profit ISPs like mesh networks & long-range wifi become popular among the numerati, we’ll see an acceleration of Capital’s slow sinking into masturbatory irrelevance in the domain of software & computer technologies. Even if you don’t have personal reasons to support these kinds of technologies (such as a concern for privacy and autonomy), I recommend supporting them on the grounds of siezing the means of computation.
You have nothing to lose but your blockchains.
By Rococo Modem Basilisk on May 31, 2018.
Fantastic, as always. Maybe even better this time.
Our own anthropologists write similar things about us. Urban legends, according to The Vanishing Hitchhiker (by an american with a long unpronouncable finnish name), function as didiactic devices: teaching us that it is important to avoid giving free rides to ghosts, and that we should periodically wash our beehive hairdos in order to limit the maximum impact of inevitable spider infestations, and that the sewer is a fantastic inverted universe of albino alligators and discarded goldfish living in harmony — the spiritual bedrock from which our collective unconscious must draw its fertile night soil, waste-water from a stone.
I suspect the american with the finnish name is right.
After all, haven’t we all seen the bloody hook, hanging off our car door handle?
By Rococo Modem Basilisk on June 1, 2018.
Invisible Architecture III
The Psychogeography of Hyperspace
[1]
Ordinarily, when we discuss psychogeography, we operate at the scale of a city: we discuss how people are drawn through a walkable space, at a speed in which they can be affected by each piece of passing scenery and make a detour to examine it. But, at the same time the Lettrists were wandering Paris and laying the groundwork for Disneyland in the micro-scale, at the macro-scale holes were being punched through the fabric of the United States in the form of highways.
Habitual migration patterns break the close association between culture and geography. There’s a psychogeographic wormhole between Fairfield County, CT and south Florida, and used to be one between there and upstate NY. If you live in Fairfield County (or certain parts of NYC) you’re liable to be culturally closer to some parts of south Florida than to many points in-between, not because of natural rock formations or bodies of water, but because highways have made it possible to pass through the middle without interacting with it.
Areas of transience — the hyperspace of psychogeography — are not ‘non-places’ with ‘non-culture’. They have a uniquely warped kind of culture, in the same way tourist-dependent places do. Some people spend large amounts of their lives there: long-haul truckers, jet-setting businessmen, touring bands or comedians or authors. The attempt at producing a consistent experience across geography produces an experience that can only exist in hyperspace.
Our own psychogeographic hyperspace is not as psychedelic as the one in Cordwainer Smith’s A Game of Rat and Dragon, but it is stranger, in a Ligottian way.
Why do they have ice machines? Why do they have pools? Why are airplanes and airports trapped in a perpetual 1963 idea of what luxury looks like? Path dependence has made the culture of our hyperspace deeply strange: there are low upper limits on quality of manufacture and high lower limits on quality of service, and the assumed transitory state of most inhabitants means bonds cannot be formed.
When new ideas are integrated here at all, they slot into existing structures — structures that don’t exist outside, in the world that is normal for most of us. Wifi passwords come with the keycard; a robot kiosk in front of the check-in desk prints tickets, as the clerk who formerly printed tickets watches.
These liminal spaces are only forgettable for people who have destinations. They’re also populated by residents, or commuters from geographically — nearby places, who fill the service jobs. What’s it like to have a diner with no regulars? This situation is capitalism-optimized. If you are a resident of psychogeographic hyperspace — nomadic or not — you are dog-fooding the experience that Rand thought was natural to man: separation of orbits, mediated by impersonal monetary transactions. It looks like endless beige hotels with broken ice machines.
What are the attributes of this Randian New Man who lives as hyperspace nomad? Sleep deprivation, boredom, and a dependence on stimulants. Gas stations in hyperspace cater to truckers, abnormally large DVD and pornography selections and every variety of pick-me-up not yet banned. Airports, whose nomads are of a higher caste and have less private space or scheduling freedom, focus more on overpriced convenience food, mediocre reading material, and headphones, and pillows. A Cinnabon could not survive outside of hyperspace, even if it grew up in the shallows of the shopping mall, but in this ecosystem, sheltered from competitors and provided with a steady stream of easy prey, it thrives.
The culture of hyperspace is not a psychogeography, except in a fairly minor sense: everywhere is the same, or tries to be. The few deviations, like Denver International Airport, are notable because deviation is so rare. There is an international internet radio station specifically for airports, and I have heard it play the same playlist in three states. (It never played Brian Eno.) However, like the hyperspace of fiction, it is a strange world that fills the gaps as we jump from one point to another without travelling in-between.
Transit systems are not the only holes in psychogeography. Communications systems also create them. Just as the prevalence of snowbirds and Adirondack cottages make it possible for small segments of northern New York and southern Florida to influence Manhattan finance and vice versa, arbitrary decisions or early-mover advantages gave Los Angeles cultural power over film, Cleveland cultural power over radio, and Atlanta cultural power over podcasting.
Communications technologies have their own hyperspace-cultures, although the geographic centralization of the big players has a larger impact on how they manifest than in the case of hotels. Twitter is the internet equivalent of a trucker-optimized gas station, and caters to journalists — the long-haul truckers of the information landscape. Facebook is the internet equivalent of a Holiday Inn: a premium-mediocre imitation of homeyness that is endearing in its complete failure to be convincing but ultimately irritating if you try to stay too long. (Facebook’s ice machine is always broken, and always loud.) The film industry is a bit like any airport: perpetually stuck in a cheap imitation of an imagined luxurious past, adapting poorly to the cosmopolitanism produced by globalization while profiting through its monopoly on the means of that globalization.
The culture of any hyperspace, because the means of association are limited to the Randian, will be corporate in its manifestation: customer service, marketing, selection, returns, free coupons as a means of apology. It differs from non-hyperspace because these are the only forms of communication truly available. This diner has no regulars with whom the waitress can be frank, only strangers perpetually passing through. It can get away with being shallow and having nothing under its surface, because only very rarely will someone stay long enough to peel it back.
By Rococo Modem Basilisk on June 1, 2018.
Have you tried making sure the problem is not actually a solution to another problem, so that they could solve each other? For instance, is the problem edible?
Have you tried reading the manual? Have you tried writing the manual? Have you tried becoming the manual?
Have you tried switching to manual?
Have you tried automation?
Have you tried reframing acceptance of the problem as a form of costly signalling, and using the problem as a gatekeeping mechanism?
Have you tried reframing ignorance of the problem as a form of costly signalling, and using acknowledgement of the problem as a gatekeeping mechanism?
Have you tried claiming that the problem ‘builds character’?
Have you tried encouraging people not subject to the problem to subject themselves to the problem in order to ‘build character’?
Have you tried accusing the outgroup of trying to solve the problem? Have you tried accusing the outgroup of failing to solve the problem? Have you tried accusing the outgroup of successfully solving the problem? Have you tried accusing the outgroup of being incapable of understanding the problem?
Have you tried accusing the outgroup of trying to prevent the ingroup from solving the problem? Have you tried accusing the outgroup of having a solution to the problem and keeping it hidden?
Have you tried accusing the outgroup of believing the problem to be a problem? Have you tried accusing the outgroup of believing the problem not to be a problem?
Have you tried accusing the outgroup of using the problem to introduce a solution that is worse than the problem? Have you tried introducing a solution that is worse than the problem?
Have you tried claiming that all solutions to the problem are worse than the problem? Have you tried claiming that a recent failure to solve the problem marks the end of all possible progress on or debate about the problem? Have you tried defining history as the story of failed attempts to solve the problem?
Have you tried accusing the outgroup of only pretending to care about the problem in order to gain social currency? Have you tried only pretending to care about the problem in order to gain social currency? Have you tried pretending not to care about the problem in order to gain social currency among people who believe that the outgroup only pretends to care about the problem in order to gain social currency?
Have you tried being very vague and confused in your descriptions of the problem, so that in your mind it metamorphoses into a different, more easily solved problem? Have you tried being very vague and confused in your descriptions of the problem, so that in your mind it metamorphoses into a different, much more difficult problem?
Have you tried using markov chain monte carlo to perform a weighted random walk of solution space?
Have you tried building an institution whose continued existence depends upon the problem having already been solved?
Have you tried building an institution whose continued existence depends upon the problem being insoluble?
Have you tried solving the problem?
By Rococo Modem Basilisk on June 5, 2018.
Microsoft, Github, and distributed revision control
Microsoft, Github, and distributed revision control
People legitimately criticize Github for creating artificial centralization of open source software & having a dysfunctional internal culture, and for being a for-profit company. Microsoft’s acquisition may not make any of these things worse, & won’t make them better. But, there’s a really specific & practical reason people not already boycotting github have begun to consider it in response to the Microsoft acquisition: Microsoft’s history of using deals, acquisitions, & standards committees as anticompetitive tools.
Github was never going to do much of anything beside host your projects, and since hosting your projects is its main business, it’s not going to do nasty things like delete them. Microsoft, however, is absolutely willing to do that kind of thing if they decide they can get away with it. History bears this out — some of it recent. Microsoft hasn’t been able to do it to the likes of IBM or Netscape since the 90s, but only because their complacency over the PC market has prevented them from being able to successfully branch out into phones or servers; however, they have been happily performing their embrace-extend-exterminate tactic on open source projects for the past fifteen years.
(Note: If Github got as big as Microsoft & had side hustles as profitable, they would do the same thing. This isn’t about particular organizations being evil — capitalism forces organizations to act unethically and illegally by punishing those unwilling to break the law.)
People concerned about open source software distribution being centralized under the aegis of unreliable for-profit companies have been boycotting Github & Gitlab for years, and Google Code and Sourceforge before that. They’ve also been working on alternatives to central repositories.
Named data networking goes beyond simply ensuring that the owner of the hostname is not a for-profit company (liable to throw out your data as soon as they decide that it’ll make them money to do so). Instead, DNS as a single point of failure goes away entirely, along with reliance on data centers.
If you’re considering migrating away from Github — even if the recent news merely reminded you of problems Github has had for years — take this opportunity to migrate your repos to git-ssb or git-ipfs, instead of moving to another temporary host-tied third party thing like gitlab or bitbucket. Your commits are already identified by hashes, so why not switch to hashes entirely & use an NDN/DHT system? That way, there’s no third party that could take down your commits if it goes down. The entire DNS system could die permanently & it wouldn’t interrupt your development.
By Rococo Modem Basilisk on June 5, 2018.
An Engineer’s Guide to the Docuverse
Introduction
Project Xanadu™ is the original hypertext project — the brainchild of Ted Nelson, and the result of careful and passionate work by innumerable clever people over the course of nearly sixty years. Because of the length of its development period, the project has spawned and used many ideas of varying importance, and particularly important ideas have had many names. Because its history spans several eras of computing, ideas spawned by the project that were once considered radical have become commonplace and other ideas that were once commonplace have become forgotten and become radical again. Over this time, most documentation available to the public has been written by Ted, and intended for a non-technical or semi-technical audience.
I had the opportunity to work on Xanadu® for five or six years, after spending years as a fan, trying to piece together a general idea of the project from scattered documentation. This gave me the privilege of being able to ask Ted for clarification on both technical and philosophical points, and it also gave me a better view of how different ideas and terms fit into different eras.
Xanadu has a poor reputation in many technical communities. Part of this is due to the popularity of a misleading and in parts factually incorrect 1995 Wired article about the project. However, I consider a larger issue to be the project’s tendency (normal through the 1980s but now very strange) to default to secrecy (even with regard to ostensibly non-secret ideas) and to consider any public release of information in terms of PR. This ultimately meant that most available information was lacking in technical detail or had its technical detail hidden behind a general-audience-friendly front. With a few exceptions (such as Udanax Green / xu88’s FeBe manual), there are no complete, well-organized, publicly-released explanations of Xanadu concepts aimed at engineers. Engineers have, as a side effect, failed to understand how certain ideas fit together and have filled in gaps in the explanation with ideas already familiar to them.
Xanadu concepts are, to a great extent, an alternate (and alien) universe. They were developed earlier than their rough general-computing counterparts in many cases, and are not widely understood outside of the community of former and current Xanadu developers.
I aim to write the guide I would have liked to have been able to read, back before Ted found me and invited me to the project. With any luck, this will clear up some misconceptions, introduce some unfamiliar ideas, and prepare people who are interested in joining the project with enough background to understand the internal documentation quickly.
This document is not intended as an introduction to Xanadu as a whole. In particular, I cannot & will not review the long chronology of the project, although when appropriate I will mention the time period in which something was developed in order to provide context to particular technical decisions. Explaining the historical importance of Xanadu and providing a gentle or gradual introduction to these ideas is also beyond the scope of this document: there is much to be explained and little time, so I will focus on the technical background and assume that readers already have a passing familiarity with Xanadu. For a non-technical introduction and brief chronology, see Ted’s video series Xanadu Basics.
I will be covering only projects I have intimate knowledge of — either from working on them, consulting on them, or immersing myself in their documentation. This leaves large gaps in the chronology (such as xu92 / Udanax Gold), and it means that I cannot adequately cover projects started after 2017 (including the current web-based demo). I will describe a few projects and ideas in passing that I don’t have detailed knowledge of, due to their importance — including OSMIC, xu92, and specialized xu88 enfilade types like the POOMfilade; however, my brief description should not be taken as complete or accurate. I will try to mark these cases clearly.
In some cases, I have independently written open source implementations of some data structures or algorithms developed under Project Xanadu™. I will provide links to these when I have them. They are intended as didactic aids — clear examples of implementations — and are often missing optimizations or integrations with other features; they should therefore not be taken as complete or production-ready implementations.
Legal notes
Xanadu®, Project Xanadu™, ZigZag™, and the flaming X logo are registered trademarks. When possible, I will be using the following generic terms: ‘xanalogical’ to refer to a system that incorporates ideas from Xanadu™; ‘translit’ or ‘transliterature’ to refer to a xanalogical system for representing hypertext or hypermedia; ‘ZZStructure’ to refer to the underlying data structure and facilities within a ZigZag™ implementation in the absence of a UI layer.
While I was at one time privy to trade secrets belonging to Project Xanadu, to my knowledge nothing I describe here is subject to currently-live trade secret protection. Furthermore, while I have written code and documentation whose copyright is ceded to Project Xanadu™, I am not writing this based on that code and documentation, but instead based on my memories of it. To my knowledge, nothing explained here is secret. Also, while there were patent applications filed with respect to some of the ideas I present, to my knowledge none of this material is currently patent-encumbered.
This document is not officially blessed by Project Xanadu™, and any inaccuracies are mine. I do not represent Project Xanadu™ or Ted Nelson.
The aerial overview: key concepts
Work on Xanadu can be divided into two general realms: transliterature, which involves hypertext and hypermedia, and ZigZag™, which involves much smaller pieces of data. These realms very occasionally interact (see my section on ZZOGL / FloatingWorld), but for the most part the technologies involved remain separate, and they have distinct and almost contradictory concerns and philosophies.
If we wanted to find a very rough equivalent in the realm of normal computing, transliterature would be word processing while ZigZag™ would be the use of spreadsheets and databases.
Transliterature, in all its incarnations, involves several concepts missing from or alien to mainstream computing: unique permanent addressing for immutable documents and source data, indirect document delivery, visible connections, and external markup. All of these ideas stem from a single basic idea: the manipulation of immutable sequences of bytes with permanent addresses.
ZigZag™’s universe is based around a particular data structure — a generalization of the spreadsheet, wherein a cell can have connections along arbitrarily many named dimensions.
I will focus on transliterature in this document because it has a longer history and more concepts associated with it. It is also what most people associate with the ‘Project Xanadu™’ name.
[1] Visible connection, AKA transpointing windows: an important UI concept in transliterature
An introduction to transliterature
Transliterature is the new name for what used to be called ‘hypertext’. Specifically: it is a way of thinking about text and other media, and a set of operations and features that way of thinking makes possible. Non-xanalogical hypertext systems like the World Wide Web implement some of the flashier features of transliterature without understanding or implementing some of the core ideas, and as a result, implementing other features is either impossible, awkward, or unreliable. Since these non-xanalogical systems dominate the discussion about ‘hypertext’, we’ve changed the name.
On the back end, there are three tenets of transliterature:
1. All addresses are permanently affixed to the data they refer to 2. All meta-information (such as links, formatting, and rearrangement rules) is distinct from the original information, and is therefore stored as a distinct chunk of data with a distinct address 3. All operations are available to all users, because no operations are destructive
This shakes out in several different ways. One is indirect document delivery.
Indirect document delivery is the idea that a document, as visible to a user, is the result of the combination of material from various sources. The way this works in practice depends on the implementation, but the current method (used since the end of the xu92 project — so, dominant since 1990 or so) is the EDL and ODL. The EDL and ODL are chunks of data with permanent addresses like anything else, but they are interpreted by the client in a particular way.
An EDL (Edit Decision List) is a set of ‘spans’ — which is to say, permanent addresses of chunks of data with byte offsets and lengths. (In xu88, a span was called a ‘threespan’ because it had three components: an address, a start, and a length.) The spans are fetched and put together in the order in which they occur in the EDL document. The resulting frankenstein is known as the ‘concatext’ (a portmanteau of ‘concatenated text’).
An ODL (Overlay Decision List) contains rules about how to display content that falls within particular address spans. Because a concatext should never contain any kind of markup (even span and div tags count as meta-information about which parts of a document are supposed to be divisible), all formatting is performed using these rules. All font information is provided by an ODL assigning a font or emphasis style to a particular span of text, for instance. The ODL also contains information about links (by claiming that one or more spans are connected by a link with some link type), and about format (by claiming that some span should be interpreted as an image, a video, an audio clip, etc.) Information about text justification or page breaks are also encoded into the ODL.
An EDL provides the capacity to remix someone else’s document, or to create a new version of your own, without storing a duplicate of the original content in a way that isn’t clearly marked. (As a matter of optimization, it is possible for a local cache to store or serve a popular concatext and reverse the process to produce the original chunk of data with holes. What matters is that placement at the original address is canonical.) The use of a reference to a distinct source — quoting — is called transclusion (a portmanteau of transliterature inclusion).
While one would expect the author of a document to provide an ODL to go along with their EDL (if necessary), such a pairing is only a recommendation: an ODL is a set of formatting rules, and each rule can be overridden by the user. Furthermore, all ODLs loaded into the system act as a formatting ruleset against any document viewed. In this way, one can open many distinct documents and see their hidden connections (if one is lucky enough to have ODLs that display links between these documents or their sources).
On the front-end, links are displayed as bridges (also called beams) between the connected spans of open documents. (In the implementations I have worked on, we had a set of configurable rules for which colors the linked text and bridges were for particular link types, and procedurally generated a color from the hash of the link type if that type wasn’t in our color database.) The use of visible beams between open documents is called “visible connection” or “transpointing windows”.
Every visible document is considered editable, because editing such a document produces a new version with a distinct address.
Differences between translit implementations
Addressing & how EDLs and ODLs are stored changed a great deal after the introduction of the web.
Udanax Green / xu88, developed in the mid-80s at Autodesk and given an open source release in 1999, represents the culmination of a particular set of ideas about addressing begun in the 1970s at itty bitty machine co. In particular, Green uses a data structure called an ‘enfilade’ — a kind of tree with special fetch rules. The enfilade works in conjunction with a ‘tumbler address’ — a sequence of numbers that tells the server how to navigate the tree.
Each tree node contains zero or more numbered subtrees, and each leaf contains a piece of information. The server uses each piece of the tumbler address to determine which subtree to navigate to, and when it runs out of numbers in the address, it concatenates and returns the data of all the subtrees.
Green was designed for clients whose bandwidth, memory, and processing speed were poor, and so effort was made to put as much processing as possible on the server and to provide the client with something practically ready to display.
Udanax Gold / xu92, developed after Green, used a different structure called the Ent. Unfortunately, full source for Gold has not been released, and I cannot provide much information about it. I met one of the primary authors once & he told me that Gold supported cryptographic signature chains for versioning, in a fashion similar to a blockchain, but I can’t confirm this.
Beginning in the 1990s, transliterature systems began using URLs and HTTP for addressing. XanaduSpace and its successor XanaSpace used this system, as do the browser-based OpenXanadu and the current browser-based demo, XanaduCambridge. This presents a few problems. The data addressed by a URL is not guaranteed to remain static — in fact, even though the W3C maintains that changing the data addressed by a URL is rude and uncouth, nearly all web content is dynamic and sites have their content completely replaced on a regular basis. Furthermore, hosting user content becomes both a technical and (with DMCA) a legal problem. The browser-based transliterature systems have an additional problem: same-origin policy makes transcluding content from different domains difficult.
In my own xanalogical systems, I prefer to use CAN/DHT based systems like IPFS. A prototype using IPFS is here.
Another concept that exists in XanaduSpace and XanaSpace is the ‘permascroll’. The permascroll is the sole violation of the rule that chunks of data are immutable. It is an append-only log of content. The idea is that, while editing, anything typed would go into the permascroll, and the EDL would transclude that content from the permascroll.
There are two ideas for how permascroll use would work in conjunction with publishing. One is that there’s a private & public permascroll division — in other words, content being edited on someone’s own machine would have addresses in the private permascroll, but the process of publishing a document would append all the portions of the private permascroll that were transcluded by that document to a public permascroll (with the exception of any that might already be present) and the published EDL would have its span addresses and offsets edited to compensate. The other uses a single public permascroll that is encrypted using the method I will describe later under the heading transcopyright, with unpublished sections considered ‘not for sale’.
ZigZag™ and the ZZStructure
The ZZStructure is relatively straightforward: mathematically speaking, it is a directed graph with colored edges, where each node can have a maximum of two edges of each color — one going outward and one coming in. We call the nodes cells, and we call the edges connections. We call the color ‘dimension’. The direction is ‘posward’ — out — or ‘negward’ — in.
Each cell, in addition to having named pairs of connections to other cells, has content (although that content can be blank).
Another way of thinking of a ZZStructure is that each run of cells along some dimension (called a ‘rank’) acts as a doubly linked list — so a ZZStructure is a tangle of objects that occupy different positions on different doubly-linked lists simultaneously.
Given these basic rules, we have a data structure that, depending on how you look at it, can effectively express a large number of different things.
One way you can look at it is as a cell being subject to properties, defined by its posward connections along dimensions. This is similar to a prototype-based object system, and is used internally in XanaSpace and elsewhere as a configuration system.
One particular property, ‘clone’, is treated specially. When you request the content of a cell, under the hood, you are getting the content of the negward-most cell along the rank ‘d.clone’ — the ‘head’ of d.clone or the ‘ clonehead’. Clones (defined as cells that have a neighbour negward along d.clone) are treated as references to their clonehead. Since a property can be a clone of some other cell, space can be saved or a property can reference a whole large group of other properties that might change independently.
Used as a personal organizer, mind-mapping tool, or database, this property model is quite useful. To the extent that ZigZag™ is used at all in practice, it is usually with a dual-pane interface as a personal organizer.
Unusual structures can be produced in ZigZag™. For instance, a ringrank is a rank that is circular — it has no head. A zipper list is a pair of ranks along some dimension with a connection along some other dimension — representing an associative array or some other correspondence.
There are currently two officially-blessed implementations of ZigZag™ available to the public: a console-based system called azz and a graphical system called gzz. Prior to joining the project, I worked on two ZigZag™-adjacent projects: dimscape, a graphical ZZStructure editor, and iX, a toy OS whose user interface and disk format was inspired by azz.
Officially-blessed implementations tend to have dual-pane interfaces because they tend to support a keyboard-based standardized interface system called KBLANG, which requires at least two panes. KBLANG is based around dividing a QWERTY keyboard into two direction pads (controlling the currently highlighted cell in their respective panes) and using the remaining keys to perform operations or switch visible dimensions.
[1] KBLANG navigation and command keys
Some effort has been made to develop programming languages that use ZigZag™ natively, the way spreadsheet formula languages take advantage of spreadsheet structure. There is some discussion here, and here is my contribution to the problem.
Transcopyright
Transcopyright is a concept that gets a lot of push-back from people who don’t really understand what it’s trying to do. I’ll do my best to explain it, with special emphasis on the confusions I have seen.
Transcopyright is a system for monetizing or controlling the distribution of one’s material within a transliterature system. It is not DRM, because it applies to data the same ownership rules that are applied to objects: what one has purchased, one owns in perpetuity, including the right to remix and release remixes.
It is specifically intended to simplify rights negotiations on derivative works, and it uses the same model as RiffTrax: remixers are providing overlay or re-arrangement rules for material already owned by the people downloading the remix. It relies upon permanent addressing for this.
On a technical level, how this works is: when a work subject to transcopyright is published, a one-time pad is generated that is the same length as all the newly-published material in the work. The material is encrypted with that one-time pad, and it is the cyphertext that is published. Anyone can view any span of the cyphertext. Anyone who wants to view a portion of the plaintext must request that portion of the one-time pad from the author or some trusted oracle.
A remix or edit can be downloaded by anyone, but those pieces the viewer doesn’t yet have permission to view will be cyphertext (subject to an ODL entry that blacks it out and provides information about how to purchase the rights to it). One may decide to pay for only the bytes present in the remix. Remixes of things the viewer already owns will not require a second copy.
Once you have a copy of the plaintext, there are no technical mechanisms to prevent you from doing what you like with it. However, etiquette dictates that you do not distribute the plaintext or encryption key beyond a reasonable level, and normal copyright law kicks in at that point. The software itself does not provide the facility to distribute either on your behalf, for material you have not created.
ZZOGL / FloatingWorld
XanaduSpace and XanaSpace use a unique system for handling display, wherein a ZZStructure controls the placement, shape, color, and interaction profile of objects in 3d space. This is called ZZOGL (for ZigZag OpenGL) or FloatingWorld (FW for short).
FW is an extreme form of the ‘property orientation’ model mentioned in the ZigZag section. In it, cells correspond either to object heads (around which properties coalesce) or property values.
Starting from a cell representing the start of the structure (the zzogl head), one dimension represents the draw order. We iterate over objects along this dimension twice. During the first pass, we calculate sizes and locations; during the second pass, we draw using our calculated sizes and locations.
Each object calculates its location with respect to some other object within a location-offset DAG. Its parent’s size in three dimensions is multiplied by a vector of between 0 and 1 (the ‘parentcenter’ — the point on the parent with respect to which we move), we add the delta, and then position it on a point on ourselves (size * center).
pos = parent.pos+(parent.size*(parent.center-parentcenter))+delta-(size*center)
This kind of dynamic relative positioning made it possible to represent relatively complex relationships between objects. We used it to display both transliterature and ZigZag systems.
There were a couple kinds of objects:
• ‘groups’: invisible dummy objects for moving whole groups of objects at the same time • ‘slabs’: rectangular prisms • ‘beams’: tetragonal two-dimensional shapes with a solid color • ‘tetroids’: textured two-dimensional shapes that are shaped like tetris-blocks or paragraphs of text (in other words, a rectangle with at most two smaller rectangles abutting it at top or bottom)
Unfortunately, getting OpenGL to render large quantities of text on top of 3d objects while being responsive enough to edit that text in real time was a difficult problem, and one that we were never able to solve. As a result, XanaduSpace and XanaSpace never had a release.
OSMIC
OSMIC is an alternative document versioning system — a general-purpose journal-based system for tree-based versioning of any byte sequence. Each version is an address in a list of steps. A step can be one of:
• insert bytes at a particular point • delete bytes from a particular point
A version’s address is, like a tumbler address, a period-separated sequence of numbers. Each time a new version is created, the last number increments. Each time the version tree forks, a new number is added at the end, starting with zero.
While it’s possible to reconstruct a version from scratch by running the journal, it’s also possible to rewind to a particular version by treating every delete as an insert and every insert as a delete. An OSMIC journal can be much larger than the byte string it represents, so a compact representation is important. I am not aware of any work on a compact binary representation of OSMIC journals.
I haven’t worked with OSMIC, but at one time it was suggested that XanaduSpace should be retrofitted with an OSMIC-based system for storing the state and history of the ZZOGL system (and thus, all internal information necessary to display and edit documents).
Transliterature editing
Several systems exist for transliterature editing, some of them released. None of them are, to my mind, satisfying. All are based around selecting and re-ordering spans from existing documents. My implementation is called SPANG and is intended for use with OpenXanadu. XanaduCambridge has a less elaborate, more manual system.
We had pitched a more natural system, wherein selected text could be dragged out of a window to create a new document, or dragged to snap to the end of a document (or to the middle of a document as an insertion) to produce transclusions. We suggested dragging a selected span of text on top of another selected span of text to produce a link between them.
Such a system would require an environment where documents floated and text could be dragged between widgets — in other words, something that the cross-platform graphics libraries we were working with didn’t support.
Formats
On-disk formats (for ZZStructures, EDLs, and ODLs) were constantly in flux while I was at Xanadu. Some were, from my perspective as a professional programmer, abysmal. Most were either based on CSV or key-value pairs. In some cases, they used CSV for mandatory fields and key-value notation for optional fields.
I prefer a subset of YAML or JSON for ZZStructures, because ZZStructures fit that model well. However, since these structures have explicit hierarchy, they were controversial.
Generally speaking, EDL formats have been one line per span, with ordered triples (address, start, end). Whether or not keys are used differs from spec to spec, as does how (or whether) one can specify a whole document without specifying its length.
Generally speaking, ODLs have been lists of addresses pointing to “link” files, and those “link” files contain a type followed by sections called ‘endsets’ that contain one or more spans each. Generally, a link has one or two endsets. If a single endset has non-contiguous spans, the resulting beam resembles a starfish or a hand. I argued in favor of arbitrarily many endsets, each with arbitrary many spans. I also argued in favor of zero-length single-sided links, which I considered to be like bookmarks. I implemented page breaks in XanaSpace as zero-length single-sided links.
There have been arguments over whether or not it is appropriate to support spans that are with respect to the concatext of some document (and if so, how to specify that). Ted has generally been against this feature, and I have generally been in favor of it — both in EDLs and ODLs.
If you’re planning to implement a xanalogical system, compatibility with an existing format will not be a problem: there are no genuine standards, even within the project. However, you should consider the problems mentioned above.
By Rococo Modem Basilisk on June 6, 2018.
As an engineer (identifying with the engineering attitude moreso than the artistic one) I think there’s something to be said for the iterative coevolution of our myths & our selves, toward proximate ends.
The feedback mechanism between narrative & behavior is not perfect, if only because it is so easy for people to spectacularly miss the point of anything in stunningly creative ways. This gives us enough wiggle room to perform a bit of stimulated annealing: conscious detournment, culture jamming. We’ve done this in mostly a reactive way, and that reaction-based pattern is easily manipulated by greater narratives that are harder to see, but we could do it in a directed way.
Collective myth is a vast machinery of nudges, weak impulses and incentives, and good-enough heuristics. Nobody polices it. Like an economy, or a large codebase, we can find vulnerable surfaces, inject our own gear assemblies, and reverse or block certain control flows. All countermeasures to mythic engineering are automatic and incapable of cunning.
By Rococo Modem Basilisk on June 11, 2018.
Enough to be Dangerous
A really important idea in programming-language-design-as-UI-design is time-to-ETBD.
How much time does it take to go from zero to Enough To Be Dangerous (i.e., enough knowledge to be able to reliably make turing-complete constructs with conditionals and simple math). If you’ve got ETBD in some language, then you can write anything in it (barring IO), given enough effort, even without learning any more features.
Some languages have a really fast ETBD — prolog, forth, basic, python. Esolangs often don’t even have features beyond the ETBD level (brainfuck and unlambda for instance).
A non-programmer can learn all of brainfuck in ten minutes. They are now a minimum-viable-programmer. Nevertheless, actually doing anything in it is a turing tarpit, and this is true of any language when you get to ETBD status in it.
Having a fast time-to-ETBD is important for a language because you need to get there before the user is ready to learn anything else. Optimizing documentation for time-to-ETBD is important for the same reason.
The amount of effort it takes to get to ETBD in Java, C, or C++ & the number of new concepts that need to be paid tribute to (if not actually understood) is a barrier to new programmers.
(Modern GUIs never reach an ETBD point except ones intended as languages. As a result, modern GUIs, as framed as programming languages, are essentially dysfunctional. Likewise, lots of systems are ostensibly non-turing-complete but have Weird Machines that allow you to glitch them into turing-complete behavior — like CSS or truetype fonts; since finding these features is not really possible for new users, it doesn’t count toward learning curve.)
Minimizing time-to-ETBD means minimizing the amount of time, effort, and documentation to go from total non-programmer to minimum-viable-programmer.
I do not consider the web stack to be a meaningful improvement over other systems by this metric, unless you consider obtaining an environment part of time-to-ETBD and do not consider knowing that such an environment exists as part of it. Even then, home computers booting into BASIC and shipping with manuals starting with simple example programs are better time-to-ETBD in that sense than browsers — the time from first computer purchase to being able to write simple turing-complete programs reliably on 8-bit micros could be minutes (or could be negative, if the salesman at the computer store taught you to program on a demo machine). There are people who have been using web browsers for twenty years without ever suspecting that they could write a program that would run in one with a text editor.
By Rococo Modem Basilisk on June 15, 2018.
Exocortex tools part I: social media automation with very small shell scripts
Exocortex tools part I: social media automation with very small shell scripts
Exocortex Tools is a series describing my personal set of utility shell scripts. The term ‘Exocortex tool’ in this context comes from @VirtualAdept, but this does not represent his toolchain. This entry will be focusing on post, links.sh, and notes.sh.
There’s a lot of interest in cross-posting tools & alternatives to traditional web-based centralized social media lately. I have been using a toolchain I wrote myself for several years, and I have avoided describing it until now because the tools I wrote are so simple that I didn’t consider them worth describing. However, I’ve seen a lot of people using webtech or otherwise doing much more work than necessary, while failing to achieve feature parity with what I’ve got.
When it comes to any form of automation, the unix shell is your friend: it will allow you to compose existing tools together with a minimum of fuss, and existing tools are typically pretty well-suited to using it (or else programs like curl are well-suited to bridge that gap).
In my particular case, I have a presence on a variety of social networking sites, only a few of which have IFTTT integration, and I would like to automate the broadcast of certain types of posts. In particular, I want the ability to post arbitrary microblogs to all services simultaneously, the ability to post links with automatically-fetched titles, and to have those links show up in a pinboard-like minimal interface on my website. Furthermore, I would like integration with a system for short plaintext notes I already keep.
The easiest problem to solve is the problem of simultaneous broadcast. There is a command-line tool for posting to almost every social network. In my case, I use the ruby gem ‘t’ for twitter, the pip package ‘tootstream’ for the fediverse, the npm package ‘sbot’ for secure scuttlebutt, and the twtxt command line client for twtxt, while relying upon IFTTT to relay twitter posts elsewhere (since twitter is the only microblogging service mentioned above that IFTTT knows about). The shell script is VERY small.
#!/usr/bin/env zsh args="$@" t post "$args" yes | twtxt tweet "$args" echo -e "toot -v $args\nu" | tootstream sbot publish --type post --text "$args"
This tool is named ‘post’ & it does what it says on the tin: it posts the args (quoted or otherwise) everywhere I care about. Posts that are too long for twitter will fail to post there and merely be posted to the other services (which have larger maximum post sizes).
As for links, the problem is slightly more difficult. We need a format for storing link information permanently, a mechanism to transform the list of links into a web page, and a mechanism to add a link to the list.
I decided to use a three-column TSV to store link information. I append lines to the end, so the resulting file is in chronological order. The columns are: URL, time, and (optional) title.
Producing a static HTML file from this is straightforward: we produce the beginning and end of the HTML file, then (reading the list of links backwards) produce an entry for each.
function fmtlinks() { echo "<html>" echo '<head><title>Links</title><link rel="stylesheet" type="text/css" href="vt240.css"></head>' echo "<body>" echo "<table>" echo "<tr><th>Link</th><th>Date</th></tr>" tac ~/.linkit| awk ' BEGIN{FS="\t"} { title=$1; if($3) title=$3; print "<tr><td><a href=\"" $1 "\">" title "</a></td><td>" $2 "</td></tr>" }' echo "</table>" echo "<center><a href=\"https://github.com/enkiv2/misc/blob/master/links.sh\">Generated with links.sh</a></center>" echo "</body>" echo "</html>" }
The only logic in the center is to use the URL as the title if the title entry is empty. This is a small & simple script, and could be made shorter if I sacrificed readability. Because HTML is a nightmare, I have erred on the side of clarity over terseness.
The most complicated part of adding links is fetching their title, and the most complicated part of fetching link titles is dealing with strange nonstandard HTML escape logic. Here is my title-fetching code:
function getTitle() { curl "$1"| grep -a -i "<title>" | head -n 1 | sed 's/^.*<[tT][iI][tT][lL][eE]>//;s/<\/[tT][iI][tT][lL][eE]>.*//' | sed 's/&#039\;/'"'"'/g;s/&#39\;/'"'"'/g;s/&quot\;/"/g' | tr '\221\222\223\224\226\227' '\047\047""--' }
First I pull down the HTML into a pipe, so the fetch for a large page will actually be aborted once I have found the first title tag, then I remove everything before the end of the open tag and everything after the end of the close tag. After that, all of the logic is related to processing common HTML escapes and stripping commonly-found but invalid characters (like ‘smartquotes’).
Adding a link involves a few steps: we must get the title, create a truncated title so that both title and URL will fit on twitter, and post the result (if it exists). I additionally use Vice Motherboard’s “mass_archive” script to add any URL in my link archive to the wayback machine and archive.is.
function linkit() { (which mass_archive 2>&1 > /dev/null && mass_archive "$1") export LC_ALL=en_US.UTF-8 echo -e "$1\t$(date)\t$(getTitle "$1")" >> ~/.linkit post="$( export LC_ALL=en_US.UTF-8 ; tail -n 1 ~/.linkit | awk ' BEGIN{FS="\t"} { url=$1 ; title=$3 ; if(url!=title && title!=""&&title!=" ") { if(length(url)+length(title)>=280) { delta=(length(url)+length(title))-280; delta+=4; if(delta<length(title)) { title=substr(title, 0, length(title)-delta) "..." ; print title " " url } } else print title " " url } }' | grep -a . | head -n 1 | recode -f UTF8)" [[ -n "$post" ]] && post "$post" stty sane }
We first run mass_archive if it exists in the path. Then, we write an entry to the link archive TSV. We read the final entry in that TSV, pull out the URL and title, and if the combined size of the URL and the title is more than 280 characters, we truncate only the title portion, adding an elipsis at the end. If the URL itself is longer than 280 characters, we instead emit an empty string. Once we’ve done that, we grab only the first line (in the rare case that a title will somehow emit a newline) and force the encoding to UTF8. Having done that, we post.
The final line, ‘stty sane’, exists only to clean up glitches that sometimes occur when curses-based clients (like tootstream) exit before cleanup — for instance, if I force-killed it before it had finished posting a link.
The remaining function in our link-archive system is just a helper method for upload:
function uploadlinks() { fmtlinks > ~/index.html scp ~/index.html $1 }
My note system is just a plain text file, to which I append single lines, with a handful of helper functions. I have special post-related helper functions for ‘band name of the day’ (which I add to my notes, since I mostly use that feature for storing interesting turns of phrase) and ‘bad idea of the day’ (which I post but do not add to notes).
#!/usr/bin/env zsh [[ -e ~/.notes ]] || touch ~/.notes function addnote() { read x echo "$x" >> ~/.notes } function rnote() { if [[ $# -eq 0 ]] ; then shuf -n 1 ~/.notes else egrep "$@" ~/.notes | shuf -n 1 fi } function gnote() { egrep "$@" ~/.notes } function lnote() { if [[ $# -eq 0 ]] ; then tail -n 1 ~/.notes else tail -n "$@" ~/.notes fi } bnotd () { echo "$@" | addnote; post "Band name of the day: $@" } biotd () { post "Bad idea of the day: $@" }
Conclusion
This is not a pretty system. It’s not necessarily an efficient system. However, it’s an extremely straightforward, low-effort system that works reliably enough for its intended goals.
Since it’s an idiosyncratic system built specifically for my needs, it presents features that other users will not desire. So, I don’t recommend anyone adopt it. However, let it be seen as evidence that this kind of thing can be done with very small shell scripts, and with next to zero developer effort.
Every time I see someone implement a trivial static-site generator and then decide to use a web service to post to it, I die a little inside: why not eliminate the most irritating thing about any website (the web portion)? The answer is probably “I didn’t think of it”.
Well, now you’ve thought of it.
By Rococo Modem Basilisk on June 15, 2018.
Alternate Style Guides
Alternate Style Guides
Announcing the winners of the 2017 No Budget Film Contest
Announcing the winners of the 2017 No Budget Film Contest
First place: The Ledger of St. Dermain, by tENTATIVELY, a cONVENIENCE
Second place: A GOMBOSTŰRENGETEGBEN, by SYPORCA WHANDAL
Third place: TEXAS CHAINSAW MASSACRE SWED, by William Sellari
[1] Your trophy, a picture of a coin
tENTATIVELY, a cONVENIENCE, please contact me with your preferred email address for taking paypal transfers.
A short glossary of tech industry terms
A short glossary of tech industry terms
Enterprise-grade: slow and broken
Enterprise ready: once reached the front page of HN
Script: a small segment of someone’s shell history
Software engineer: a programmer with a bachelor’s degree
Software engineering: writing code that doesn’t need to exist, but writing it in Java
Market Myths: Good, Bad, and Bazaar
Market Myths: Good, Bad, and Bazaar
The stories that hold up western* capitalism
A procedural note
The truth value of a myth doesn’t matter. However, some myths have become so strongly internalized that they become difficult to identify as myths: they are mistaken for “common sense”. For most of us, the ideas underlying western* capitalism are like this. It’s difficult to separate ourselves from these myths & gain the appropriate distance, so I’m going to engage in a little bit of ‘debunking’ — specifically, I’m going to take some time specifically pointing out parts of the capitalist model that don’t match with reality or history, during the course of analyzing its structure and function. This doesn’t take away from the immense power and importance of capitalist mythology, nor does it indicate that I consider all of the ideas associated with capitalism to be strictly false.
On tautology
Academics tend to treat tautologies as a lesser form. Tautologies are shallow, by their nature. It’s quite reasonable for a system optimizing for novel and interesting ideas to reject tautologies. Nevertheless, some really important ideas can be rephrased as tautologies — as Charles Fort points out, natural selection is better summarized as “survival of the survivors” than “survival of the fittest” — and one can make the argument that any really true argument is in some sense circular. There’s no shame in a circular argument that depends only on true premises: in fact, this is one way to look at all of mathematics — which is true because of its internal consistency, and only accidentally coincides with physical reality.
When someone dismisses a seemingly profound statement as “just a tautology” they omit important information. An obvious tautology contains no information; however, a non-obvious tautology is just about the most profound thing imaginable: it takes a complex, incomplete, vague collection of loosely related ideas and replaces it with a much smaller and simpler set of rules, which (if the tautology is reasonably close to correct) is both at least as accurate as the original set of ideas and easier to reason about. A non-obvious true tautology refactors huge sections of our mental models. Obviousness is a function of existing knowledge, so what is an obvious tautology to some people will be non-obvious to others. It should come as no surprise that people seek out ideas that present themselves as non-obvious tautologies.
The drive toward seeking non-obvious tautologies can lead to mistakes. Looking for simple and efficient models of the world is a mechanism for enabling lazy thinking; when lazy thinking is correct it’s strictly superior to difficult thinking, but lazy thinking often comes with lazy metacognition. If we jump on ideas that look like non-obvious tautologies too greedily, we fail to see hidden assumptions.
Market efficiency is a very attractive model. Under certain circumstances, we can expect things to actually work that way. If a large number of competing producers really do start off completely even in capability, we really can expect the best product to price ratio to win out. To accept it completely means ignoring hidden assumptions that serious thinkers should at least consider.
One hidden assumption in market efficiency is that competitors start off even in capability. This is almost never the case outside of a classroom demonstration. Companies enter established markets and compete with established competitors, and companies established in one market will enter another; both of these mechanisms make use of existing resource inequality in order to reduce precisely the kinds of risks that lead to efficient markets, and while perhaps in the long run poor products might lose out, with the extreme spread of resource availability the “long run” can easily last until long after we are all dead. Given no other information, if age is not normally or logarithmically distributed, we can reasonably expect something to last about twice as long as it already has; with corporations, the tails of this distribution are further apart: we can expect a startup to be on its last legs, and we can expect a 50 year old company to last 75 more years, because resource accumulation corrects for risks. A company that has a great deal of early success can coast on that success for a much longer period of poor customer satisfaction.
Another hidden assumption is that communication is free within the set of consumers and between consumers and producers but not within the set of producers.
Free communication within the set of producers is called collusion, and the SEC will hit you with an antitrust suit if you are found to engage in it. People do it all the time, and it is usually worth the risk, since it reduces market efficiency down to almost zero.
Free communication between producers and consumers is also pretty rare: even failing producers typically have too many consumers to manage individually and must work with lossy and biased aggregate information; successful producers have enough resources to be capable of ignoring consumer demand for quite a while, and often encourage ‘customer loyalty’ via branding (in other words, cultivating a live stock of people who will buy their products regardless of quality — ideally enough to provide sufficient resources that appealing to the rest of the customers is unnecessary). Customer loyalty can have its benefits compounded if wealthy customers are targeted: “luxury brands” are lucrative because something can be sold well above market price regardless of its actual quality or desirability, and sometimes the poor price/desirability ratio is actually the point (as a form of lekking / conspicuous consumption).
Free communication between consumers is becoming more and more rare, since flooding consumer information channels with fake reviews and native advertising is cheap and easy. There used to be stronger social and economic incentives to clearly differentiate advertising from word of mouth, but advertising’s effectiveness has dropped significantly as customers develop defenses against it and economic instability has encouraged lots of people to lower their standards. Eventually, consumer information channels will become just as untrusted as clearly paid advertising is now considered to be, and communication between consumers will be run along the same lines as cold war espionage.
Motivated reasoning
Considering that the hidden assumptions in market efficiency are dependent upon situations even uninformed consumers know from experience are very rare, why would people accept it so easily? The inefficiency of markets has no plausible deniability, but motivated reasoning lowers the bar for plausibility significantly.
During the bulk of the 20th century we could probably argue that anti-communist propaganda played a large role. I don’t think that’s true anymore. Nevertheless, in many circles faith in the invisible hand actually is increasing.
There’s another kind of circular reasoning — one that operates on the currency of guilt and hope. If one accepts market efficiency, it tells the poor that they can rise up through hard work, and it tells the rich that they earned their wealth. (This is remarkably similar to the prosperity gospel, which claims that god rewards the righteous with wealth and therefore the poor must have secret sins; it also resembles the mandate of heaven, which claims that all political situations are divinely ordained and therefore disagreeing with the current ruler is sinful.)
The similarity between the guilt/hope axis of the market efficiency myth and the prosperity gospel explains the strange marriage between Randian objectivists and Evangelical christians found in the religious right. We can reasonably expect many members of this group to be heavily motivated by the desire to believe that the world is fair. It’s not appropriate to characterize this movement as lacking in empathy: empathy is a necessary prerequisite for a guilt so extreme that it makes an elaborate and far-fetched framework for victim-blaming look desirable.
For the poor of this movement, at least on the prosperity gospel side, it might not be so terrible: motivating a group of people to do the right thing has a good chance of actually improving life generally, even if their promised reward never materialized; second order effects from accidental windfalls are more dangerous, though (if you disown your gay son and then win the lottery, you’re liable to get the wrong idea about what “doing the right thing” means).
That said, while the above factors encourage people to trust more strongly in an idea of market efficiency they already accept, bootstrapping the idea of market efficiency is much more difficult.
Natural law / myth vs legend
Market efficiency draws power from an older myth: the idea that money is a natural and universal means of exchange. This is historically and anthropologically dubious. David Graeber, in his book Debt: The First 5,000 Years, makes an argument for the idea that systematic accounting of debts predates the use of actual currency and furthermore only became necessary when cities became large enough to necessitate something resembling modern bureaucracy. Regardless of how accurate that timeline is, we know that gift economies, potlatch, and feasting are more common in tribal nomadic societies than any kind of currency exchange, and that feasting in particular remained extremely important in Europe through the Renaissance.
The legend that backs up the myth of money-as-natural-law takes place in a town. A shoemaker trades shoes for potatoes, but doesn’t want potatoes, so he organizes a neutral currency so that potatoes and apples can be traded for shoes. Graeber points out that this level of specialization couldn’t be ‘natural’ — the town is an appropriate place to set it, since specializing in a particular crop or craft would have been suicidal in the bands of 20–50 people that most humans lived in prior to around 2000 BC.
Our first examples of writing, of course, coincide with the first permanent settlements to have a large enough population to justify heavy specialization. Our first examples of writing are, in fact, spreadsheets recording debt and credit. This, along with the evidence that the unit of currency (the mina of silver) was too substantial for most people to afford even one of (and probably was mostly moved between rooms in the temple complex), is part of Graeber’s argument that independent individuals carrying money for the purpose of direct transactions (i.e., our conception of money) probably only became common later, when imperial armies were expected to feed themselves in foreign lands.
So, on the one hand, it seems to have taken a very long time for the ‘natural’ ‘common sense’ concept of money to take hold among humans. On the other hand, people exposed to the idea of money tend to adapt to it quickly and we have even been able to teach apes to exchange tokens between themselves in exchange for goods and services — in other words, it’s a simple and intuitive system that even animals we mostly don’t consider conscious can grasp.
If something is considered natural law, it’s very easy for people to believe that it is also providence. If something is straightforward and useful in every day life, it’s very easy for people to consider it natural law.
Moral economies
Thoughtful economists tend to recognize the caveats I present here. Some behavioral economists have done great work on illuminating what kinds of things aren’t — or shouldn’t be — subject to the market. This, in turn, illuminates the market myth itself.
It’s possible to think of social relations as economic in nature. Indeed, this is a pretty common model. Transactional psychology presents social interactions as the exchange of a currency of strokes, for instance. Nevertheless, Khaneman presents an experiment that shows social relations aren’t, and shouldn’t, be fungible.
The experiment went like this: a busy day care center has a problem with parents picking up their children late, and instates a fee; parents respond by picking up their kids late more often, and paying the fee; after the fee is eliminated, the percentage of on-time pickups does not return to the pre-fee state.
Khaneman interprets the results in this way: initially, parents thought of picking their kids up late as incurring a social debt (they were guilty about inconveniencing the day care); the fee reframed it as a service (they can pay some money in exchange for their kids being watched a little longer, guilt-free); when the fee was eliminated, they felt as though they were getting the service for free.
This result looks a whole lot like the way fines for immoral business practices end up working.
If we consider that, typically, we can make up to people we feel we have wronged in a variety of ways, we consider social currency to be somewhat fungible. Nevertheless, exchanging money for social currency is still mostly taboo: paying for sex is widely considered taboo, and even those of us who feel no taboo about sex work would find the idea of someone paying someone else to be their friend a little disturbing. If my best friend helps me move furniture and I give him a twenty dollar bill, he would be insulted; if I left money on the dresser after having sex with my girlfriend, she would be insulted.
We could consider the ease with which money is quantified to be the problem. We rarely can put a number on our guilt or joy. On the other hand, we can generally determine if we feel like we’ve “done enough” to make up for something — our measures of social currency have ordinality, if not cardinality.
Instead, I think the disconnect is that money is, by design, impersonal. I cannot pay back my guilt over Peter by giving him Paul’s gratitude toward me. This is where transactional psychology’s monetary metaphor for strokes falls apart: a relationship is built up via the exchange of strokes, and that relationship has value based on trust; meanwhile, any currency has, as a key feature, the ability to operate without trust or even with distrust. Money makes living under paranoia possible, and sometimes even pleasant. But exchange of strokes has its own inherent value, and the trust it builds likewise: it cannot be replaced with money because money’s value is based only on what it can buy.
Speculation
The belief in market efficiency, and the emotional and moral dimensions of that belief, have some unfortunate consequences in speculation. Paradoxically, these consequences are opposed by the myth of money as natural law.
With speculation, one can create money without substance: promises, bets, and hedges can be nested indefinitely to create value held in superposition. A stake in a speculative market is both credit and debt until it is sold. This is natural, since social constructs are eldrich, operating on fairy logic. This is both a pot of gold and a pile of leaves until I leave the land of the sidhe. Of course, there’s every incentive to oversell, so more often than not it’s a pile of leaves: when too many superpositions collapse, so does the market.
Naive materialism, when it intersects with the idea of money as natural law, finds the eldrich nature of money in speculation disturbing. Isn’t money gold? Or coins? How can something be in my hand and then disappear? So, we get arguments for the gold standard along moral lines: “it’s immoral for something that’s real to behave like fairy dust, so we should limit its growth to match mining efficiency”.
The eldrich behavior of money has some paradoxical results. Being aware that money is a social construct tends to decrease its value (clap your hands if you believe!). The question “if someone burns a million quid on TV, does the value of the pound go up or down” is very had to answer. (If you think you know the answer, substitute a million for a trillion, or for twenty.) On the other hand, being aware of its eldrich nature also tends to slightly decouple one from potentially-destructive drives.
Belief in market efficiency leads successful speculators to believe themselves skilled. While skill at speculation might be possible, statisticians who have studied the problem have generally come to the conclusion that the success distribution is adequately explained by market speculation being entirely random. Unwarranted confidence can lead to larger bets, which (if results are random) means half the time the money disappears into thin air. This does not require malfeasance, misrepresentation, or willful ignorance (as with the 2008 housing crisis): believing that speculation involves skill is sufficient to cause the market to have larger and larger bubbles and crashes.
The Lone Gunman
The Lone Gunman
There’s a lot of debate about gun control in the United States. However, both sides, by participating in the conversation at all, have a central confusion. The gun control debate isn’t (or at least shouldn’t be) about guns at all.
Gun control advocates and anti-gun-control advocates typically focus on the use of firearms in a very specific situation: when firearms are used in mass violence. The debate centers around mass shootings on one hand, and on the other hand, upon self defense against a large group of targets. Regulation debates focus on automatic and semi-automatic weapons and large clips. This is strangely at odds with reality. After all, even a machine gun is significantly less effective at mowing down large numbers of targets than a bomb — or a car. The firearm is a weapon oddly unsuited to mass murder: even for semi-automatic weapons, the ideal use case is against a single easily identified stationary target from relatively far away. As a weapon, a gun is a great deal like a bow and arrow, although a gun can shoot farther with more accuracy and with greater force, and it’s faster to reload its projectiles. This should be enough to immediately reject both sides’ arguments from the perspective of materialism: any constraints placed on guns should be placed doubly or triply on automobiles, pressure cookers, fertilizer, boats, and weak poisons. The argument isn’t about guns as physical objects.
If the best analogy to the gun as a physical object is the crossbow, then the best analogy to the gun as a symbol is the katana. Physically, the katana is a very limited weapon: it’s a sword, long and heavy enough to take a great deal of effort to wield yet with clearly shorter range than a projectile weapon or even a spear or lance; created via a laborious process made necessary by the poor quality of Japanese iron deposits and the relatively primitive state of Japanese metalworking techniques, even katanas legendary for their high quality steel would be laughed at by medieval european blacksmiths. Yet, because of the association between the katana and the samurai class (enforced by multi-century rules about who was allowed to own these weapons), the katana has incredible symbolic power. In an age where actual warfare in Japan was largely being performed by domestic copies of imported Portugese flintlocks, a sword ban was instated to keep the samurai down. Even today, Japanese cinema is full of sword users, and invents magical techniques by which the sword might act as a ranged weapon. Despite its impracticality as an actual weapon, the katana has an incredible symbolic power to the Japanese (and to some westerners) that keeps it from being ignored. The katana represents a romanticized view of the samurai, and especially the ronin — in other words, it represents the image of a lone warrior who maintains his pride despite disgrace and whose power comes from intense training and self-discipline.
It is another such image that keeps the idea of the firearm relevant in a world where most actual warfare is performed by bombs of varying degrees of autonomy: the image of the lone gunman.
Let us examine the action hero. He is a middle-aged white man — never young, never black, never blonde or a red-head. He is very much like the standard FPS protagonist. He is muscular, poorly shaven, and is usually either ex-police or ex-military (although occasionally he is still affiliated, but not considered a part of the in-group). He works alone. He fights a large and organized force of well-equipped enemies; he does not do so out of some traditional defense of “justice” or “the law” (because he is too cynical to believe in such things) but instead for some intensely personal reason (usually to protect or avenge a family member, who is most often female). Even as the enemy uses bombs, noxious gasses, poisonous injections, throwing knives, or other weapons, our action hero protagonist uses firearms; to the extent that he uses any other weapon, he does so out of necessity, improvising, after he loses his gun or runs out of ammunition, and the weapon he improvises is almost never more destructive than a gun. (This is mirrored in samurai flicks, particularly in parodies — in the first episode of Gintama, the title character destroys a highly advanced alien-made nuclear weapon by hitting it with a wooden sword, having refused to accept a laser gun previously.) The action hero doesn’t plant bombs, although he may allow the enemy to be blown up by their own bombs; when encountering a piece of destructive machinery, even after defeating its operator, the action hero will not choose to use it, except perhaps as a transportation device, and any destructive effects of such a device will be accidental — our action hero won’t steal a tank, and although he might steal an attack helicopter he won’t use the helicopter’s bombs or machine guns.
Our gun control advocates fear the action hero to some degree; after all, the action hero works toward the goal of a safe society only incidentally. Our gun control advocates also fear those actual human beings who have been possessed by the action hero / lone gunman archetype: school shooters, right-wing terrorists, and corrupt cops. To some degree this is justified: while the action hero himself does not and cannot exist, those who have sublimated themselves into this archetype can do quite a lot of damage before their luck runs out. However, in another sense, this is foolish: the terrorist who packs a machine gun instead of a bomb is a bit like the man who tries to take on the army with a sword; he has confused symbolic strength with literal strength, and the limitations of his weapon will prevent him from doing nearly as much damage as he expects. In a sense, those who fear these groups should feel lucky that they suffer under the delusion that their weapon of choice is ideal; were they to replace their media consumption with proper training and think clearly about weapons as tools, they would be far more dangerous.
On the other hand, those who fear gun control identify strongly with the action hero, or at least believe that they could become his manifestation under the right circumstances. People who hoard guns against what they see as an oppressive government are operating on action movie logic: a small group of people with automatic weapons cannot even defend themselves against a national army, although a single con artist could probably decimate a national army with some poison and a great deal of courage.
The lone gunman, though he is often associated with the religious right’s reformulation of Randian Objectivism, in a sense is a stranger bedfellow with Objectivism than the religious right itself is. No Randian hero, the lone gunman is a loser who does not win, but instead causes others to lose. He never profits from his actions, nor does he intend to; he comes into the story already damaged and rejected by a world that he doesn’t fit into, and his goal is to save someone (usually a family member) from a threat that appears after the beginning of the narrative, or to take revenge for that threat. He plays only negative sum games: his goal is to return to the same level of dysfunction he is used to, having caused harm to some third party (usually some variety of “foreign terrorist”). The family he rescues is one he is almost invariably estranged from, just as he invariably has a warped relationship with the career that gave him the training he uses: while usually a former soldier or police officer, if he happens to be a current officer he is a pariah.
I would place the beginning of the lone gunman figure in film with the release of Die Hard. The elements of Die Hard that were originally (in the style of the Last Action Hero) a satire or subversion of action movie tropes eventually became the defining traits that separate the lone gunman from older 80s-style action hero figures, and these traits are important to note: the lone gunman, though skilled, is not ‘fit’; rather than being a well-rounded person who happens to excel at violence, this figure is a loser and outsider who (in a strange warping of the hero’s journey) discovers that he has a talent for violence when he is thrust into a situation where he uses it. He may be an ex-police-officer, but he can fight off hundreds of current police officers who have better training. Much like how, out of context, the stories of popular detective characters appear to be about a person who supernaturally attracts criminal acts to happen around them, the lone gunman appears to attract swarms of unrelated attacks.
I would like to also distinguish the lone gunman figure from another star in our constellation of men of action, the hardboiled detective. While the hardboiled/noir protagonist appears to have much in common with the lone gunman — both are losers thrust into lives of violence to which they are unnaturally acclimated, within the matrix of a society they cannot integrate into — the hardboiled protagonist’s cynicism is always a put-on. A hardboiled protagonist, being a “shop-worn Galahad”, has more in common with the ronin figure or with the hero of westerns: he may pretend to have purely selfish and material reasons for his actions, but he acts according to a strict moral code he would rather not admit he adheres to. The cynicism and nihilism of the lone gunman figure is real, and in an inversion of the hardboiled protagonist, the lone gunman acts as if his behavior is justified by familial loyalty or revenge, when it is clear that revenge is just an excuse for immersing himself in a world of violence. Where all other action hero protagonists are acclimated to violence by necessity and are at least as estranged from violent exchange as they are from the rest of the social world, the lone gunman has a greater connection to violence than with the every-day. All other forms we have discussed are rejects who carry a set of moral guidelines from a world that no longer exists or is closed to them; the lone gunman has never had a home, but finds one in the process of taking revenge, and his moral sense is warped accordingly.
In other words, the lone gunman breaks from the tradition of justified violence, instead engaging in violence that justifies itself: loss for loss’s sake. Hardly sociopathic; this is instead the logic of a perpetually frustrated death wish. That this resonates with society is interesting but not impossible to predict: prescriptive codes of ethics, to the extent that they are narratively interesting, must be problematic (a hardboiled protagonist who will “never hit a woman” is foiled on several fronts, not least by wicked women who take advantage of him); furthermore, prescriptive codes of ethics also don’t age well, particularly now that widespread and fast communications across demographics have brought about a nearly scientific style of inspection of moral and ethical issues in the public sphere. An everyman whose abilities are unknown to him at the start, the lone gunman can become an aspirational figure for those who have no skills but suspect that they may discover that they too can mow down faceless waves of military police if given the opportunity. Finally, the lack of interiority in the lone gunman figure — the reliance on a supernatural luck, the lack of planning or aspirations, and the absence of intellectual rather than material challenges — is easily mistaken for unflappable cool: it is not that the lone gunman is unflappable out of some internal wellspring of strength, but instead because there is nothing inside him to flap.
